<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2023/docker-neuralprophet/" itemprop="url">初识时间序列预测神器 NeuralProphet 实战预测股票指数</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2023-01-01T18:45:01.000Z" itemprop="datePublished">1月 2 2023</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            19 分钟 读完 (约 2793 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><h2 id="neuralprophet深度学习版prophet">NeuralProphet深度学习版Prophet</h2>
<p><code>NeuralProphet</code> 负有盛名，是 Facebook开发的新一代 Prophet
时间序列预测框架，堪称时间序列预测神器。但是它的API使用，调参，原理不太为大家所知，我们会花几期文章和视频，我们将由浅入深，由实践至原理，揭开其神秘面纱。</p>
<p><code>NeuralProphet</code> 继承了 Prophet
模块可接受性的特点，将预测的值分解到趋势、季节性、AR、事件（节日）几个模块。其中
AR 部分的神经网络实现由 <em><u>AR-Net: A simple Auto-Regressive Neural
Network for time-series</u></em> 这篇论文详细描述。此外，NeuralProphet
整体用 PyTorch 重新实现，主要特性如下</p>
<ul>
<li>使用 PyTorch 的优化，性能比原始 Prophet 快不少</li>
<li>引入 AR-Net 建模时间序列自回归，并配有非线性层</li>
<li>自定义损失和指标</li>
<li>滞后协变量（lagged covariates） 和 AR 本地上下文特性 （local
context）</li>
</ul>
<p>尽管 <code>NeuralProphet</code>
有不少优势，但是使用起来小问题不断，主要表现为文档不甚详细，API
设计的比较智能（隐晦），坑不少。这一期我们来实战体验一下，后续会深入代码和论文。</p>
<p><em><u>相关论文链接：</u></em></p>
<ul>
<li><p>[Prophet] Forecasting at scale <a target="_blank" rel="noopener" href="https://peerj.com/preprints/3190/">https://peerj.com/preprints/3190/</a></p></li>
<li><p>NeuralProphet: Explainable Forecasting at Scale <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.15397">https://arxiv.org/abs/2111.15397</a></p></li>
<li><p>AR-Net: A simple Auto-Regressive Neural Network for time-series
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.12436">https://arxiv.org/abs/1911.12436</a></p></li>
</ul>
<h2 id="安装neuralprophet">安装NeuralProphet</h2>
<p>使用命令通过 pip 就可以安装 NeuralProphet。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install neuralprophet==0.5.0</span><br></pre></td></tr></tbody></table></figure>
<p>如果在 Jupyter Notebook 中使用
NeuralProphet，最好安装实时版本，允许你实时可视化模型损失。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install neuralprophet[live]==0.5.0</span><br></pre></td></tr></tbody></table></figure>
<p>要注意一点，安装 neuralprophet 会关联安装 Pytorch
CPU版本库，如果你需要使用 GPU 或者不希望覆盖原有的 Pytorch
版本，请手动安装。</p>
<p>此外，<strong>MyEncyclopedia</strong> 和往常一样，为大家准备了一个
docker 镜像，预装最新的 NeuralProphet
库，镜像中还包含预下载的数据集和本文所有的 Jupyter Notebook
代码。大家关注
<strong>MyEncyclopedia公众号</strong>，执行下面命令后网页打开
http://localhost:8888/ 开箱即用</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker pull myencyclopedia/neuralprophet-tut</span><br><span class="line">docker run -p 8888:8888 myencyclopedia/neuralprophet-tut bash -c <span class="hljs-string">'jupyter notebook --port 8888 --NotebookApp.token='</span><span class="hljs-string">' --NotebookApp.password='</span><span class="hljs-string">' --ip 0.0.0.0 --allow-root'</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="标准普尔-500-指数数据集">标准普尔 500 指数数据集</h2>
<p>这次实战我们使用过去 10 年标准普尔 500
指数的每日股价数据。可以通过如下命令下载数据集，使用 docker
镜像的同学无需额外下载。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pandas_datareader <span class="hljs-keyword">as</span> pdr</span><br><span class="line"><span class="hljs-keyword">from</span> datetime <span class="hljs-keyword">import</span> datetime</span><br><span class="line"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">%matplotlib inlinestart = datetime(<span class="hljs-number">2010</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>)</span><br><span class="line">end = datetime(<span class="hljs-number">2020</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>)</span><br><span class="line">df_sp500 = pdr.get_data_fred(<span class="hljs-string">'sp500'</span>, start, end)</span><br><span class="line">plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">7</span>))</span><br><span class="line">plt.plot(df_sp500)</span><br><span class="line">plt.title(<span class="hljs-string">'S&amp;P 500 Prices'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/10_year.png"></p>
<p>从图中我们可以清楚地看到标准普尔 500
指数总体呈上升趋势，其中有几个点的价格大幅上涨或下跌。我们可以将这些点视为趋势变化点。鉴于此，我们先从一个仅有趋势模块的
NeuralProphet
模型开始，逐渐加入季节性，AR和节日模块，观察其预测表现和API
具体使用。</p>
<p>使用
NeuralProphet，我们必须确保数据的格式包含如下两列：日期列名<strong>ds</strong>，目标变量列名
<strong>y</strong>。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_sp500 = df_sp500.reset_index().rename(columns={<span class="hljs-string">'DATE'</span>: <span class="hljs-string">'ds'</span>, <span class="hljs-string">'sp500'</span>: <span class="hljs-string">'y'</span>})</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/sp500.png"></p>
<p><img src="/zh/2023/docker-neuralprophet/sp500_head.png"></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-built_in">len</span>(df_sp500[~df_sp500.y.isnull()])</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">2007</span></span><br></pre></td></tr></tbody></table></figure>
<p>总结 SP 500 数据观察到的特点，后面会反复和过程变量做对比：</p>
<ul>
<li><p>总共<strong>2080</strong>条数据中非空数据有<strong>2007</strong>条</p></li>
<li><p>开始日期为 <strong>2012-12-24</strong>，结束日期
<strong>2020-12-11</strong></p></li>
<li><p>在上述有效时间段内，非交易的日期（周末，节日）没有在列。</p></li>
</ul>
<h2 id="模块一趋势">模块一：趋势</h2>
<p>使用
NeuralProphet，我们可以通过指定几个重要参数来对时间序列数据中的趋势进行建模。</p>
<ul>
<li><strong>n_changepoints</strong> — 指定趋势发生变化的点数。</li>
<li><strong>trend_reg</strong> — 控制趋势变化点的正则化参数。较大的值
(~1–100) 将惩罚更多的变化点。较小的值 (~0.001–1.0)
将允许更多的变化点。</li>
<li><strong>changepoints_range</strong> — 默认值
0.8，表示后20%的训练数据无 changepoints</li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralProphet(n_changepoints=<span class="hljs-number">100</span>,</span><br><span class="line">                      trend_reg=<span class="hljs-number">0.05</span>,</span><br><span class="line">                      yearly_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">                      weekly_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">                      daily_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">                      epochs=<span class="hljs-number">100</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="训练模型">训练模型</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_train, df_val = model.split_df(df_sp500, freq=<span class="hljs-string">"D"</span>, valid_p=<span class="hljs-number">0.2</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">metrics = model.fit(df_train,</span><br><span class="line">                    freq=<span class="hljs-string">'D'</span>,</span><br><span class="line">                    validation_df=df_val,</span><br><span class="line">                    progress=<span class="hljs-string">"plot"</span></span><br><span class="line">                    )</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/plot.gif"></p>
<p>训练最终趋于稳定。我们来看看 <code>split_df API</code> 的细节。</p>
<p><img src="/zh/2023/docker-neuralprophet/trend_df_train.png"></p>
<p><img src="/zh/2023/docker-neuralprophet/trend_df_val.png"></p>
<p><code>df_train</code> 共1606 行，为前 80% 记录，<code>df_val</code>
共401 行，为后20% 记录，两者没有交集，合计 2007 行数据，等于
<code>df_sp500</code> 有效数据数。原来默认情况下 <code>split_df</code>
会扔掉 y 值为 NaN
数据。这里两者没有交集，大家注意对比启用AR后的数据切分两者会有交集。原有是启用自回归后，预测需要过去
k 个点作为输入。</p>
<h3 id="预测验证集">预测验证集</h3>
<p>接着来看看验证集，即 <code>df_val</code> 上的预测表现。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">future = model.make_future_dataframe(df_sp500, periods=<span class="hljs-number">60</span>, n_historic_predictions=<span class="hljs-literal">True</span>)</span><br><span class="line">forecast = model.predict(future)</span><br><span class="line">fig = model.plot(forecast)</span><br><span class="line">fig.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_trend.png"></p>
<p><code>make_future_dataframe</code> 准备好待预测的数据格式，参数
<code>periods=60</code>，<code>n_historic_predictions=True</code>
意义扩展 df_sp500 到未来60天后，同时保留所有所有现有
<code>df_sp500</code> 的数据点，这些历史点也将做预测。我们 dump 出
<code>make_future_dataframe</code> 后的 <code>future</code> 变量。</p>
<p><img src="/zh/2023/docker-neuralprophet/trend_future.png"></p>
<p><img src="/zh/2023/docker-neuralprophet/trend_future_y.png"></p>
<p><code>future</code> 序列扩展了 <code>df_sp500</code>，有 y
值的共2007条，和 <code>df_sp500</code> 一致。时间扩展到了
2021-02-09，大约是 2021-12-11 后的60天，这个也和总条数 2140 一致，等于
<code>df_sp500</code> 总条数 2080 加上 <code>periods=60</code>
的部分。</p>
<p>接着来看 predict 后的 <code>forecast</code> 变量。y 列依然有 2007
条，多了 yhat1 和 trend 两列。</p>
<p><img src="/zh/2023/docker-neuralprophet/trend_forecast.png"></p>
<p>最后，<code>model.plot(forecast)</code>
会绘制出事实点和预测点的曲线，注意图中预测值比实际值要稍长一些，因为预测值到
<strong>2021-02-09</strong>，实际值仅到
<strong>2020-12-11</strong>。</p>
<h3 id="模块归因">模块归因</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fig_components = model.plot_components(forecast)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_trend_comp.png"></p>
<p>由于只启用了趋势，只有一个模块输出。</p>
<p>很明显，我们的模型捕捉到了标准普尔 500
指数的总体上涨趋势，但该模型存在欠拟合问题，尤其是当我们查看未知未来的60天的预测，更能发现问题。</p>
<h3 id="仅预测未来">仅预测未来</h3>
<p>同样的预测代码，将<code>n_historic_predictions</code> 改成 False
会只预测未知未来60天。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">future = model.make_future_dataframe(df_sp500, periods=<span class="hljs-number">60</span>, n_historic_predictions=<span class="hljs-literal">False</span>)</span><br><span class="line">forecast = model.predict(future)</span><br><span class="line">fig = model.plot(forecast)</span><br><span class="line">fig.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_trend_no_hist.png"></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(future), <span class="hljs-built_in">len</span>(forecast))</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">60</span> <span class="hljs-number">60</span></span><br></pre></td></tr></tbody></table></figure>
<p>根据上图，我们可以看到模型对未来的预测遵循一条直线，天天上涨的股票，还在这里看什么，还不赶紧去买！</p>
<h2 id="模块二季节性">模块二：季节性</h2>
<p>真实世界的时间序列数据通常涉及季节性模式。即使对于股票市场也是如此，一月效应等趋势可能会逐年出现。我们可以通过添加年度季节性来使之前的模型更加完善。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralProphet(n_changepoints=<span class="hljs-number">100</span>,</span><br><span class="line">                      trend_reg=<span class="hljs-number">0.05</span>,</span><br><span class="line">                      yearly_seasonality=<span class="hljs-literal">True</span>,</span><br><span class="line">                      weekly_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">                      daily_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">                      epochs=<span class="hljs-number">100</span>)</span><br><span class="line"></span><br><span class="line">df_train, df_val = model.split_df(df_sp500, freq=<span class="hljs-string">"D"</span>, valid_p=<span class="hljs-number">0.2</span>)</span><br><span class="line"></span><br><span class="line">metrics = model.fit(df_train,</span><br><span class="line">                    freq=<span class="hljs-string">'D'</span>,</span><br><span class="line">                    validation_df=df_val,</span><br><span class="line">                    progress=<span class="hljs-string">"bar"</span></span><br><span class="line">                    )</span><br></pre></td></tr></tbody></table></figure>
<h3 id="预测验证集-1">预测验证集</h3>
<p><img src="/zh/2023/docker-neuralprophet/pred_season.png"></p>
<p>和之前一条直线相比，现在对数据的预测显得更现实些。</p>
<h3 id="模块归因-1">模块归因</h3>
<p>现在预测的 Y 值是两个部分的模块的加和了。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fig_components = model.plot_components(forecast)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_season_comp.png"></p>
<p>标准普尔 500 指数预测具有年度季节性，包括历史数据。</p>
<h3 id="仅预测未来-1">仅预测未来</h3>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecast(model, sp500_data, periods=60, historic_predictions=False, highlight_steps_ahead=60)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_season_no_hist.png"></p>
<p>根据上图，我们可以看到这个模型更真实一些，但仍然存在欠拟合问题。因此，我们再引入自回归模型
AR 来进一步拟合。</p>
<h2 id="模块三自回归-ar">模块三：自回归 AR</h2>
<p>AR-Net
是一种用于时间序列预测的自回归神经网络。自回归模型使用来自过去历史数据点来预测后续点，这就是<strong>自回归</strong>一词的来源。</p>
<p>例如，为了预测标准普尔 500 指数的价格，我们可以训练一个模型，使用过去
60 天的价格来预测未来 60
天的价格。分别对应以下代码中的<strong>n_lags</strong>和<strong>n_forecasts</strong>参数。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralProphet(</span><br><span class="line">    n_forecasts=<span class="hljs-number">60</span>,</span><br><span class="line">    n_lags=<span class="hljs-number">60</span>,</span><br><span class="line">    changepoints_range=<span class="hljs-number">0.95</span>,</span><br><span class="line">    n_changepoints=<span class="hljs-number">100</span>,</span><br><span class="line">    yearly_seasonality=<span class="hljs-literal">True</span>,</span><br><span class="line">    weekly_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">    daily_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">    batch_size=<span class="hljs-number">32</span>,</span><br><span class="line">    epochs=<span class="hljs-number">100</span>,</span><br><span class="line">    learning_rate=<span class="hljs-number">1.0</span>,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="训练模型-1">训练模型</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df_train, df_val = model.split_df(df_sp500, freq=<span class="hljs-string">"D"</span>, valid_p=<span class="hljs-number">0.2</span>)</span><br><span class="line">metrics = model.fit(df_train,</span><br><span class="line">                    freq=<span class="hljs-string">'D'</span>,</span><br><span class="line">                    validation_df=df_val,</span><br><span class="line">                    progress=<span class="hljs-string">"plot"</span></span><br><span class="line">                    )</span><br></pre></td></tr></tbody></table></figure>
<p>切分训练和验证集代码一样，但是由于引入
AR，<code>df_train</code>，<code>df_val</code>
之间有60条数据重合，这是因为，在验证或者预测过程中，传入的 dataframe
前60条不做预测，从61条开始预测，预测会使用当前日期前60条作为 AR
模块的输入。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(df_train.ds.tolist()).intersection(<span class="hljs-built_in">set</span>(df_val.ds.tolist())))</span><br><span class="line"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">60</span></span><br></pre></td></tr></tbody></table></figure>
<p>不过奇怪的是，<code>df_train</code> 加上 <code>df_val</code> 总共有
2305 + 665 = 2970 条记录，时间跨度依然是 2012-12-24 至
2020-12-11。但是去除重复的60条记录后居然剩余2910 条， 比
<code>df_sp500</code> 2080 条记录数还要多不少。</p>
<p><img src="/zh/2023/docker-neuralprophet/ar_df_train.png"></p>
<p><img src="/zh/2023/docker-neuralprophet/ar_df_val.png"></p>
<p>这里笔者稍微花了点时间终于弄清楚：<code>df_train</code> 和
<code>df_val</code> 会填充 2012-12-24 至 2020-12-11 所有的 missing
日期，并使用插值填充 y！</p>
<p><img src="/zh/2023/docker-neuralprophet/ar_df_train_head.png"></p>
<h3 id="预测验证集-2">预测验证集</h3>
<p>这一次，我们将 <code>periods</code> 设成 0，也就是不扩展
<code>df_sp500</code> 时间到未知的未来。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">future = model.make_future_dataframe(df_sp500, periods=<span class="hljs-number">0</span>, n_historic_predictions=<span class="hljs-literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/ar_future.png"></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forecast = model.predict(future)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/ar_forecast.png"></p>
<p><code>forecast</code> 格式变得复杂，引入了 yhat1, yhat2,
...，yhat60，ar1, ar2, ...，ar60 等众多列，这里的60对应于
<code>n_forecasts=60</code></p>
<p>第一个预测值开始于 <code>forecast</code> 的第61条记录，对应于
<code>n_lags = 60</code></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forecast[~forecast.yhat1.isnull()]</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/ar_forecast_predicted.png"></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig = model.plot(forecast)</span><br><span class="line">fig.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_ar.png"></p>
<h3 id="模块归因-2">模块归因</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fig_components = model.plot_components(forecast)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_ar_comp.png"></p>
<h3 id="仅预测未来-2">仅预测未来</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">future = model.make_future_dataframe(df_sp500, periods=<span class="hljs-number">60</span>, n_historic_predictions=<span class="hljs-literal">False</span>)</span><br><span class="line">forecast = model.predict(future)</span><br><span class="line">fig = model.plot(forecast)</span><br><span class="line">fig.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_ar_no_hist.png"></p>
<h2 id="模块四事件节日">模块四：事件（节日）</h2>
<p>我们还可以配置模型以考虑节假日因素，因为节假日很可能会影响股市走势。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = NeuralProphet(</span><br><span class="line">    n_forecasts=<span class="hljs-number">60</span>,</span><br><span class="line">    n_lags=<span class="hljs-number">60</span>,</span><br><span class="line">    changepoints_range=<span class="hljs-number">0.95</span>,</span><br><span class="line">    n_changepoints=<span class="hljs-number">100</span>,</span><br><span class="line">    yearly_seasonality=<span class="hljs-literal">True</span>,</span><br><span class="line">    weekly_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">    daily_seasonality=<span class="hljs-literal">False</span>,</span><br><span class="line">    batch_size=<span class="hljs-number">32</span>,</span><br><span class="line">    epochs=<span class="hljs-number">100</span>,</span><br><span class="line">    learning_rate=<span class="hljs-number">1.0</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = model.add_country_holidays(<span class="hljs-string">"US"</span>, mode=<span class="hljs-string">"additive"</span>, lower_window=-<span class="hljs-number">1</span>, upper_window=<span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>只需 <code>add_country_holidays</code>
一条语句就可以启用预定义的美国节假日。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_forecast(model, sp500_data, periods=<span class="hljs-number">60</span>, historic_predictions=<span class="hljs-literal">False</span>, highlight_steps_ahead=<span class="hljs-number">60</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="预测验证集-3">预测验证集</h3>
<p><img src="/zh/2023/docker-neuralprophet/pred_holiday.png"></p>
<h3 id="模块归因-3">模块归因</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fig_components = model.plot_components(forecast)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_holiday_comp.png"></p>
<h3 id="仅预测未来-3">仅预测未来</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">future = model.make_future_dataframe(df_sp500, periods=<span class="hljs-number">60</span>, n_historic_predictions=<span class="hljs-literal">False</span>)</span><br><span class="line">forecast = model.predict(future)</span><br><span class="line">fig = model.plot(forecast)</span><br><span class="line">fig.show()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/zh/2023/docker-neuralprophet/pred_holiday_no_hist.png"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/pure-math-course/" itemprop="url">程序员的纯代数视频和教材指南</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-10-28T18:45:01.000Z" itemprop="datePublished">10月 29 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            20 分钟 读完 (约 3063 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在入门机器人视觉和机器人运动后，开始逐步接触到了3D计算机视觉中的高阶数学概念，包括三维物体到二维图片的变换（术语称之为射影几何）；三维欧氏空间的物体运动坐标系变换，分为主动变换（active
transformations）和被动变换（passive
transformation）；另外在更高阶的计算机渲染中常会用到Mesh和黎曼曲面；此外，几何深度学习（Geometric
Deep
Learning）中也涉及到群论，李群等。这些迷之概念使得我对于本科高等数学课程（多元微积分，线性代数，概率论）后面的纯数学感到兴趣。本来一直觉得纯数学会非常难学，但是当我写了很多年代码和阅读了多个AI领域的众多论文之后，总有一些本质问题萦绕在心，得不到解释：</p>
<ul>
<li>代码抽象的极限在哪里？高阶纯函数编程？</li>
<li>深度学习多个领域（CV，NLP，RL，GNN）是否有统一视角？目前深度学习的论文汗牛充栋，既有很多相似的pattern，也有每个领域的特性，是否有通用设计原则</li>
</ul>
<p>第一个问题在范畴论（category
theory）中广泛探讨，不过范畴论过于深奥，是抽象代数后的一门纯函数编程的理论基础。第二个问题在几何深度学习中被探讨。不过，这两个问题肯定不会被完全回答，但是，即使无法得到最好的解释和回答，就如同统计学中频率派和贝叶斯派各有所长，没有公认的一家通吃，但是对于这些根本问题的探究的阶段性成功也是非常有意义的。正如理查德·费曼的名言：</p>
<blockquote>
<p>I would rather have questions that can't be answered than answers
that can't be questioned</p>
</blockquote>
<p>鉴于AI从业人员乃至程序员进阶掌握高阶数学的武器，我斗胆总结一份从我这个非数学系的理工毕业生角度出发的自学纯数学指南。以下所有课程视频都已经搬运到了我的B站频道（MyEncyclopedia公号）中，此外，视频课程有对应教材的也一并列出。</p>
<p>在学习这些知识时候，发现至今的联系是非常广泛的，这个倒是非常出乎我意料，也让学习非常有成就感。以下总结了若干贯穿所有领域的重要概念。</p>
<ul>
<li>群的最一般概念和离散数学群</li>
<li>群论体系下构建的抽象代数结构</li>
<li>有限维向量空间的线性映射和基变换，并且此视角下的矩阵（线性代数）理解</li>
<li>从基变换的角度理解傅里叶变换，即傅里叶变换是函数表示的特殊的基。</li>
<li>有限维向量空间上定义了距离（内积空间）后，向量空间有了几何结构。</li>
<li>从线性映射推广到多变量线性映射（multilinear
map），这是张量（Tensor）的数学意义</li>
<li>从离散数学群论到连续函数映射，形成了微分流形（Differential
Manifold）的概念。在此基础上进而发展出了李群，本质是为了解决例如旋转矩阵群虽然在乘法上封闭（因此组成群）但是在加法上不封闭的问题。</li>
<li>一般群论扩展出范畴论，这是函数式编程的数学原理。</li>
</ul>
<h2 id="学习顺序">学习顺序</h2>
<p>下图为我推荐的课程学习顺序。</p>
<p><img src="/zh/2022/pure-math-course/roadmap.jpg"></p>
<p>当对张量和李群掌握充分后，原则上可以更上一层楼，学习广义相对论和量子力学。不过我并没有计划涉足这两个艰深的领域，也就不多言了。</p>
<p><img src="/zh/2022/pure-math-course/roadmap-2.jpg"></p>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">graph TD;</span><br><span class="line">    map[高等线性代数] --&gt;abs;</span><br><span class="line">    map[高等线性代数] --&gt;T[Tensor];</span><br><span class="line">    group[群论]--&gt;abs[抽象代数];</span><br><span class="line">    abs--&gt;C[范畴论];</span><br><span class="line">    map--&gt;F;</span><br><span class="line">    F--&gt;W[小波变换];</span><br><span class="line">    F--&gt;FA[泛函分析];</span><br><span class="line">    abs--&gt;F[傅里叶变换];</span><br><span class="line">    T--&gt;m[Manifold];</span><br><span class="line">    m--&gt;L[李群];</span><br><span class="line">    T--&gt;L;</span><br><span class="line">    L--&gt;G[广义相对论];</span><br><span class="line">    L--&gt;Q[量子力学];</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph TD;</span><br><span class="line">    map[高等线性代数] --&gt;T[Tensor];</span><br><span class="line">    T--&gt;m[Manifold];</span><br><span class="line">    m--&gt;L[李群];</span><br><span class="line">    T--&gt;L;</span><br><span class="line">    T--&gt;G;</span><br><span class="line">    T--&gt;Q;</span><br><span class="line">    L--&gt;G[广义相对论];</span><br><span class="line">    L--&gt;Q[量子力学];</span><br></pre></td></tr></tbody></table></figure>
<h2 id="线性映射下的高等线性代数">线性映射下的高等线性代数</h2>
<h3 id="sheldon-axler线性代数应该这样学">【Sheldon
Axler】线性代数应该这样学</h3>
<h4 id="b站链接">B站链接</h4>
<p>https://www.bilibili.com/video/BV1Dm4y1c7Cn</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Dm4y1c7Cn/?spm_id_from=333.999.0.0">Sheldon
Axler Linear Algebra Done Right (线性代数这样学) 原作者亲自讲解</a></li>
</ul>
<h4 id="评价">评价</h4>
<p>本课程的教师是配套同名教材<code>线性代数应该这样学</code>（<code>Linear
Algebra Done Right</code>）这本书的作者。课程和教材都是从线性映射(linear
map)角度来看待矩阵。由最基本的向量空间和内积空间定义出发导出对偶空间（Dual
Space），特征值，特征根，对角矩阵，正交基，直到最核心的谱论（Spectral
theorem）。
此课程说实话不适宜第一次学线性代数的同学。同时，不适合数学系的，因为跟高代相比太浅；不适合工程类的，因为太深而且没有跟应用联系起来。但是，本书却广受赞誉，中文也有翻译本。其最大的特点是通过空间变换和矩阵联系起来，从最基本的定义导出矩阵中的最核心概念：特征值，特征根，对角矩阵，正交基等。</p>
<h4 id="配套教材">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/linear-alg-done-right.jpg"></p>
<h3 id="nathaniel-johnston-advanced-linear-algebra">【Nathaniel
Johnston】 Advanced Linear Algebra</h3>
<h4 id="b站链接-1">B站链接</h4>
<p>https://www.bilibili.com/video/BV1uV4y157XE</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1uV4y157XE/?spm_id_from=333.999.0.0">Nathaniel
Johnston Advanced Linear Algebra</a></p>
<h4 id="评价-1">评价</h4>
<p>课程关于线性映射背后的基变换等讲述的非常清楚。不过对比<code>Nathaniel
Johnston</code>的教材<code>Advanced Linear Algebra and Matrix
Algebra</code>来说，视频课程只涉及到教材的一二章的内容，但并没有涉及到第三章Tensor的部分，有点可惜。视频包含很多向量空间和线性映射的例子和证明，可以作为<code>线性代数应该这样学</code>的很好补充。同时推荐教材第三章Tensor，适合作为Tensor概念的入门理论学习。</p>
<h4 id="配套教材-1">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/adv-linear-alg.jpg"></p>
<h3 id="matthew-macauleyadvanced-linear-algebra-math">【Matthew
Macauley】Advanced Linear Algebra Math</h3>
<h4 id="b站链接-2">B站链接</h4>
<p>https://www.bilibili.com/video/BV1Me4y1a7Uk</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Me4y1a7Uk/?spm_id_from=333.999.0.0">Advanced
Linear Algebra Math 4120 Modern Algebra, Matthew Macauley @
Clemson</a></p>
<h4 id="评价-2">评价</h4>
<p><code>Matthew
Macauley</code>教授讲课节奏条理都很赞，示例也比较充足，他的多个数学课程都很棒，尤其是下面群论部分的<code>Visual
Group
Theory</code>最为经典。本课程可以作为上述两门自成体系的课程很好的补充，尤其是Tensor作为multilinear
map的部分。课程对应教材为<code>Peter
Lax</code>的经典老教材<code>线性代数及其应用</code>。</p>
<h4 id="配套教材-2">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/linear-alg-lax.jpg"></p>
<h2 id="群论抽象代数">群论，抽象代数</h2>
<h3 id="matthew-macauley可视化群论">【Matthew Macauley】可视化群论</h3>
<h4 id="b站链接-3">B站链接</h4>
<p>https://www.bilibili.com/video/BV1ka411378X</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1ka411378X/?spm_id_from=333.999.0.0">Visual
Group Theory Math 4120 Modern Algebra, Matthew Macauley @
Clemson</a></p>
<h4 id="配套教材-3">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/visual-group-theory.jpg"></p>
<h4 id="评价-3">评价</h4>
<p>课程基于<code>Nathan Carter</code>负有盛名的<code>Visual Group
Theory</code>
这本<strong>非常非常棒</strong>的群论入门书。视频课程本身在教材的基础上补充了不少例子和定理证明，使抽象难以理解的群里严重降低了入门难度。我作为同时仔细看过视频和教材的读者，觉得视频课程有以下几个优势：首先是建立概念时一步一步带你walk
through，比如最核心的群同态概念，使得概念的建立更加深刻形象；其次课程补充了不少代数方法的严格证明，有的时候，代数方法比视觉方法更显示了问题的本质和联系。强烈推荐视频和教材，因为群论是现代数学的基石。</p>
<h3 id="哈佛benedict-gross抽象代数">【哈佛Benedict Gross】抽象代数</h3>
<h4 id="b站链接-4">B站链接</h4>
<p>https://www.bilibili.com/video/BV1Yt4y1F72S</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Yt4y1F72S/?spm_id_from=333.999.0.0">【Harvard
MATH E-222】 Abstract Algebra (Benedict Gross)</a></p>
<h4 id="评价-4">评价</h4>
<p>很老的课程了，对应的教材为经典的 <code>Michael Artin</code>
的<code>代数</code>。学完本视频和配套教材会对抽象代数有扎实深入的理解。</p>
<h4 id="配套教材-4">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/algebra-artin.jpg"></p>
<h2 id="张量">张量</h2>
<h3 id="eigenchris-系列">eigenchris 系列</h3>
<h4 id="b站链接-5">B站链接</h4>
<ul>
<li><p>【eigenchris】 Tensors for Beginners
https://www.bilibili.com/video/BV1zW4y1n7Cy</p></li>
<li><p>【eigenchris】 Tensor Calculus
https://www.bilibili.com/video/BV1Ae4y167ty</p></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1zW4y1n7Cy/?spm_id_from=333.999.0.0">【eigenchris】
Tensors for Beginners</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Ae4y167ty/?spm_id_from=333.999.0.0">【eigenchris】
Tensor Calculus</a></p>
<h4 id="评价-5">评价</h4>
<p><code>eigenchris</code> 的 tensor
入门系列讲的应该是最好理解的，举了很多具体的例子，例如关于重要的概念行向量作为covector。可惜</p>
<h3 id="xylyxylyx">XylyXylyX</h3>
<h4 id="b站链接-6">B站链接</h4>
<p>【XylyXylyX】 What is a Tensor
https://www.bilibili.com/video/BV1NK411Q7p4</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1NK411Q7p4/?spm_id_from=333.999.0.0">【XylyXylyX】
What is a Tensor</a></p>
<h4 id="相关教材">相关教材</h4>
<p>并非上述两个视频课程的配套教材，我找到一些市面上不错的纯 Tensor
类入门书籍 <img src="/zh/2022/pure-math-course/tensor-student.jpg"></p>
<p><img src="/zh/2022/pure-math-course/tensor-calc.jpg"></p>
<h2 id="李群">李群</h2>
<p>如之前所说，李群的本质是为了解决例如旋转矩阵群虽然在乘法上封闭（因此组成群）但是在加法上不封闭的问题。因为在做优化或者其他运算时，旋转代表的矩阵必然用到矩阵加法。李群的解决方式是将小量上的映射到李空间上做完运算后映射回李群。不过一般李群过于复杂，学习内容过于庞大，因此简单的入门李群本身也比较困难。</p>
<h3 id="xylyxylyx-1">XylyXylyX</h3>
<h4 id="b站链接-7">B站链接</h4>
<p>【XylyXylyX】 Lie Groups and Lie Algebras]
https://www.bilibili.com/video/BV1Z14y1j78c</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Z14y1j78c/?spm_id_from=333.999.0.0">【XylyXylyX】
Lie Groups and Lie Algebras</a></p>
<h4 id="评价-6">评价</h4>
<p><code>XylyXylyX</code>的李群视频是众多李群入门中比较不错的，有不少重复，适合初学。配套
<code>Robert Gilmore</code>的如下教材（教材后面部分过于复杂）。</p>
<h4 id="配套教材-5">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/lie-gilmore.jpg"></p>
<h3 id="jonathan-evans">Jonathan Evans</h3>
<h4 id="b站链接-8">B站链接</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FK411D7dv/?spm_id_from=333.999.0.0">【Jonathan
Evans】 Lie Groups and Lie Algebras</a></p>
<h4 id="配套教材-6">配套教材</h4>
<p>视频有专门一节推荐了众多不错的教材，不过李群这个领域过于深奥，我也只是刚入门。因此在此仅列其一：Springer
的GTM（研究生数学系列）<code>表示理论</code> <img src="/zh/2022/pure-math-course/representation.jpg"></p>
<h2 id="傅里叶变换">傅里叶变换</h2>
<p>傅里叶变换的本质是将函数分解成一组相对卷积算子的对角化基，是进阶泛函分析的重要概念。</p>
<h3 id="斯坦福-brad-osgood傅里叶变换和应用">【斯坦福 Brad
Osgood】傅里叶变换和应用</h3>
<h4 id="b站链接-9">B站链接</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Gm4y1A7bw/?spm_id_from=333.999.0.0">【Stanford】
The Fourier Transforms and its Applications</a></p>
<h4 id="配套教材-7">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/stanford-fourier.jpg"> 可由下方地址下载
https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf</p>
<h2 id="微分流形">微分流形</h2>
<h3 id="xylyxylyx-2">XylyXylyX</h3>
<h4 id="b站链接-10">B站链接</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1fG411E7Tc/?spm_id_from=333.999.0.0">【XylyXylyX】What
is a Manifold</a></p>
<h2 id="程序员的范畴论">程序员的范畴论</h2>
<h4 id="b站链接-11">B站链接</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1v14y1e7et/?spm_id_from=333.999.0.0">【Bartosz
Milewski】 Category Theory for Programmers</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV14P4y1Z7Ci/?spm_id_from=333.999.0.0">【Bartosz
Milewski】 Category Theory for Programmers II</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1gB4y1n72g/?spm_id_from=333.999.0.0">【Bartosz
Milewski】 Category Theory for Programmers III</a></p>
<h4 id="配套教材-8">配套教材</h4>
<p><img src="/zh/2022/pure-math-course/category-bartosz.jpg"></p>
<h2 id="其他">其他</h2>
<h4 id="b站链接-12">B站链接</h4>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1De4y1m7uS/?spm_id_from=333.999.0.0" title="[NJ Wildberger] Famous Math Problems">【NJ Wildberger】 Famous
Math Problems</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/app-arxiv-search/" itemprop="url"></a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-07-22T18:45:01.000Z" itemprop="datePublished">7月 23 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            3 分钟 读完 (约 502 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>如果你是一个重度论文阅读者，一定会在自己的笔记本上全副武装。但是也许只是偶尔，也会心血来潮地在手机端查看一篇论文，此时，你打开的这篇从
Arxiv
下载的论文，就像下图上半部分所示，时间稍长，便会觉得文字，公式和图片不忍直视。你多么希望在手机端查看PDF，也像图中下半部分一样重新排版来轻松阅读。</p>
<p><img src="/zh/2022/app-arxiv-search/mobile-conv-html.jpg"></p>
<h2 id="arxiv-论文手机智能重排和阅读服务">Arxiv
论文手机智能重排和阅读服务</h2>
<p>现在，MyEncyclopedia 推出了智能的手机端重排和阅读 arxiv
论文服务。手机端只需浏览器甚至在<strong>微信</strong>中就能将指定的
arxiv 论文重排成适应移动端的格式，并在线浏览。</p>
<p>具体效果如下</p>
<p><img src="/zh/2022/app-arxiv-search/view.gif"></p>
<h3 id="微信操作步骤">微信操作步骤</h3>
<p>下面，以手机微信为例，讲解详细步骤。其他浏览器可以直接点击
<code>https://mydoc.myencyclopedia.top/app</code>，操作步骤类似。</p>
<ul>
<li><p>进入 MyEncyclopedia 公众号，回复 <code>?</code> 点击
<code>AI在线服务</code>。 <img src="/zh/2022/app-arxiv-search/op_01_enter_service.jpg"></p></li>
<li><p>拷贝显示的口令</p></li>
</ul>
<p><img src="/zh/2022/app-arxiv-search/op_02_get_token.jpg"></p>
<ul>
<li>返回到公众号后回复口令，登录使用服务</li>
</ul>
<p><img src="/zh/2022/app-arxiv-search/op_03_enter.jpg"></p>
<ul>
<li>输入 <code>Arxiv ID</code>，点击 <code>Go to Page</code></li>
</ul>
<p><img src="/zh/2022/app-arxiv-search/op_04_arxiv_id.jpg"></p>
<ul>
<li>在新页面点击 <code>Convert Html</code></li>
</ul>
<p><img src="/zh/2022/app-arxiv-search/op_05_trigger_html.jpg"></p>
<ul>
<li>等待大约十几分钟，再次进入服务页面，此时，若下载转换任务成功，出现如下页面，点击<code>View
Html</code>，即可在微信浏览器中完美查看论文了。</li>
</ul>
<p><img src="/zh/2022/app-arxiv-search/op_06_view_html.jpg"></p>
<h3 id="pdf-论文双列切割成单列服务">PDF 论文双列切割成单列服务</h3>
<p>此外，也提供将 arxiv PDF从双列切割成单列服务。即上图中的<code>Convert
PDF</code>
按钮，切割后的PDF文件可以在线浏览，或者下载到手机离线查看。也欢迎大家尝试。</p>
<p><img src="/zh/2022/app-arxiv-search/mobile-conv-pdf.jpg"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/share-cs25-transformer-united/" itemprop="url"></a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-07-15T18:45:01.000Z" itemprop="datePublished">7月 16 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 分钟 读完 (约 871 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>斯坦福大学的 CS 课程上线了一门经典课程：《CS 25: Transformers
United》。自 2017 年推出以来，Transformer 彻底改变了自然语言处理
(NLP)领域。现在，Transformer 在深度学习中被广泛使用，无论是计算机视觉
(CV)、强化学习 (RL)、生成对抗网络
(GAN)、语音甚至是生物学。除此之外，Transformer
还能够创建强大的语言模型（如 GPT-3），并在 AlphaFold2
中发挥了重要作用，该算法解决了蛋白质折叠问题。</p>
<p>目前这门课程在 <code>Youtube</code> 上日更连载中，地址为
https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM</p>
<p><strong>MyEncyclopedia Bilibili
为大家每日搬运同步视频，至今天7/16日已经有6集</strong></p>
<p><img src="/zh/2022/share-cs25-transformer-united/my_bili.jpg"> <img src="/zh/2022/share-cs25-transformer-united/course_home.jpg"></p>
<h2 id="明星讲课阵容">明星讲课阵容</h2>
<p>在今天公布的第一节课中，讲师为斯坦福大学硕士生 Divyansh
Garg、软件工程师 Chetanya Rastogi（毕业于斯坦福大学）、软件工程师 Advay
Pal（毕业于斯坦福大学）。</p>
<p>此外，第一节课的指导教授为 Christopher
Manning，他是斯坦福大学计算机与语言学教授，也是将深度学习应用于自然语言处理领域的领军者。</p>
<p>从之前的课程描述来看，CS 25 课程邀请了来自不同领域关于 Transformer
研究的前沿人士进行客座讲座。OpenAI 的研究科学家 Mark Chen，主要介绍基于
Transformers 的 GPT-3、Codex；Google Brain 的科学家 Lucas
Beyer，主要介绍 Transformer 在视觉领域的应用；Meta FAIR 科学家 Aditya
Grover，主要介绍 RL 中的 Transformer 以及计算引擎等。</p>
<p>值得一提的是，AI 教父 <strong>Geoff Hinton</strong>
也带来了一次讲座。</p>
<h2 id="课程明细">课程明细</h2>
<h4 id="sep-20-introduction-to-transformers">1. (Sep 20) Introduction to
Transformers</h4>
<p>Recommended Readings:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All
You Need</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The
Illustrated Transformer</a></p></li>
<li><p><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The
Annotated Transformer (Assignment)</a></p></li>
</ul>
<h4 id="sept-27-transformers-in-language-gpt-3-codex">2. (Sept 27)
Transformers in Language: GPT-3, Codex</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=5fU-QMwAAAAJ&amp;hl=en"><strong>Mark
Chen</strong> (OpenAI)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot
Learners</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Evaluating Large Language
Models Trained on Code</a></p>
<h4 id="oct-4-applications-in-vision">3. (Oct 4) Applications in
Vision</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=p2gwhK4AAAAJ&amp;hl=en"><strong>Lucas
Beyer</strong> (Google Brain)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An
Image is Worth 16x16 Words (Vision Transfomer)</a><br>
- Additional Readings:<br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.10270">How to train your
ViT?</a></p>
<h4 id="oct-11-transformers-in-rl-universal-compute-engines">4. (Oct 11)
Transformers in RL &amp; Universal Compute Engines</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://aditya-grover.github.io/"><strong>Aditya
Grover</strong> (FAIR)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.05247">Pretrained Transformers as
Universal Computation Engines</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.01345">Decision Transformer:
Reinforcement Learning via Sequence Modeling</a></p>
<h4 id="oct-18-scaling-transformers">5. (Oct 18) Scaling
transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://barretzoph.github.io/"><strong>Barret
Zoph</strong> (Google Brain)</a> with <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=mY6p8gcAAAAJ&amp;hl=en"><strong>Irwan
Bello</strong></a> and <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=-ZfwQOkAAAAJ&amp;hl=en"><strong>Liam
Fedus</strong></a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to
Trillion Parameter Models with Simple and Efficient Sparsity</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.08906">ST-MoE: Designing Stable
and Transferable Sparse Expert Models</a></p>
<h4 id="oct-25-perceiver-arbitrary-io-with-transformers">6. (Oct 25)
Perceiver: Arbitrary IO with transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://www.drewjaegle.com/"><strong>Andrew
Jaegle</strong> (DeepMind)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.03206">Perceiver: General Perception
with Iterative Attention</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.14795">Perceiver IO: A General
Architecture for Structured Inputs &amp; Outputs</a></p>
<h4 id="nov-1-self-attention-non-parametric-transformers">7. (Nov 1)
Self Attention &amp; Non-Parametric Transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.ca/citations?user=2oq9614AAAAJ&amp;hl=en"><strong>Aidan
Gomez</strong> (University of Oxford)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.02584">Self-Attention Between
Datapoints: Going Beyond Individual Input-Output Pairs in Deep
Learning</a></p>
<h4 id="nov-8-glom-representing-part-whole-hierarchies-in-a-neural-network">8.
(Nov 8) GLOM: Representing part-whole hierarchies in a neural
network</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/"><strong>Geoffrey
Hinton</strong> (UoT)</a> Recommended Readings:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.12627">How to represent
part-whole hierarchies in a neural network</a></li>
</ul>
<h4 id="nov-15-interpretability-with-transformers">9. (Nov 15)
Interpretability with transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://colah.github.io"><strong>Chris
Olah</strong> (AnthropicAI)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://distill.pub/2021/multimodal-neurons/">Multimodal Neurons
in Artificial Neural Networks</a> Additional Readings: - <a target="_blank" rel="noopener" href="https://distill.pub/2018/building-blocks/">The Building Blocks of
Interpretability</a></p>
<h4 id="nov-29-transformers-for-applications-in-audio-speech-and-music-from-language-modeling-to-understanding-to-synthesis">10.
(Nov 29) Transformers for Applications in Audio, Speech and Music: From
Language Modeling to Understanding to Synthesis</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://ai.stanford.edu/~prateekv/"><strong>Prateek
Verma</strong> (Stanford)</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-faiss-transformer/" itemprop="url">实战入门 faiss 搜索bert 最邻近句子：docker CPU镜像开箱即用，无需额外安装下载</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-07-08T18:45:01.000Z" itemprop="datePublished">7月 9 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            13 分钟 读完 (约 1886 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们延续上一期 <em>Bert 中文短句相似度计算 Docker
CPU镜像</em>，继续使用 <code>huggingface transformer</code> 和
<code>sentence-transformer</code> 类库，并将英语句子生成 bert
embedding，然后引入 <code>faiss</code>
类库来建立索引，最后查询最接近的句子。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>本期 docker 镜像获取方式为，关注 <code>MyEncyclopedia</code>
公众号后回复 <code>docker-faiss-transformer</code>
即可获取如下完整命令。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8888:8888 myencyclopedia/faiss-demo bash -c <span class="hljs-string">'jupyter notebook --allow-root --port 8888 --NotebookApp.token= --ip 0.0.0.0'</span></span><br></pre></td></tr></tbody></table></figure>
<p>然后打开浏览器，输入
<code>http://localhost:8888/notebooks/faiss_demo.ipynb</code></p>
<h2 id="faiss-简介">faiss 简介</h2>
<p>Faiss 的全称是Facebook AI Similarity Search，是由 Facebook
开发的适用于稠密向量匹配的开源库，作为向量化检索开山鼻祖，Faiss
提供了一套查询海量高维数据集的解决方案，它从两个方面改善了暴力搜索算法存在的问题：降低空间占用和加快检索速度。此外，Faiss
提供了若干种方法实现数据压缩，包括 PCA、Product-Quantization等。</p>
<p><strong>Faiss 主要特性：</strong></p>
<ul>
<li>支持相似度检索和聚类；</li>
<li>支持多种索引方式；</li>
<li>支持CPU和GPU计算；</li>
<li>支持Python和C++调用；</li>
</ul>
<h3 id="faiss-使用流程">Faiss 使用流程</h3>
<p>使用 faiss
分成两部，第一步需要对原始向量建立索引文件，第二步再对索引文件进行向量
<code>search</code> 操作。</p>
<p>在第一次建立索引文件的时候，需要经过 <code>train</code> 和
<code>add</code>
两个过程；后续如果有新的向量需要被添加到索引文件，只需要一个
<code>add</code>
操作来实现增量索引更新，但是如果增量的量级与原始索引差不多的话，整个向量空间就可能发生了一些变化，这个时候就需要重新建立整个索引文件，也就是再用全部的向量来走一遍
<code>train</code> 和 <code>add</code>，至于具体是如何
<code>train</code> 和 <code>add</code>的，就和特定的索引类型有关了。</p>
<h3 id="indexflatl2-indexflatip"><strong>1. IndexFlatL2</strong> &amp;
indexFlatIP</h3>
<p>对于精确搜索，例如欧式距离 faiss.indexFlatL2 或 内积距离
faiss.indexFlatIP，没有 <code>train</code> 过程，<code>add</code>
完直接可以 <code>search</code>。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss </span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 建立索引, 定义为dimension d = 128</span></span><br><span class="line">index = faiss.IndexFlatL2(d)</span><br><span class="line"></span><br><span class="line"> <span class="hljs-comment"># add vectors, xb 为 (100000,128)大小的numpy</span></span><br><span class="line">index.add(xb)                 </span><br><span class="line"><span class="hljs-built_in">print</span>(index.ntotal) </span><br><span class="line"><span class="hljs-comment"># 索引中向量的数量, 输出100000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 求4-近邻</span></span><br><span class="line">k = <span class="hljs-number">4</span></span><br><span class="line"><span class="hljs-comment"># xq为query embedding, 大小为(10000,128)</span></span><br><span class="line">D, I = index.search(xq, k)     </span><br><span class="line"><span class="hljs-comment">## D shape (10000,4)，表示每个返回点的embedding 与 query embedding的距离,</span></span><br><span class="line"><span class="hljs-comment">## I shape (10000,4)，表示和query embedding最接近的k个物品id，</span></span><br><span class="line"><span class="hljs-built_in">print</span>(I[:<span class="hljs-number">5</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="indexivfflat"><strong>2. IndexIVFFlat</strong></h3>
<p>IndexFlatL2
的结果虽然精确，但当数据集比较大的时候，暴力搜索的时间复杂度很高，因此我们一般会使用其他方式的索引来加速。比如
IndexIVFFlat，将数据集在 <code>train</code> 阶段分割为几部分，技术术语为
<code>Voronoi
Cells</code>，每个数据向量只能落在一个cell中。<code>Search</code>
时只需要查询query向量落在cell中的数据了，降低了距离计算次数。这个过程本质就是高维
KNN 聚类算法。<code>search</code> 阶段使用倒排索引来。</p>
<p>IndexIVFFlat 需要一个训练的阶段，其与另外一个索引 quantizer
有关，通过 quantizer 来判断属于哪个cell。IndexIVFFlat
在搜索阶段，引入了nlist(cell的数量)与nprob(执行搜索的cell数)参数。增大nprobe可以得到与brute-force更为接近的结果，nprobe就是速度与精度的调节器。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss</span><br><span class="line">nlist = <span class="hljs-number">100</span></span><br><span class="line">k = <span class="hljs-number">4</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 建立索引, 定义为dimension d = 128</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 使用欧式距离 L2 建立索引。</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">## xb: (100000,128)</span></span><br><span class="line">index.train(xb) </span><br><span class="line">index.add(xb)                </span><br><span class="line">index.nprobe = <span class="hljs-number">10</span>  <span class="hljs-comment"># 默认 nprobe 是 1 ,可以设置的大一些试试</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line"><span class="hljs-built_in">print</span>(I[-<span class="hljs-number">5</span>:])   <span class="hljs-comment"># 最后五次查询的结果</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="indexivfpq"><strong>3. IndexIVFPQ</strong></h3>
<p>IndexFlatL2 和
IndexIVFFlat都要存储所有的向量数据。对于超大规模数据集来说，可能会不大现实。因此IndexIVFPQ
索引可以用来压缩向量，具体的压缩算法就是
Product-Quantization，注意，由于高维向量被压缩，因此 <code>search</code>
时候返回也是近似的结果。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line">nlist = <span class="hljs-number">100</span></span><br><span class="line"><span class="hljs-comment"># 每个向量分8段</span></span><br><span class="line">m = <span class="hljs-number">8</span> </span><br><span class="line"><span class="hljs-comment"># 求4-近邻</span></span><br><span class="line">k = <span class="hljs-number">4</span> </span><br><span class="line">quantizer = faiss.IndexFlatL2(d)    <span class="hljs-comment"># 内部的索引方式依然不变</span></span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="hljs-number">8</span>) <span class="hljs-comment"># 每个向量都被编码为8个字节大小</span></span><br><span class="line">index.train(xb)</span><br><span class="line">index.add(xb)</span><br><span class="line">index.nprobe = <span class="hljs-number">10</span>                </span><br><span class="line">D, I = index.search(xq, k)  <span class="hljs-comment"># 检索</span></span><br><span class="line"><span class="hljs-built_in">print</span>(I[-<span class="hljs-number">5</span>:])</span><br></pre></td></tr></tbody></table></figure>
<p>在本期中，我们仅使用基本的 IndexIVFFlat 和 IndexFlatIP 完成 bert
embedding 的索引和搜索，后续会有篇幅来解读 Product-Quantization
的论文原理和代码实践。</p>
<h2 id="ag_news-新闻数据集">ag_news 新闻数据集</h2>
<p>ag_news 新闻数据集 3.0 包含了英语新闻标题，training 部分包含
120000条数据， test 部分包含 7600条数据。</p>
<p>ag_news 可以通过 huggingface datasets API 自动下载</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_dataset</span>(<span class="hljs-params">part=<span class="hljs-string">'test'</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:</span></span><br><span class="line">    ds = datasets.load_dataset(<span class="hljs-string">"ag_news"</span>)</span><br><span class="line">    list_str = [r[<span class="hljs-string">'text'</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> ds[part]]</span><br><span class="line">    <span class="hljs-keyword">return</span> list_str</span><br><span class="line">    </span><br><span class="line">list_str = load_dataset(part=<span class="hljs-string">'train'</span>)</span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{<span class="hljs-built_in">len</span>(list_str)}</span>'</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> list_str[:<span class="hljs-number">3</span>]:</span><br><span class="line">    <span class="hljs-built_in">print</span>(s)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">'\n'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>显示前三条新闻标题为</p>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">120000</span><br><span class="line">Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\band of ultra-cynics, are seeing green again.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\which has a reputation for making well-timed and occasionally\controversial plays in the defense industry, has quietly placed\its bets on another part of the market.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\about the economy and the outlook for earnings are expected to\hang over the stock market next week during the depth of the\summer doldrums.</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="sentence-transformer">sentence-transformer</h2>
<p>和上一期一样，我们利用<code>sentence-transformer</code>
生成句子级别的embedding。其原理基于 Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks
（https://arxiv.org/abs/1908.10084）这篇论文。基本思想很直接，将句子中的每个词的
bert embedding
，输进入一个池化层(pooling)，例如选择最简单的平均池化层，将所有token
embedding 的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence
embedding。</p>
<p><img src="/zh/2022/docker-faiss-transformer/model.png"></p>
<h2 id="结果展示">结果展示</h2>
<p>数据集 train 部分由于包含的样本比较多，需要一段时间生成 bert
embedding，大家可以使用 <code>load_dataset(part='test')</code>
来快速体验。下面我们演示一个查询 how to make money 的最接近结果。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = load_index(<span class="hljs-string">'news_train.index'</span>)</span><br><span class="line">list_id = query(model, index, <span class="hljs-string">'how to make money'</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> list_id:</span><br><span class="line">    <span class="hljs-built_in">print</span>(list_str[<span class="hljs-built_in">id</span>])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Profit From That Traffic Ticket Got a traffic ticket? Can't beat 'em? Join 'em by investing in the company that processes those tickets.</span><br><span class="line"></span><br><span class="line">Answers in the Margins By just looking at operating margins, investors can find profitable industry leaders.</span><br><span class="line"></span><br><span class="line">Types of Investors: Which Are You? Learn a little about yourself, and it may improve your performance.</span><br><span class="line"></span><br><span class="line">Target Can Aim High Target can maintain its discount image while offering pricier services and merchandise.</span><br><span class="line"></span><br><span class="line">Finance moves Ford into the black US carmaker Ford Motor returns to profit, as the money it makes from lending to customers outweighs losses from selling vehicles.</span><br></pre></td></tr></tbody></table></figure>
<h2 id="核心代码">核心代码</h2>
<p>所有可运行代码和数据都已经包含在 docker
镜像中了，下面列出核心代码</p>
<h3 id="建立索引">建立索引</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_flat</span>(<span class="hljs-params">index_name, id_list, embedding_list, num_clusters</span>):</span></span><br><span class="line">    <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">    <span class="hljs-keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line">    dim = <span class="hljs-number">768</span></span><br><span class="line">    m = <span class="hljs-number">16</span></span><br><span class="line">    </span><br><span class="line">    embeddings = np.asarray(embedding_list)</span><br><span class="line">    </span><br><span class="line">    quantiser = faiss.IndexFlatIP(dim)</span><br><span class="line">    index = faiss.IndexIVFFlat(quantiser, dim, num_clusters, faiss.METRIC_INNER_PRODUCT)</span><br><span class="line">    index.train(embeddings)  <span class="hljs-comment">## clustering</span></span><br><span class="line">    </span><br><span class="line">    ids = np.arange(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(id_list))</span><br><span class="line">    ids = np.asarray(ids.astype(<span class="hljs-string">'int64'</span>))</span><br><span class="line">    </span><br><span class="line">    index.add_with_ids(embeddings, ids)</span><br><span class="line">    <span class="hljs-built_in">print</span>(index.is_trained) </span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total Number of Embeddings in the index"</span>, index.ntotal)</span><br><span class="line">    faiss.write_index(index, index_name)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查询结果">查询结果</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">query</span>(<span class="hljs-params">model, index, query_str: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:</span></span><br><span class="line">    topk = <span class="hljs-number">5</span></span><br><span class="line">    q_embed = model.encode([query_str])</span><br><span class="line">    D, I = index.search(q_embed, topk)</span><br><span class="line">    <span class="hljs-built_in">print</span>(D)</span><br><span class="line">    <span class="hljs-built_in">print</span>(I)</span><br><span class="line">    <span class="hljs-keyword">return</span> I[<span class="hljs-number">0</span>].tolist()</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-sentence-transformer-chinese/" itemprop="url">Bert 中文短句相似度计算 Docker CPU镜像</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-06-17T18:45:01.000Z" itemprop="datePublished">6月 18 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 分钟 读完 (约 1576 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们还是制作了一个集数据，模型，代码一体的 docker
环境，给大家开箱即用体验中文BERT句子embedding体验。具体地，我们基于
<code>BERT-wwm-ext</code>，<code>huggingface transformer</code> 和
<code>sentence-transformer</code> 把玩中文句子embedding
并寻找和查询短语相似度最接近的句子。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>本期 docker 镜像获取方式为，关注 <code>MyEncyclopedia</code>
公众号后回复 <code>docker-sentence-transformer</code>
即可获取镜像地址和启动命令。</p>
<h2 id="哈工大讯飞中文-bert">哈工大讯飞中文 Bert</h2>
<p>在中文预训练领域，哈工大讯飞联合实验室（HFL）发布的基于全词Mask的中文预训练模型
<code>BERT-wwm-ext</code> 是业界的标杆之一。<code>BERT-wwm-ext</code>
支持 <code>Tensorflow</code>, <code>Pytorch</code> （通过
<code>huggingface transformer</code> 接口）以及 <code>PaddleHub</code>
的接口或者类库，使用起来十分方便。下面的代码为官网中通过
<code>huggingface transformer</code> 接口直接下载并加载到
<code>Pytorch</code> 平台中。Github 地址为
https://github.com/ymcui/Chinese-BERT-wwm</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line">model_name = <span class="hljs-string">'hfl/chinese-bert-wwm'</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure>
<p>通过 <code>huggingface transformer</code> 的好处在于
<code>sentence-transformer</code> 也支持
<code>huggingface</code>，因此，通过
<code>huggingface</code>，我们无需手动串联 <code>BERT-wwm-ext</code> 和
<code>sentence-transformer</code>，少写了不少代码。</p>
<h2 id="sentence-transformer">sentence-transformer</h2>
<p><code>sentence-transformer</code> 顾名思义是利用
<code>transformer</code>
词向量的预训练模型来生成句子级别的embedding。原理基于这篇论文
Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
（https://arxiv.org/abs/1908.10084）。基本思想直接了当：将句子中的每个词经
bert embedding 后，输入池化层
(pooling)，例如选择最简单的平均池化层，再将所有token embedding
的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence
embedding。</p>
<p><img src="/zh/2022/docker-sentence-transformer-chinese/model.png"></p>
<p>下面的代码是其官网的一个基本例子，底层通过 <code>huggingface</code>
接口自动下载并加载 bert 词向量，并计算三句英语句子的 sentence
embedding。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="hljs-string">'paraphrase-MiniLM-L6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Our sentences we like to encode</span></span><br><span class="line">sentences = [<span class="hljs-string">'This framework generates embeddings for each input sentence'</span>,</span><br><span class="line">    <span class="hljs-string">'Sentences are passed as a list of string.'</span>,</span><br><span class="line">    <span class="hljs-string">'The quick brown fox jumps over the lazy dog.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Sentences are encoded by calling model.encode()</span></span><br><span class="line">embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Print the embeddings</span></span><br><span class="line"><span class="hljs-keyword">for</span> sentence, embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sentences, embeddings):</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Sentence:"</span>, sentence)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Embedding:"</span>, embedding)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">""</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>当然，我们也可以绕过 <code>sentence-transformer</code> API，直接使用
<code>pytorch</code> API 和 <code>huggingface</code>
手动实现平均池化层，生成句子的 sentence embedding。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Mean Pooling - Take attention mask into account for correct averaging</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mean_pooling</span>(<span class="hljs-params">model_output, attention_mask</span>):</span></span><br><span class="line">    token_embeddings = model_output[<span class="hljs-number">0</span>] <span class="hljs-comment">#First element of model_output contains all token embeddings</span></span><br><span class="line">    input_mask_expanded = attention_mask.unsqueeze(-<span class="hljs-number">1</span>).expand(token_embeddings.size()).<span class="hljs-built_in">float</span>()</span><br><span class="line">    sum_embeddings = torch.<span class="hljs-built_in">sum</span>(token_embeddings * input_mask_expanded, <span class="hljs-number">1</span>)</span><br><span class="line">    sum_mask = torch.clamp(input_mask_expanded.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>), <span class="hljs-built_in">min</span>=<span class="hljs-number">1e-9</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> sum_embeddings / sum_mask</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Sentences we want sentence embeddings for</span></span><br><span class="line">sentences = [<span class="hljs-string">'This framework generates embeddings for each input sentence'</span>,</span><br><span class="line">             <span class="hljs-string">'Sentences are passed as a list of string.'</span>,</span><br><span class="line">             <span class="hljs-string">'The quick brown fox jumps over the lazy dog.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Load AutoModel from huggingface model repository</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="hljs-string">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Tokenize sentences</span></span><br><span class="line">encoded_input = tokenizer(sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">128</span>, return_tensors=<span class="hljs-string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Compute token embeddings</span></span><br><span class="line"><span class="hljs-keyword">with</span> torch.no_grad():</span><br><span class="line">    model_output = model(**encoded_input)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Perform pooling. In this case, mean pooling</span></span><br><span class="line">sentence_embeddings = mean_pooling(model_output, encoded_input[<span class="hljs-string">'attention_mask'</span>])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="中文最相近的句子">中文最相近的句子</h2>
<p>有了上面每个组件的使用方法，让我们生成下面中文句子的embedding</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    <span class="hljs-string">'今天晚上想吃牛排'</span>,</span><br><span class="line">    <span class="hljs-string">'MyEncyclopedia公众号全栈人工智能'</span>,</span><br><span class="line">    <span class="hljs-string">'人工智能需要懂很多数学么'</span>,</span><br><span class="line">    <span class="hljs-string">'上海疫情有完没完'</span>,</span><br><span class="line">    <span class="hljs-string">'教育部：连续7天社会面无疫情 高校可组织校园招聘'</span>,</span><br><span class="line">    <span class="hljs-string">'福建舰"下水！100秒看中国航母高光时刻'</span>,</span><br><span class="line">    <span class="hljs-string">'医保承担多少核酸检测费用？压力多大？'</span>,</span><br><span class="line">    <span class="hljs-string">'张家口过度防疫整改后又被曝光：要证明牛是阴性'</span>,</span><br><span class="line">    <span class="hljs-string">'上海多家银行天天排队爆满 有老人凌晨2点开始排队'</span>,</span><br><span class="line">    <span class="hljs-string">'A股不惧海外暴跌！走出独立行情沪指收复3300点'</span>,</span><br><span class="line">    <span class="hljs-string">'俄方称已准备好重启俄乌和谈'</span>,</span><br><span class="line">    <span class="hljs-string">'《自然》：奥密克戎感染后嗅觉丧失症状比原来少了'</span></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>接着我们给出如下三个短语的查询，找到和每个查询最匹配的三个句子
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q1 = <span class="hljs-string">'码农的春天来了么'</span></span><br><span class="line">q2 = <span class="hljs-string">'国际局势'</span></span><br><span class="line">q3 = <span class="hljs-string">'健康'</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>运行结果如下</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Query: 码农的春天来了么</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">人工智能需要懂很多数学么 (Cosine Score: 0.7606)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.7498)</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.7449)</span><br><span class="line"></span><br><span class="line">----------------------------------------------</span><br><span class="line">Query: 国际局势</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">俄方称已准备好重启俄乌和谈 (Cosine Score: 0.7041)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.6897)</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.6888)</span><br><span class="line"></span><br><span class="line">----------------------------------------------</span><br><span class="line">Query: 健康</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.5882)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.5870)</span><br><span class="line">今天晚上想吃牛排 (Cosine Score: 0.5815)</span><br></pre></td></tr></tbody></table></figure>
<p>结果发现 <code>上海疫情有完没完</code> 是一切问题的关键。。。</p>
<h2 id="完整代码">完整代码</h2>
<p>附上完整代码</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line">model_name = <span class="hljs-string">'hfl/chinese-bert-wwm'</span></span><br><span class="line">model = SentenceTransformer(model_name)</span><br><span class="line"></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="hljs-string">'今天晚上想吃牛排'</span>,</span><br><span class="line">    <span class="hljs-string">'MyEncyclopedia公众号全栈人工智能'</span>,</span><br><span class="line">    <span class="hljs-string">'人工智能需要懂很多数学么'</span>,</span><br><span class="line">    <span class="hljs-string">'上海疫情有完没完'</span>,</span><br><span class="line">    <span class="hljs-string">'教育部：连续7天社会面无疫情 高校可组织校园招聘'</span>,</span><br><span class="line">    <span class="hljs-string">'福建舰"下水！100秒看中国航母高光时刻'</span>,</span><br><span class="line">    <span class="hljs-string">'医保承担多少核酸检测费用？压力多大？'</span>,</span><br><span class="line">    <span class="hljs-string">'张家口过度防疫整改后又被曝光：要证明牛是阴性'</span>,</span><br><span class="line">    <span class="hljs-string">'上海多家银行天天排队爆满 有老人凌晨2点开始排队'</span>,</span><br><span class="line">    <span class="hljs-string">'A股不惧海外暴跌！走出独立行情沪指收复3300点'</span>,</span><br><span class="line">    <span class="hljs-string">'俄方称已准备好重启俄乌和谈'</span>,</span><br><span class="line">    <span class="hljs-string">'《自然》：奥密克戎感染后嗅觉丧失症状比原来少了'</span></span><br><span class="line">]</span><br><span class="line">sentence_embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line">q1 = <span class="hljs-string">'码农的春天来了么'</span></span><br><span class="line">q2 = <span class="hljs-string">'国际局势'</span></span><br><span class="line">q3 = <span class="hljs-string">'健康'</span></span><br><span class="line">queries = [q1, q2, q3]</span><br><span class="line">query_embeddings = model.encode(queries)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> scipy</span><br><span class="line"></span><br><span class="line">number_top_matches = <span class="hljs-number">3</span></span><br><span class="line"><span class="hljs-keyword">for</span> query, query_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(queries, query_embeddings):</span><br><span class="line">    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, <span class="hljs-string">"cosine"</span>)[<span class="hljs-number">0</span>]</span><br><span class="line">    results = <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(distances)), distances)</span><br><span class="line">    results = <span class="hljs-built_in">sorted</span>(results, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nQuery:"</span>, query)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTop {} most similar sentences in corpus:"</span>.<span class="hljs-built_in">format</span>(number_top_matches))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> idx, distance <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>:number_top_matches]:</span><br><span class="line">        <span class="hljs-built_in">print</span>(sentences[idx].strip(), <span class="hljs-string">"(Cosine Score: %.4f)"</span> % (<span class="hljs-number">1</span>-distance))</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-flair-transformer-zero-shot/" itemprop="url">玩转transformer+flair zero shot 短文本分类：无需翻墙或额外下载模型和数据集的CPU docker镜像</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-06-09T18:45:01.000Z" itemprop="datePublished">6月 10 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            8 分钟 读完 (约 1160 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们来体验两个知名的 NLP 预训练类库 flair 和 transformer
的 zero-shot 短文本分类。所谓zero-shot
的意思是完全不需要数据集来训练，直接掉包解决问题。和以往一样，本期的
docker 镜像已经预装了 flair，transformer，pytorch，jupyter
notebook等<strong>包依赖</strong>，并且还预先下载了 flair 和 transformer
的两个<strong>预训练模型</strong>和 <strong>yahoo
短文本主题数据集</strong>，整个 docker
镜像达到12GB，为了就是让大家无需翻墙下载额外数据或者模型，并且使用CPU就能体验最新的NLP
zero shot 文本分类。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>关注 <code>MyEncyclopedia</code> 公众号后回复
<code>docker-transformer-zero-shot</code>
即可获取镜像地址和启动命令。</p>
<h2 id="flair-zero-shot">Flair zero shot</h2>
<p>先来看一个 flair 短文本 zero shot 短文本分类的例子。下面的代码将句子
<strong>Spain beat Swiss for first Nations League win</strong> 归类到
<strong>politics</strong>,
<strong>sports</strong>，<strong>health</strong> 之一。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> flair.models <span class="hljs-keyword">import</span> TARSClassifier</span><br><span class="line"><span class="hljs-keyword">from</span> flair.data <span class="hljs-keyword">import</span> Sentence</span><br><span class="line"><span class="hljs-keyword">import</span> flair, torch</span><br><span class="line">flair.device = torch.device(<span class="hljs-string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span></span><br><span class="line">tars = TARSClassifier.load(<span class="hljs-string">'tars-base'</span>)</span><br><span class="line">sentence = Sentence(text)</span><br><span class="line">classes = [<span class="hljs-string">'politics'</span>, <span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>]</span><br><span class="line">tars.predict_zero_shot(sentence, classes)</span><br><span class="line"></span><br><span class="line"><span class="hljs-built_in">print</span>(sentence)</span><br><span class="line"><span class="hljs-built_in">print</span>(sentence.to_dict())</span><br></pre></td></tr></tbody></table></figure>
<p>最后两行输出如下，<code>all labels</code> 字段显示概率最高的是
<code>sports</code>类别，达到 0.99+。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Sentence: <span class="hljs-string">"Spain beat Swiss for first Nations League win"</span> → sports (0.9952)</span><br><span class="line">{</span><br><span class="line">  <span class="hljs-string">'text'</span>: <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span>, </span><br><span class="line">  <span class="hljs-string">'all labels'</span>: [{<span class="hljs-string">'value'</span>: <span class="hljs-string">'sports'</span>, <span class="hljs-string">'confidence'</span>: 0.9952359795570374}]</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">注意，在上面的代码中，`flair.device = torch.device(<span class="hljs-string">'cpu'</span>)` 强制使用了 cpu 资源，否则 flair 默认使用 gpu 会报错。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">## Transformer zero shot</span></span><br><span class="line">再来看看大名鼎鼎的 transformer zero shot 的结果。这里使用了默认的 transformer zero shot 分类的模型 Transformer Bart，小伙伴们可以使用其他模型，但是有些不兼容 zero shot 分类。代码如下</span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line">from transformers import pipeline</span><br><span class="line"></span><br><span class="line">text = <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span></span><br><span class="line">classes = [<span class="hljs-string">'politics'</span>, <span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>]</span><br><span class="line">classifier = pipeline(<span class="hljs-string">"zero-shot-classification"</span>, device=-1)</span><br><span class="line">result = classifier(text, classes, multi_label=False)</span><br><span class="line"></span><br><span class="line"><span class="hljs-built_in">print</span>(result)</span><br><span class="line"><span class="hljs-built_in">print</span>(result[<span class="hljs-string">'labels'</span>][0])</span><br></pre></td></tr></tbody></table></figure>
<p>最后两行输出为</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="hljs-string">'sequence'</span>: <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span>, </span><br><span class="line">  <span class="hljs-string">'labels'</span>: [<span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>, <span class="hljs-string">'politics'</span>], </span><br><span class="line">  <span class="hljs-string">'scores'</span>: [0.9476209878921509, 0.03594793379306793, 0.016431059688329697]</span><br><span class="line">}</span><br><span class="line">sports</span><br></pre></td></tr></tbody></table></figure>
<p><code>result</code> 的
<code>labels</code>中会按照最大概率排序输出类别和对应的分数。对于这句句子，也分的相当准确，<code>sports</code>
为 0.94+。</p>
<p>也注意到 <code>pipeline("zero-shot-classification", device=-1)</code>
语句中 <strong>-1</strong> 表示强制使用 cpu。</p>
<h2 id="yahoo-短文本主题数据分类效果">Yahoo 短文本主题数据分类效果</h2>
<p>最后，来看一个真实数据集中这两者的实际效果，<code>yahoo_answers_topics</code>
是
<code>huggingface</code>的一个短文本分类数据集，可以通过以下命令下载并加载</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yahoo = load_dataset(<span class="hljs-string">'yahoo_answers_topics'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>它的具体类别为</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line"><span class="hljs-string">'Society &amp; Culture'</span>, </span><br><span class="line"><span class="hljs-string">'Science &amp; Mathematics'</span>, </span><br><span class="line"><span class="hljs-string">'Health'</span>, </span><br><span class="line"><span class="hljs-string">'Education &amp; Reference'</span>, </span><br><span class="line"><span class="hljs-string">'Computers &amp; Internet'</span>, </span><br><span class="line"><span class="hljs-string">'Sports'</span>, </span><br><span class="line"><span class="hljs-string">'Business &amp; Finance'</span>, </span><br><span class="line"><span class="hljs-string">'Entertainment &amp; Music'</span>, </span><br><span class="line"><span class="hljs-string">'Family &amp; Relationships'</span>, </span><br><span class="line"><span class="hljs-string">'Politics &amp; Government'</span></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>由于数量比较大，这里只取随机的1000个来测试，一些数据点如下</p>
<table>
<colgroup>
<col style="width: 74%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Text</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A Permanent resident of Canada may stay out of Canada 3 years
without losing status.</td>
<td>Politics &amp; Government</td>
</tr>
<tr class="even">
<td>The official major league opening game occurred on April 10, 2006,
as the Cardinals defeated the Milwaukee Brewers 6-4. (Day Game)</td>
<td>Sports</td>
</tr>
<tr class="odd">
<td>Hold down the Command key while dragging and dropping files.</td>
<td>Computers &amp; Internet</td>
</tr>
</tbody>
</table>
<p>接着，对于每条短文本用 flair 和 transformer
来预测类别，最终统计准确率。</p>
<p>结果是 flair 准确率为 <strong>0.275</strong>，Transformer Bart 为
<strong>0.392</strong>，果然 transformer 显著胜出。其实，在
Yahoo数据集上取得 0.3 - 0.4
左右的效果已经不错了，毕竟有十个类别，全随机的准确率是
0.1。如果大家觉得这个效果一般的话，可以试试 tweet
情感分类数据集（具体在下面的链接中），Transformer 能达到惊人的
0.73。</p>
<p>下面附部分代码，完整代码可以从镜像中获得，或者感兴趣的小伙伴也可以访问</p>
<p>https://github.com/nlptown/nlp-notebooks/blob/master/Zero-Shot%20Text%20Classification.ipynb
获取所有五个数据集的代码，不过由于类库版本的关系，部分代码和模型或数据无法兼容，需要自行调试。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_flair</span>(<span class="hljs-params">dataset, default_name=<span class="hljs-string">'neutral'</span></span>):</span></span><br><span class="line">    classifier = TARSClassifier.load(<span class="hljs-string">'tars-base'</span>)</span><br><span class="line">    total, correct = <span class="hljs-number">0</span>, <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> item, gold_label_idx <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">zip</span>(dataset[<span class="hljs-string">"test_texts"</span>], dataset[<span class="hljs-string">"test_labels"</span>]),</span><br><span class="line">                                     total=<span class="hljs-built_in">len</span>(dataset[<span class="hljs-string">"test_texts"</span>])):</span><br><span class="line">        sentence = Sentence(item)</span><br><span class="line">        classifier.predict_zero_shot(sentence, dataset[<span class="hljs-string">"class_names"</span>])</span><br><span class="line">        sorted_labels = <span class="hljs-built_in">sorted</span>(sentence.to_dict()[<span class="hljs-string">'all labels'</span>], key=<span class="hljs-keyword">lambda</span> k: k[<span class="hljs-string">'confidence'</span>], reverse=<span class="hljs-literal">True</span>)</span><br><span class="line">        gold_label = dataset[<span class="hljs-string">"class_names"</span>][gold_label_idx]</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(sorted_labels) &gt; <span class="hljs-number">0</span>:</span><br><span class="line">            predicted_label = sorted_labels[<span class="hljs-number">0</span>][<span class="hljs-string">'value'</span>]</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            predicted_label = default_name</span><br><span class="line">        <span class="hljs-keyword">if</span> predicted_label == gold_label:</span><br><span class="line">            correct += <span class="hljs-number">1</span></span><br><span class="line">        total += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> correct / total</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_huggingface</span>(<span class="hljs-params">dataset</span>):</span></span><br><span class="line">    classifier = pipeline(<span class="hljs-string">"zero-shot-classification"</span>, device=-<span class="hljs-number">1</span>)</span><br><span class="line">    correct = <span class="hljs-number">0</span></span><br><span class="line">    predictions, gold_labels = [], []</span><br><span class="line">    <span class="hljs-keyword">for</span> text, gold_label_idx <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">zip</span>(dataset[<span class="hljs-string">"test_texts"</span>], dataset[<span class="hljs-string">"test_labels"</span>]),</span><br><span class="line">                                     total=<span class="hljs-built_in">len</span>(dataset[<span class="hljs-string">"test_texts"</span>])):</span><br><span class="line"></span><br><span class="line">        result = classifier(text, dataset[<span class="hljs-string">"class_names"</span>], multi_label=<span class="hljs-literal">False</span>)</span><br><span class="line">        predicted_label = result[<span class="hljs-string">'labels'</span>][<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">        gold_label = dataset[<span class="hljs-string">"class_names"</span>][gold_label_idx]</span><br><span class="line"></span><br><span class="line">        predictions.append(predicted_label)</span><br><span class="line">        gold_labels.append(gold_label)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> predicted_label == gold_label:</span><br><span class="line">            correct += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    accuracy = correct / <span class="hljs-built_in">len</span>(predictions)</span><br><span class="line">    <span class="hljs-keyword">return</span> accuracy</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/app-arxiv-mobile/" itemprop="url">手机和微信中完美重排和阅读 Arxiv 论文</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-05-27T18:45:01.000Z" itemprop="datePublished">5月 28 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            3 分钟 读完 (约 502 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>如果你是一个重度论文阅读者，一定会在自己的笔记本上全副武装。但是也许只是偶尔，也会心血来潮地在手机端查看一篇论文，此时，你打开的这篇从
Arxiv
下载的论文，就像下图上半部分所示，时间稍长，便会觉得文字，公式和图片不忍直视。你多么希望在手机端查看PDF，也像图中下半部分一样重新排版来轻松阅读。</p>
<p><img src="/zh/2022/app-arxiv-mobile/mobile-conv-html.jpg"></p>
<h2 id="arxiv-论文手机智能重排和阅读服务">Arxiv
论文手机智能重排和阅读服务</h2>
<p>现在，MyEncyclopedia 推出了智能的手机端重排和阅读 arxiv
论文服务。手机端只需浏览器甚至在<strong>微信</strong>中就能将指定的
arxiv 论文重排成适应移动端的格式，并在线浏览。</p>
<p>具体效果如下</p>
<p><img src="/zh/2022/app-arxiv-mobile/view.gif"></p>
<h3 id="微信操作步骤">微信操作步骤</h3>
<p>下面，以手机微信为例，讲解详细步骤。其他浏览器可以直接点击
<code>https://mydoc.myencyclopedia.top/app</code>，操作步骤类似。</p>
<ul>
<li><p>进入 MyEncyclopedia 公众号，回复 <code>?</code> 点击
<code>AI在线服务</code>。 <img src="/zh/2022/app-arxiv-mobile/op_01_enter_service.jpg"></p></li>
<li><p>拷贝显示的口令</p></li>
</ul>
<p><img src="/zh/2022/app-arxiv-mobile/op_02_get_token.jpg"></p>
<ul>
<li>返回到公众号后回复口令，登录使用服务</li>
</ul>
<p><img src="/zh/2022/app-arxiv-mobile/op_03_enter.jpg"></p>
<ul>
<li>输入 <code>Arxiv ID</code>，点击 <code>Go to Page</code></li>
</ul>
<p><img src="/zh/2022/app-arxiv-mobile/op_04_arxiv_id.jpg"></p>
<ul>
<li>在新页面点击 <code>Convert Html</code></li>
</ul>
<p><img src="/zh/2022/app-arxiv-mobile/op_05_trigger_html.jpg"></p>
<ul>
<li>等待大约十几分钟，再次进入服务页面，此时，若下载转换任务成功，出现如下页面，点击<code>View
Html</code>，即可在微信浏览器中完美查看论文了。</li>
</ul>
<p><img src="/zh/2022/app-arxiv-mobile/op_06_view_html.jpg"></p>
<h3 id="pdf-论文双列切割成单列服务">PDF 论文双列切割成单列服务</h3>
<p>此外，也提供将 arxiv PDF从双列切割成单列服务。即上图中的<code>Convert
PDF</code>
按钮，切割后的PDF文件可以在线浏览，或者下载到手机离线查看。也欢迎大家尝试。</p>
<p><img src="/zh/2022/app-arxiv-mobile/mobile-conv-pdf.jpg"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/bili-youtube/" itemprop="url">油管搬运大学计划纪念B站千粉千赞</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-05-07T18:45:01.000Z" itemprop="datePublished">5月 8 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            4 分钟 读完 (约 583 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>五一期间突然发现我的B站频道一点一滴地攒到<strong>1000个粉丝</strong>和<strong>1000个获赞</strong>了，微不足道的小成就，和很多粉丝过万甚至十几万的大UP主差的很远，不过就自然增长而言，B站上的流量不会像公众号那样随着时间的推移而减少，只要原创内容精良，可以一直发光发热下去，这一点和
<code>Youtube</code> 很类似。</p>
<p><img src="/zh/2022/bili-youtube/1000fan.png"></p>
<h2 id="原创方向">原创方向</h2>
<p>对于未来的原创计划我是很有信心的，会在之前的基础上做好几个方向的内容创造</p>
<ul>
<li>深度学习经典或前沿论文讲解，涉及到强化学习，NLP 和 CV
等通用方向。</li>
<li>可视化基础的数学课程，例如正在制作连载中的
<code>MIT18.06线性代数</code>，还有未来的统计，优化等课程，。同样的，也会在合适的时候开始基础机器学习经典教程，计划中有
<code>Bishop</code> 的 <code>PRML</code> (<code>Pattern Recognition
&amp; Machine Learning</code>)
。这些经典教程都会精雕细琢，通过代码，尤其是 <code>manim</code>
可视化来完成。</li>
<li>机器学习实践系列：用 <code>docker</code>
封装各种机器学习或者深度学习的代码教学实例。</li>
<li>原创 AI 服务，例如目前更新中的 <code>Mark-ME</code> 等。</li>
</ul>
<h2 id="油管搬运大学">油管搬运大学</h2>
<p>此外，平时我会经常从 <code>Youtube</code>
下载最新的各大名校的公开课。之后也会同时搬运一些，来源以
<code>MIT</code>，<code>Stanford</code>，<code>Google</code>，<code>UC
Berkeley</code>
为主，课程类别不限于计算机，AI或者数学，以飨读者，同时我可以释放海量硬盘空间（家里已经有4台电脑，6个移动硬盘，总计20T的容量了
(lll￢ω￢)）。</p>
<p>上次搬运 <code>Microsoft Research</code>
的杰出AI学者炉边雅谈系列时实现了自动提供中英文字幕，效果如下（中文字幕由油管机翻，效果一般般，看看就行）。
<img src="/zh/2022/bili-youtube/msr_distinguish.png"></p>
<p>英文字幕</p>
<p><img src="/zh/2022/bili-youtube/sub_en.png"></p>
<p>中文字幕</p>
<p><img src="/zh/2022/bili-youtube/sub_zh.png"></p>
<p>奇怪的是好像我无法在手机端加载字幕，有知道的同学能否告知一下？</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/markme-0.3.0/" itemprop="url">Markme 0.3.0 发布：支持图片转latex，微信公众号排版，上传下载图片</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-04-29T18:45:01.000Z" itemprop="datePublished">4月 30 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            3 分钟 读完 (约 460 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p><code>Mark-ME</code> 在几个月的开发之后，发布了 v0.3.0
版本。<code>Mark-ME</code> 致力于做最好的跨平台（Win, Mac,
Linux）markdown 编辑器，面向AI辅助写作，一站式多平台发布。超越
<code>Typora</code>，<code>mdnice</code>
等。本版本增加了不少高级的功能，包括</p>
<ul>
<li>导出兼容 <code>mdnice</code> 样式的微信公众号文章</li>
<li><code>mathpix</code> 图片转 <code>latex</code> 代码</li>
<li>通过 <code>PicGo</code> 上传本地图片到图床</li>
<li>下载文字远程图片到本地</li>
</ul>
<p>下载地址：</p>
<p>https://github.com/MyEncyclopedia/Mark-ME/releases/download/v0.3.0/Mark-ME.0.3.0.exe</p>
<h2 id="markdown-显示效果">Markdown 显示效果</h2>
<h3 id="图片显示">1. 图片显示</h3>
<p>注意，本地和远程图片都支持</p>
<p><img src="/zh/2022/markme-0.3.0/func_image.png"></p>
<h3 id="代码高亮">2. 代码高亮</h3>
<p><img src="/zh/2022/markme-0.3.0/func_code.png"></p>
<h3 id="数学公式">3. 数学公式</h3>
<p>除了普通行内公式和块公式，还支持 <code>Typora</code>，
<code>mdnice</code> 不支持的扩展 <code>Mathjax</code>
表达式。例如图中的彩色矩阵。</p>
<p><img src="/zh/2022/markme-0.3.0/func_math.png"></p>
<p>同样的表达式在 <code>Typora</code> 和 <code>mdnice</code>
中均不支持</p>
<p><img src="/zh/2022/markme-0.3.0/typora-math.png"></p>
<p><img src="/zh/2022/markme-0.3.0/mdnice-math.png"></p>
<h3 id="内嵌-html-标签">4. 内嵌 <code>Html</code> 标签</h3>
<p>完美显示内嵌 <code>html</code> 标签。</p>
<p>注意，第二个例子中内嵌了 <code>svg</code> 的 <code>div</code>
标签，渲染结果为 <span class="math inline">\(x^2\)</span>。这个功能很方便将 <code>latex</code>
导出成 <code>svg</code> 后嵌入 <code>markdown</code> 文章中，即保护了
<code>latex</code>
源码，也无需担心多平台的显示效果。后续会加强此功能。</p>
<p><img src="/zh/2022/markme-0.3.0/func_html.png"></p>
<h2 id="兼容-mdnice-样式导出到微信公众号">兼容 <code>mdnice</code>
样式导出到微信公众号</h2>
<p>将 <code>mdnice</code> 的样式填入右边的输入中，完美换皮。</p>
<p><img src="/zh/2022/markme-0.3.0/wechat_style.gif"></p>
<h2 id="mathpix-图片转-latex"><code>Mathpix</code> 图片转
<code>Latex</code></h2>
<p><img src="/zh/2022/markme-0.3.0/mathpix.gif"></p>
<h2 id="下载远程图片到本地">下载远程图片到本地</h2>
<p><img src="/zh/2022/markme-0.3.0/download_img.gif"></p>
<h2 id="通过-picgo-上传本地图片到图床">通过 <code>PicGo</code>
上传本地图片到图床</h2>
<p><img src="/zh/2022/markme-0.3.0/upload_img.gif"></p>
<h2 id="其他">其他</h2>
<h3 id="菜单展示">菜单展示</h3>
<p><img src="/zh/2022/markme-0.3.0/menu.gif"></p>
<h3 id="缩放">缩放</h3>
<p><img src="/zh/2022/markme-0.3.0/zoom.gif"></p>
<h3 id="配置窗口">配置窗口</h3>
<p><img src="/zh/2022/markme-0.3.0/option.png"></p>
<h3 id="注册">注册</h3>
<p>一些高级功能虽然免费，但是需要用户注册
<code>myencyclopedia</code>，以便更好的了解用户。 <img src="/zh/2022/markme-0.3.0/reg.png"></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
        
<nav class="pagination is-centered is-rounded" role="navigation" aria-label="pagination">
    <div class="pagination-previous is-invisible is-hidden-mobile">
        <a href="/page/0/">上一页</a>
    </div>
    <div class="pagination-next">
        <a href="/page/2/">下一页</a>
    </div>
    <ul class="pagination-list is-hidden-mobile">
        
        <li><a class="pagination-link is-current" href="/">1</a></li>
        
        <li><a class="pagination-link" href="/page/2/">2</a></li>
        
        <li><span class="pagination-ellipsis">&hellip;</span></li>
        
        <li><a class="pagination-link" href="/page/6/">6</a></li>
        
    </ul>
</nav>
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>