<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>Tag: PyTorch - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/tags/PyTorch/" rel="alternate" hreflang="zh" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/en/tags/PyTorch/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/en">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/en/archives">Archives</a>
            
            <a class="navbar-item "
               href="/en/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="Search" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#PyTorch</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/en/2020/tsp-5-rl/" itemprop="url">TSP From DP to Deep Learning. Episode 5: Reinforcement Learning</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-06-12T18:45:01.000Z" itemprop="datePublished">Jun 13 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/en/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            13 minutes read (About 1952 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>This is fifth episode of series: TSP From DP to Deep Learning. In
this episode, we turn to Reinforcement Learning technology, in
particular, a model-free policy gradient method that embeds pointer
network to learn minimal tour without supervised best tour label in
dataset. Full list of this series is listed below.</p>
<ul>
<li><p>Episode 1: <a href="/en/2020/tsp-5-rl/!--swig￼3--">AC TSP on AIZU with recursive
DP</a></p></li>
<li><p>Episode 2: <a href="/en/2020/tsp-5-rl/!--swig￼4--">TSP DP on a Euclidean
Dataset</a></p></li>
<li><p>Episode 3: <a href="/en/2020/tsp-5-rl/!--swig￼5--">Pointer Networks in
PyTorch</a></p></li>
<li><p>Episode 4: <a href="/en/2020/tsp-5-rl/!--swig￼6--">Search for Most Likely
Sequence</a></p></li>
<li><p><strong>Episode 5: <a href="/en/2020/tsp-5-rl/!--swig￼7--">Reinforcement Learning
PyTorch Implementation</a></strong></p></li>
</ul>
<h2 id="pointer-network-refresher">Pointer Network Refresher</h2>
<p>In previous episode <a href="/en/2020/tsp-5-rl/!--swig￼8--">Pointer Networks in
PyTorch</a>, we implemented <em>Pointer Networks </em> in PyTorch with a
2D Euclidean dataset.</p>
Recall that the input is a graph as a sequence of <span class="math inline">\(n\)</span> cities in a two dimensional space
<div>
<p><span class="math display">\[
s=\{\mathbf{x_i}\}_{i=1}^n,   \mathbf{x}_{i} \in \mathbb{R}^{2}
\]</span></p>
</div>
<p>The output is a permutation of the points <span class="math inline">\(\pi\)</span>, that visits each city exactly once
and returns to starting point with minimal distance.</p>
<p>Let us define the total distance of a <span class="math inline">\(\pi\)</span> with respect to <span class="math inline">\(s\)</span> as <span class="math inline">\(L\)</span></p>
<div>
<p><span class="math display">\[
L(\pi |
s)=\left\|\mathbf{x}_{\pi(n)}-\mathbf{x}_{\pi(1)}\right\|_{2}+\sum_{i=1}^{n-1}\left\|\mathbf{x}_{\pi(i)}-\mathbf{x}_{\pi(i+1)}\right\|_{2}
\]</span></p>
</div>
<p>The stochastic policy <span class="math inline">\(p(\pi | s;
\theta)\)</span>, parameterized by <span class="math inline">\(\theta\)</span>, is aiming to assign high
probabilities to short tours and low probabilities to long tours. The
joint probability assumes independency to allow factorization.</p>
<p><span class="math display">\[
p(\pi | s; \theta) =
\prod_{i=1}^{n} p\left({\pi(i)} | {\pi(1)}, \ldots, {\pi(i-1)} , s;
\theta\right)
\]</span></p>
<p>The loss of the model is cross entropy between the network’s output
probabilities <span class="math inline">\(\pi\)</span> and the best tour
<span class="math inline">\(\hat{\pi}\)</span> generated by a TSP
solver.</p>
<p>Contribution made by Pointer networks is that it addressed the
constraint in that it allows for dynamic index value given by the
particular test case, instead of from a fixed-size vocabulary.</p>
<h2 id="reinforcement-learning">Reinforcement Learning</h2>
<p><em>Neural Combinatorial Optimization with Reinforcement Learning
</em> combines the power of Reinforcement Learning (RL) and Deep
Learning to further eliminate the constraint required by Pointer
Networks that the training dataset has to have supervised labels of best
tour. With deep RL, test cases do not need to have a solution which is
common pattern in deep RL. In the paper, a model-free policy-based RL
method is adopted.</p>
<h3 id="model-free-policy-gradient-methods">Model-Free Policy Gradient
Methods</h3>
<p>In the authoritative RL book, <em>chapter 8 Planning and Learning
with Tabular Methods</em>, there are two major approaches in RL. One is
model-based RL and the other is model-free RL. Distinction between the
two relies on concept of model, which is stated as follows:</p>
<blockquote>
<p>By a model of the environment we mean anything that an agent can use
to predict how the environment will respond to its actions.</p>
</blockquote>
<p>So model-based methods demand a model of the environment, and hence
dynamic programming and heuristic search fall into this category. With
model in mind, utility of the state can be computed in various ways and
planning stage that essentially builds policy is needed before agent can
take any action. In contrast, model-free methods, without building a
model, are more direct, ignoring irrelevant information and just
focusing on the policy which is ultimately needed. Typical examples of
model-free methods are Monte Carlo Control and Temporal-Difference
Learning. &gt;Model-based methods rely on planning as their primary
component, while model-free methods primarily rely on learning.</p>
<p>In TSP problem, the model is fully determined by all points given,
and no feedback is generated for each decision made. So it's unclear to
how to map state value with a tour. Therefore, we turn to model-free
methods. In <em>chapter 13 Policy Gradient Methods</em>, a particular
approximation model-free method that learns a parameterized policy that
can select actions without consulting a value function. This approach
fits perfectly with aforementioned pointer networks where the
parameterized policy <span class="math inline">\(p(\pi | s;
\theta)\)</span> is already defined.</p>
Training objective is obvious, the expected tour length of <span class="math inline">\(\pi_\theta\)</span> which, given an input graph
<span class="math inline">\(s\)</span>
<div>
<p><span class="math display">\[
J(\theta | s) = \mathbb{E}_{\pi \sim p_{\theta}(\cdot | s)} L(\pi | s)
\]</span></p>
</div>
<h3 id="monte-carlo-policy-gradient-reinforce-with-baseline">Monte Carlo
Policy Gradient: REINFORCE with Baseline</h3>
<p>In order to find largest reward, a typical way is to optimize the
parameters <span class="math inline">\(\theta\)</span> in the direction
of derivative: <span class="math inline">\(\nabla_{\theta} J(\theta |
s)\)</span>.</p>
<p><span class="math display">\[
\nabla_{\theta} J(\theta | s)=\nabla_{\theta} \mathbb{E}_{\pi \sim
p_{\theta}(\cdot | s)} L(\pi | s)
\]</span></p>
<p>RHS of equation above is the derivative of expectation that we have
no idea how to compute or approximate. Here comes the well-known
REINFORCE trick that turns it into form of expectation of derivative,
which can be approximated easily with Monte Carlo sampling, where the
expectation is replaced by averaging.</p>
<p><span class="math display">\[
\nabla_{\theta} J(\theta | s)=\mathbb{E}_{\pi \sim p_{\theta}(. |
s)}\left[L(\pi | s) \nabla_{\theta} \log p_{\theta}(\pi | s)\right]
\]</span></p>
<p>Another common trick, subtracting a baseline <span class="math inline">\(b(s)\)</span>, leads the derivative of reward to
the following equation. Note that <span class="math inline">\(b(s)\)</span> denotes a baseline function that
must not depend on <span class="math inline">\(\pi\)</span>. <span class="math display">\[
\nabla_{\theta} J(\theta | s)=\mathbb{E}_{\pi \sim p_{\theta}(. |
s)}\left[(L(\pi | s)-b(s)) \nabla_{\theta} \log p_{\theta}(\pi |
s)\right]
\]</span></p>
<p>The trick is explained in as:</p>
<blockquote>
<p>Because the baseline could be uniformly zero, this update is a strict
generalization of REINFORCE. In general, the baseline leaves the
expected value of the update unchanged, but it can have a large effect
on its variance.</p>
</blockquote>
<p>Finally, the equation can be approximated with Monte Carlo sampling,
assuming drawing <span class="math inline">\(B\)</span> i.i.d: <span class="math inline">\(s_{1}, s_{2}, \ldots, s_{B} \sim
\mathcal{S}\)</span> and sampling a single tour per graph: $ <em>{i}
p</em>{}(. | s_{i}) $, as follows <span class="math display">\[
\nabla_{\theta} J(\theta) \approx \frac{1}{B}
\sum_{i=1}^{B}\left(L\left(\pi_{i} |
s_{i}\right)-b\left(s_{i}\right)\right) \nabla_{\theta} \log
p_{\theta}\left(\pi_{i} | s_{i}\right)
\]</span></p>
<h3 id="actor-critic-methods">Actor Critic Methods</h3>
<p>REINFORCE with baseline works quite well but it also has
disadvantage.</p>
<blockquote>
<p>REINFORCE with baseline is unbiased and will converge asymptotically
to a local minimum, but like all Monte Carlo methods it tends to learn
slowly (produce estimates of high variance) and to be inconvenient to
implement online or for continuing problems.</p>
</blockquote>
<p>A typical improvement is actor–critic methods, that not only learn
approximate policy, the actor job, but also learn approximate value
funciton, the critic job. This is because it reduces variance and
accelerates learning via a bootstrapping critic that introduce bias
which is often beneficial. Detailed algorithm in the paper illustrated
below.</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Algorithm Actor-critic training} \\
&amp;1: \quad \textbf{ procedure } \text{ TRAIN(training set }S \text{,
training steps }T \text{, batch size } B \text{)} \\
&amp;2: \quad \quad \text{Initialize pointer network params } \theta \\
&amp;3: \quad \quad \text{Initialize critic network params } \theta_{v}
\\
&amp;4: \quad \quad \textbf{for }t=1  \text{ to } T  \textbf{ do }\\
&amp;5: \quad \quad \quad s_{i} \sim \operatorname{SAMPLE INPUT }  (S)
\text{ for } i \in\{1, \ldots, B\} \\
&amp;6: \quad \quad \quad  \pi_{i} \sim \operatorname{SAMPLE SOLUTION }
\left(p_{\theta}\left(\cdot | s_{i}\right)\right) \text{ for } i \in\{1,
\ldots, B\} \\
&amp;7: \quad \quad \quad b_{i} \leftarrow
b_{\theta_{v}}\left(s_{i}\right) \text{ for } i \in\{1, \ldots, B\} \\
&amp;8: \quad \quad \quad g_{\theta} \leftarrow \frac{1}{B}
\sum_{i=1}^{B}\left(L\left(\pi_{i} | s_{i}\right)-b_{i}\right)
\nabla_{\theta} \log p_{\theta}\left(\pi_{i} | s_{i}\right) \\
&amp;9: \quad \quad \quad  \mathcal{L}_{v} \leftarrow \frac{1}{B}
\sum_{i=1}^{B} \left\| b_{i}-L\left(\pi_{i}\right) \right\| _{2}^{2} \\
&amp;10: \quad \quad \quad  \theta \leftarrow \operatorname{ADAM} \left(
\theta, g_{\theta} \right) \\
&amp;11: \quad \quad \quad  \theta_{v} \leftarrow
\operatorname{ADAM}\left(\theta_{v}, \nabla_{\theta_{v}}
\mathcal{L}_{v}\right) \\
&amp;12: \quad \quad \textbf{end for} \\
&amp;13: \quad \textbf{return } \theta  \\
&amp;14: \textbf{end procedure}
\end{align*}
\]</span></p>
</div>
<h2 id="implementation-in-pytorch">Implementation in PyTorch</h2>
<h3 id="beam-search-in-opennmt-py">Beam Search in OpenNMT-py</h3>
<p>In <a href="/en/2020/tsp-5-rl/!--swig￼9--">Episode 4 Search for Most Likely
Sequence</a>, an 3x3 rectangle trellis is given and several decoding
methods are illustrated in plain python. In PyTorch version, there is a
package <strong>OpenNMT-py</strong> that supports efficient batched beam
search. But due to its complicated <strong>BeamSearch</strong> usage,
previous problem is demonstrated using its API. For its details, please
refer to <a target="_blank" rel="noopener" href="https://medium.com/the-artificial-impostor/implementing-beam-search-part-1-4f53482daabe">Implementing
Beam Search — Part 1: A Source Code Analysis of OpenNMT-py</a></p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> deepcopy</span><br><span class="line"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> exp</span><br><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"><span class="hljs-keyword">from</span> onmt.translate <span class="hljs-keyword">import</span> BeamSearch, GNMTGlobalScorer</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_example</span>():</span></span><br><span class="line">    BEAM_SIZE = <span class="hljs-number">2</span></span><br><span class="line">    N_BEST = <span class="hljs-number">1</span></span><br><span class="line">    BATCH_SZ = <span class="hljs-number">1</span></span><br><span class="line">    SEQ_LEN = <span class="hljs-number">3</span></span><br><span class="line"></span><br><span class="line">    initial = [<span class="hljs-number">0.35</span>, <span class="hljs-number">0.25</span>, <span class="hljs-number">0.4</span>]</span><br><span class="line">    transition_matrix = [</span><br><span class="line">        [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.1</span>],</span><br><span class="line">        [<span class="hljs-number">0.4</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.4</span>],</span><br><span class="line">        [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.4</span>]]</span><br><span class="line"></span><br><span class="line">    beam = BeamSearch(BEAM_SIZE, BATCH_SZ, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, N_BEST, GNMTGlobalScorer(<span class="hljs-number">0.7</span>, <span class="hljs-number">0.</span>, <span class="hljs-string">"avg"</span>, <span class="hljs-string">"none"</span>), <span class="hljs-number">0</span>, <span class="hljs-number">30</span>, <span class="hljs-literal">False</span>, <span class="hljs-number">0</span>, <span class="hljs-built_in">set</span>(), <span class="hljs-literal">False</span>, <span class="hljs-number">0.</span>)</span><br><span class="line">    device_init = torch.zeros(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">    beam.initialize(device_init, torch.randint(<span class="hljs-number">0</span>, <span class="hljs-number">30</span>, (BATCH_SZ,)))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">printBestNPaths</span>(<span class="hljs-params">beam: BeamSearch, step: <span class="hljs-built_in">int</span></span>):</span></span><br><span class="line">        <span class="hljs-built_in">print</span>(<span class="hljs-string">f'\nstep <span class="hljs-subst">{step}</span> beam results:'</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(BEAM_SIZE):</span><br><span class="line">            best_path = beam.alive_seq[k].squeeze().tolist()[<span class="hljs-number">1</span>:]</span><br><span class="line">            prob = exp(beam.topk_log_probs[<span class="hljs-number">0</span>][k])</span><br><span class="line">            <span class="hljs-built_in">print</span>(<span class="hljs-string">f'prob <span class="hljs-subst">{prob:<span class="hljs-number">.3</span>f}</span> with path <span class="hljs-subst">{best_path}</span>'</span>)</span><br><span class="line"></span><br><span class="line">    init_scores = torch.log(torch.tensor([initial], dtype=torch.<span class="hljs-built_in">float</span>))</span><br><span class="line">    init_scores = deepcopy(init_scores.repeat(BATCH_SZ * BEAM_SIZE, <span class="hljs-number">1</span>))</span><br><span class="line">    beam.advance(init_scores, <span class="hljs-literal">None</span>)</span><br><span class="line">    printBestNPaths(beam, <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(SEQ_LEN - <span class="hljs-number">1</span>):</span><br><span class="line">        idx_list = beam.topk_ids.squeeze().tolist()</span><br><span class="line">        beam_transition = []</span><br><span class="line">        <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> idx_list:</span><br><span class="line">            beam_transition.append(transition_matrix[idx])</span><br><span class="line">        beam_transition_tensor = torch.log(torch.tensor(beam_transition))</span><br><span class="line"></span><br><span class="line">        beam.advance(beam_transition_tensor, <span class="hljs-literal">None</span>)</span><br><span class="line">        beam.update_finished()</span><br><span class="line"></span><br><span class="line">        printBestNPaths(beam, step + <span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>The output is as follows. When <span class="math inline">\(k=2\)</span> and 3 steps, the most likely sequence
is <span class="math inline">\(0 \rightarrow 1 \rightarrow 0\)</span>,
whose probability is 0.084. </p><figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">step 0 beam results:</span><br><span class="line">prob 0.400 with path [2]</span><br><span class="line">prob 0.350 with path [0]</span><br><span class="line"></span><br><span class="line">step 1 beam results:</span><br><span class="line">prob 0.210 with path [0, 1]</span><br><span class="line">prob 0.160 with path [2, 1]</span><br><span class="line"></span><br><span class="line">step 2 beam results:</span><br><span class="line">prob 0.084 with path [0, 1, 0]</span><br><span class="line">prob 0.000 with path [0, 1, 2]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="rl-with-pointernetwork">RL with PointerNetwork</h3>
<p>The complete code is on <a target="_blank" rel="noopener" href="https://github.com/MyEncyclopedia/blog/blob/master/tsp/rl_pytorch/TSP_RL_main.py">github
TSP RL</a>. Below are partial core classes.</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CombinatorialRL</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">    actor: PointerNet</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, rnn_type, use_embedding, embedding_size, hidden_size, seq_len, num_glimpse, tanh_exploration, use_tanh, attention</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(CombinatorialRL, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.actor = PointerNet(rnn_type, use_embedding, embedding_size, hidden_size, seq_len, num_glimpse, tanh_exploration, use_tanh, attention)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, batch_input: Tensor</span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, <span class="hljs-type">List</span>[Tensor], <span class="hljs-type">List</span>[Tensor], <span class="hljs-type">List</span>[Tensor]]:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Args:</span></span><br><span class="line"><span class="hljs-string">            batch_input: [batch_size * 2 * seq_len]</span></span><br><span class="line"><span class="hljs-string">        Returns:</span></span><br><span class="line"><span class="hljs-string">            R: Tensor of shape [batch_size]</span></span><br><span class="line"><span class="hljs-string">            action_prob_list: List of [seq_len], tensor shape [batch_size]</span></span><br><span class="line"><span class="hljs-string">            action_list:      List of [seq_len], tensor shape [batch_size * 2]</span></span><br><span class="line"><span class="hljs-string">            action_idx_list:  List of [seq_len], tensor shape [batch_size]</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        batch_size = batch_input.size(<span class="hljs-number">0</span>)</span><br><span class="line">        seq_len = batch_input.size(<span class="hljs-number">2</span>)</span><br><span class="line">        prob_list, action_idx_list = self.actor(batch_input)</span><br><span class="line"></span><br><span class="line">        action_list = []</span><br><span class="line">        batch_input = batch_input.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)</span><br><span class="line">        <span class="hljs-keyword">for</span> action_id <span class="hljs-keyword">in</span> action_idx_list:</span><br><span class="line">            action_list.append(batch_input[[x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)], action_id.data, :])</span><br><span class="line">        action_prob_list = []</span><br><span class="line">        <span class="hljs-keyword">for</span> prob, action_id <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prob_list, action_idx_list):</span><br><span class="line">            action_prob_list.append(prob[[x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size)], action_id.data])</span><br><span class="line"></span><br><span class="line">        R = self.reward(action_list)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> R, action_prob_list, action_list, action_idx_list</span><br><span class="line">        </span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reward</span>(<span class="hljs-params">self, sample_solution: <span class="hljs-type">List</span>[Tensor]</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Computes total distance of tour</span></span><br><span class="line"><span class="hljs-string">        Args:</span></span><br><span class="line"><span class="hljs-string">            sample_solution: list of size N, each tensor of shape [batch_size * 2]</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">        Returns:</span></span><br><span class="line"><span class="hljs-string">            tour_len: [batch_size]</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        batch_size = sample_solution[<span class="hljs-number">0</span>].size(<span class="hljs-number">0</span>)</span><br><span class="line">        n = <span class="hljs-built_in">len</span>(sample_solution)</span><br><span class="line">        tour_len = Variable(torch.zeros([batch_size]))</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - <span class="hljs-number">1</span>):</span><br><span class="line">            tour_len += torch.norm(sample_solution[i] - sample_solution[i + <span class="hljs-number">1</span>], dim=<span class="hljs-number">1</span>)</span><br><span class="line">        tour_len += torch.norm(sample_solution[n - <span class="hljs-number">1</span>] - sample_solution[<span class="hljs-number">0</span>], dim=<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> tour_len</span><br></pre></td></tr></tbody></table></figure>
<h2 id="references">References</h2>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/en/2020/tsp-3-pointer-net/" itemprop="url">TSP From DP to Deep Learning. Episode 3: Pointer Network</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-05-22T06:45:01.000Z" itemprop="datePublished">May 22 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/en/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            9 minutes read (About 1349 words)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>This is third episode of series: TSP From DP to Deep Learning. In
this episode, we will be entering the realm of deep learning,
specifically, a type of sequence-to-sequence called Pointer Networks is
introduced. It is tailored to solve problems like TSP or Convex Hull.
Full list of this series is listed below.</p>
<ul>
<li><p>Episode 1: <a href="/en/2020/tsp-3-pointer-net/!--swig￼4--">AC TSP on AIZU with recursive
DP</a></p></li>
<li><p>Episode 2: <a href="/en/2020/tsp-3-pointer-net/!--swig￼5--">TSP DP on a Euclidean
Dataset</a></p></li>
<li><p><strong>Episode 3: <a href="/en/2020/tsp-3-pointer-net/!--swig￼6--">Pointer Networks in
PyTorch</a></strong></p></li>
<li><p>Episode 4: <a href="/en/2020/tsp-3-pointer-net/!--swig￼7--">Search for Most Likely
Sequence</a></p></li>
<li><p>Episode 5: <a href="/en/2020/tsp-3-pointer-net/!--swig￼8--">Reinforcement Learning PyTorch
Implementation</a></p></li>
</ul>
<h2 id="pointer-networks">Pointer Networks</h2>
<p>In traditional sequence-to-sequence RNN, output classes depend on
pre-defined size. For instance, a word generating RNN will utter one
word from vocabulary of <span class="math inline">\(|V|\)</span> size at
each time step. However, there is large set of problems such as Convex
Hull, Delaunay Triangulation and TSP, where range of the each output is
not pre-defined, but of variable size, defined by the input. <em>Pointer
Networks </em> overcame the constraint by selecting <span class="math inline">\(i\)</span> -th input with probability derived from
attention score.</p>
<h3 id="convex-hull">Convex Hull</h3>
<p>In following example, 10 points are given, the output is a sequence
of points that bounds the set of all points. Each value in the output
sequence is a integer ranging from 1 to 10, in this case, which is the
value given by the concrete example. Generally, finding exact solution
has been proven to be equivelent to sort problem, and has time
complexity <span class="math inline">\(O(n*log(n))\)</span>.</p>
<figure>
<img src="/en/2020/tsp-3-pointer-net/./convex_hull.png" alt="image info">
<figcaption aria-hidden="true">image info</figcaption>
</figure>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\text{Input:  } \mathcal{P} &amp;=&amp; \left\{P_{1}, \ldots,
P_{10} \right\} \\
&amp;\text{Output:  } C^{\mathcal{P}} &amp;=&amp; \{2,4,3,5,6,7,2\}
\end{align*}
\]</span></p>
</div>
<h3 id="tsp">TSP</h3>
<p>TSP is almost identical to Convex Hull problem, though output
sequence is of fixed length. In previous epsiode, we reduced from <span class="math inline">\(O(n!)\)</span> to <span class="math inline">\(O(n^2*2^n)\)</span>.</p>
<figure>
<img src="/en/2020/tsp-3-pointer-net/./tsp.svg" alt="image info">
<figcaption aria-hidden="true">image info</figcaption>
</figure>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\text{Input:  } \mathcal{P} &amp;= &amp;\left\{P_{1}, \ldots, P_{6}
\right\} \\
&amp;\text{Output:  } C^{\mathcal{P}} &amp;=&amp; \{1,3,2,4,5,6,1\}
\end{align*}
\]</span></p>
</div>
<h3 id="delaunay-triangulation">Delaunay Triangulation</h3>
<p>A Delaunay triangulation for a set of points in a plane is a
triangulation such that each circumcircle of every triangle is empty,
meaning no point from <span class="math inline">\(\mathcal{P}\)</span>
in its interior. This kind of problem outputs a sequence of sets, and
each item in the set ranges from the input set <span class="math inline">\(\mathcal{P}\)</span>. <img src="/en/2020/tsp-3-pointer-net/./delaunay_triangulation.png" alt="image info"></p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\text{Input:  } \mathcal{P} &amp;=&amp; \left\{P_{1}, \ldots, P_{5}
\right\} \\
&amp;\text{Output:  } C^{\mathcal{P}} &amp;=&amp;
\{(1,2,4),(1,4,5),(1,3,5),(1,2,3)\}
\end{align*}
\]</span></p>
</div>
<h3 id="sequence-to-sequence-model">Sequence-to-Sequence Model</h3>
<p>Suppose now n is fixed. given a training pair, <span class="math inline">\((\mathcal{P}, C^{\mathcal{P}})\)</span>, the
vanilla sequence-to-sequence model parameterized by <span class="math inline">\(\theta\)</span> computes the conditional
probability.</p>
<p><span class="math display">\[
\begin{equation}
p\left(\mathcal{C}^{\mathcal{P}} | \mathcal{P} ;
\theta\right)=\prod_{i=1}^{m(\mathcal{P})} p\left(C_{i} | C_{1}, \ldots,
C_{i-1}, \mathcal{P} ; \theta\right)
\end{equation}
\]</span> The parameters of the model are learnt by maximizing the
conditional probabilities for the training set, i.e. <span class="math display">\[
\begin{equation}
\theta^{*}=\underset{\theta}{\arg \max } \sum_{\mathcal{P},
\mathcal{C}^{\mathcal{P}}} \log p\left(\mathcal{C}^{\mathcal{P}} |
\mathcal{P} ; \theta\right)
\end{equation}
\]</span></p>
<h3 id="content-based-input-attention">Content Based Input
Attention</h3>
<p>When attention is applied to vanilla sequence-to-sequence model,
better result is obtained.</p>
<p>Let encoder and decoder states be $ (e_{1}, , e_{n}) $ and $ (d_{1},
, d_{m()}) $, respectively. At each output time <span class="math inline">\(i\)</span>, compute the attention vector <span class="math inline">\(d_i\)</span> to be linear combination of $ (e_{1},
, e_{n}) $ with weights $ (a_{1}^{i}, , a_{n}^{i}) $ <span class="math display">\[
d_{i} = \sum_{j=1}^{n} a_{j}^{i} e_{j}
\]</span></p>
$ (a_{1}^{i}, , a_{n}^{i}) $ is softmax value of $ (u_{1}^{i}, ,
u_{n}^{i}) $ and <span class="math inline">\(u_{j}^{i}\)</span> can be
considered as distance between <span class="math inline">\(d_{i}\)</span> and <span class="math inline">\(e_{j}\)</span>. Notice that <span class="math inline">\(v\)</span>, <span class="math inline">\(W_1\)</span>, and <span class="math inline">\(W_2\)</span> are learnable parameters of the
model.
<div>
<p><span class="math display">\[
\begin{eqnarray}
u_{j}^{i} &amp;=&amp; v^{T} \tanh \left(W_{1} e_{j}+W_{2}
d_\right) \quad j \in(1, \ldots, n) \\
a_{j}^{i} &amp;=&amp; \operatorname{softmax}\left(u_{j}^{i}\right) \quad
j \in(1, \ldots, n)
\end{eqnarray}
\]</span></p>
</div>
<h3 id="pointer-networks-1">Pointer Networks</h3>
<figure>
<img src="/en/2020/tsp-3-pointer-net/./ptr_net.png" alt="image info">
<figcaption aria-hidden="true">image info</figcaption>
</figure>
<p>Pointer Networks does not blend the encoder state <span class="math inline">\(e_j\)</span> to propagate extra information to the
decoder, but instead, use <span class="math inline">\(u^i_j\)</span> as
pointers to the input element.</p>
<div>
<p><span class="math display">\[
\begin{eqnarray*}
u_{j}^{i} &amp;=&amp; v^{T} \tanh \left(W_{1} e_{j}+W_{2} d_{i}\right)
\quad j \in(1, \ldots, n) \\
p\left(C_{i} | C_{1}, \ldots, C_{i-1}, \mathcal{P}\right) &amp;=&amp;
\operatorname{softmax}\left(u^{i}\right)
\end{eqnarray*}
\]</span></p>
</div>
<h2 id="more-on-attention">More on Attention</h2>
<p>In <em>FloydHub Blog - Attention Mechanism </em>, a clear and
detailed explanation of difference and similarity between the classic
first type of Attention, commonly referred to as Additive Attention by
<em>Dzmitry Bahdanau </em> and second classic type, known as
Multiplicative Attention and proposed by <em>Thang Luong </em>, is
discussed.</p>
It's well known that in Luong Attention, three ways of alignment scoring
function is defined, or the distance between <span class="math inline">\(d_{i}\)</span> and <span class="math inline">\(e_{j}\)</span>.
<div>
<p><span class="math display">\[
\operatorname{score} \left( d_i, e_j \right)=
\begin{cases}
d_i^{\top} e_j &amp; \text { dot } \\
d_i^{\top} W_a e_j &amp; \text { general } \\
v_a^{\top} \tanh \left( W_a \left[ d_i ; e_j \right] \right) &amp; \text
{ concat }
\end{cases}
\]</span></p>
</div>
<h2 id="pytorch-implementation">PyTorch Implementation</h2>
<p>In <a href="/en/2020/tsp-3-pointer-net/!--swig￼10--">episode 2</a>, we have introduced TSP
dataset where each case is a line, of following form.</p>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x0, y0, x1, y1, ... output 1 v1 v2 v3 ... 1</span><br></pre></td></tr></tbody></table></figure>
<h3 id="pytorch-dataset">PyTorch Dataset</h3>
<p>Each case is converted to (input, input_len, output_in, output_out,
output_len) of type nd.ndarray with appropriate padding and encapsulated
in a extended PyTorch Dataset. </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TSPDataset</span>(<span class="hljs-params">Dataset</span>):</span></span><br><span class="line">	<span class="hljs-string">"each data item of form (input, input_len, output_in, output_out, output_len)"</span></span><br><span class="line">	data: <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]]</span><br><span class="line">	</span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">		<span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span>(<span class="hljs-params">self, index</span>):</span></span><br><span class="line">		<span class="hljs-built_in">input</span>, input_len, output_in, output_out, output_len = self.data[index]</span><br><span class="line">		<span class="hljs-keyword">return</span> <span class="hljs-built_in">input</span>, input_len, output_in, output_out, output_len</span><br></pre></td></tr></tbody></table></figure> <img src="/en/2020/tsp-3-pointer-net/./data_loader.svg" alt="image info"><p></p>
<h3 id="pytorch-pad_packed_sequence">PyTorch pad_packed_sequence</h3>
<p>Code in PyTorch seq-to-seq model typically utilizes
<strong>pack_padded_sequence</strong> and
<strong>pad_packed_sequence</strong> API to reduce computational cost. A
detailed explanation is given here
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#decoder-1.</p>
<figure>
<img src="/en/2020/tsp-3-pointer-net/./ex_padded_seq.jpg" alt="image info">
<figcaption aria-hidden="true">image info</figcaption>
</figure>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RNNEncoder</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">	rnn: <span class="hljs-type">Union</span>[nn.LSTM, nn.GRU, nn.RNN]</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, rnn_type: <span class="hljs-built_in">str</span>, bidirectional: <span class="hljs-built_in">bool</span>, num_layers: <span class="hljs-built_in">int</span>, input_size: <span class="hljs-built_in">int</span>, hidden_size: <span class="hljs-built_in">int</span>, dropout: <span class="hljs-built_in">float</span></span>):</span></span><br><span class="line">		<span class="hljs-built_in">super</span>(RNNEncoder, self).__init__()</span><br><span class="line">		<span class="hljs-keyword">if</span> bidirectional:</span><br><span class="line">			<span class="hljs-keyword">assert</span> hidden_size % <span class="hljs-number">2</span> == <span class="hljs-number">0</span></span><br><span class="line">			hidden_size = hidden_size // <span class="hljs-number">2</span></span><br><span class="line">		self.rnn = rnn_init(rnn_type, input_size=input_size, hidden_size=hidden_size, bidirectional=bidirectional,num_layers=num_layers, dropout=dropout)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, src: Tensor, src_lengths: Tensor, hidden: Tensor = <span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:</span></span><br><span class="line">		lengths = src_lengths.view(-<span class="hljs-number">1</span>).tolist()</span><br><span class="line">		packed_src = pack_padded_sequence(src, lengths)</span><br><span class="line">		memory_bank, hidden_final = self.rnn(packed_src, hidden)</span><br><span class="line">		memory_bank = pad_packed_sequence(memory_bank)[<span class="hljs-number">0</span>]</span><br><span class="line">		<span class="hljs-keyword">return</span> memory_bank, hidden_final</span><br></pre></td></tr></tbody></table></figure>
<h3 id="attention-code">Attention Code</h3>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Attention</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">	linear_out: nn.Linear</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, dim: <span class="hljs-built_in">int</span></span>):</span></span><br><span class="line">		<span class="hljs-built_in">super</span>(Attention, self).__init__()</span><br><span class="line">		self.linear_out = nn.Linear(dim * <span class="hljs-number">2</span>, dim, bias=<span class="hljs-literal">False</span>)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">score</span>(<span class="hljs-params">self, src: Tensor, target: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">		batch_size, src_len, dim = src.size()</span><br><span class="line">		_, target_len, _ = target.size()</span><br><span class="line">		target_ = target</span><br><span class="line">		src_ = src.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)</span><br><span class="line">		<span class="hljs-keyword">return</span> torch.bmm(target_, src_)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, src: Tensor, target: Tensor, src_lengths: Tensor</span>) -&gt; <span class="hljs-type">Tuple</span>[Tensor, Tensor]:</span></span><br><span class="line">		<span class="hljs-keyword">assert</span> target.dim() == <span class="hljs-number">3</span></span><br><span class="line"></span><br><span class="line">		batch_size, src_len, dim = src.size()</span><br><span class="line">		_, target_len, _ = target.size()</span><br><span class="line"></span><br><span class="line">		align_score = self.score(src, target)</span><br><span class="line"></span><br><span class="line">		mask = sequence_mask(src_lengths)</span><br><span class="line">		<span class="hljs-comment"># (batch_size, max_len) -&gt; (batch_size, 1, max_len)</span></span><br><span class="line">		mask = mask.unsqueeze(<span class="hljs-number">1</span>)</span><br><span class="line">		align_score.data.masked_fill_(~mask, -<span class="hljs-built_in">float</span>(<span class="hljs-string">'inf'</span>))</span><br><span class="line">		align_score = F.softmax(align_score, -<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">		c = torch.bmm(align_score, src)</span><br><span class="line"></span><br><span class="line">		concat_c = torch.cat([c, target], -<span class="hljs-number">1</span>)</span><br><span class="line">		attn_h = self.linear_out(concat_c)</span><br><span class="line"></span><br><span class="line">		<span class="hljs-keyword">return</span> attn_h, align_score</span><br></pre></td></tr></tbody></table></figure>
<p>Complete PyTorch implementation source code is also available on <a target="_blank" rel="noopener" href="https://github.com/MyEncyclopedia/blog/tree/master/tsp/ptr_net_pytorch">github</a>.</p>
<h2 id="references">References</h2>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>English</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/tags/PyTorch/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/tags/PyTorch/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("en-AU");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.en.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>