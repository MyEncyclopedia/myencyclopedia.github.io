<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>组合游戏系列4: AlphaGo Zero 强化学习算法原理深度分析 - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/2020/combinatorial-game-4-alphago-zero-theory/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta name="description" content="AlphaGo Zero是Deepmind 最后一代AI围棋算法，因为已经达到了棋类游戏AI的终极目的：给定任何游戏规则，AI从零出发只通过自我对弈的方式提高，最终可以取得超越任何对手（包括顶级人类棋手和上一代AlphaGo）的能力。换种方式说，当给定足够多的时间和计算资源，可以取得无限逼近游戏真实解的能力。这一篇，我们深入分析AlphaGo Zero的设计理念和关键组件的细节并解释组件之间的关联">
<meta property="og:type" content="article">
<meta property="og:title" content="组合游戏系列4: AlphaGo Zero 强化学习算法原理深度分析">
<meta property="og:url" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:description" content="AlphaGo Zero是Deepmind 最后一代AI围棋算法，因为已经达到了棋类游戏AI的终极目的：给定任何游戏规则，AI从零出发只通过自我对弈的方式提高，最终可以取得超越任何对手（包括顶级人类棋手和上一代AlphaGo）的能力。换种方式说，当给定足够多的时间和计算资源，可以取得无限逼近游戏真实解的能力。这一篇，我们深入分析AlphaGo Zero的设计理念和关键组件的细节并解释组件之间的关联">
<meta property="og:locale">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/policy_net.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/pos.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_selection.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_expansion.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_simulation.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_backprop.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/exhaustive_search.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/value_net.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/reduce_depth.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/reduce_breadth.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/policy_value_net.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/self-play.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_alphago.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/net_training.png">
<meta property="article:published_time" content="2020-08-07T18:45:01.000Z">
<meta property="article:modified_time" content="2022-01-27T08:59:17.934Z">
<meta property="article:author" content="MyEncyclopedia">
<meta property="article:tag" content="Algorithm">
<meta property="article:tag" content="Gaming">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="Policy Iteration">
<meta property="article:tag" content="Monte Carlo">
<meta property="article:tag" content="MCTS">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/policy_net.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="目录">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#alphago-zero-综述">1&nbsp;&nbsp;<b>AlphaGo Zero 综述</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#蒙特卡洛树搜索mcts概述">2&nbsp;&nbsp;<b>蒙特卡洛树搜索（MCTS）概述</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#权衡-exploration-和-exploitation">2.1&nbsp;&nbsp;权衡 Exploration 和
Exploitation</a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#从第一性原理来理解alphago-zero">3&nbsp;&nbsp;<b>从第一性原理来理解AlphaGo
Zero</b></a>
                    
                    
                    
                    <a class="navbar-item" href="#搜索空间的两个优化原则">3.1&nbsp;&nbsp;搜索空间的两个优化原则</a>
                    
                    
                    
                    <a class="navbar-item" href="#原则1-通过value-network减少搜索的深度">3.1.1&nbsp;&nbsp;原则1: 通过Value
Network减少搜索的深度</a>
                    
                    
                    
                    <a class="navbar-item" href="#原则2-通过policy-network减少搜索的宽度">3.1.2&nbsp;&nbsp;原则2: 通过Policy
Network减少搜索的宽度</a>
                    
                    
                    
                    <a class="navbar-item" href="#神经网络结构">3.2&nbsp;&nbsp;神经网络结构</a>
                    
                    
                    
                    <a class="navbar-item" href="#alphago-zero-mcts-具体过程">3.3&nbsp;&nbsp;AlphaGo Zero MCTS 具体过程</a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#参考资料">4&nbsp;&nbsp;<b>参考资料</b></a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
            <strong class="sidebar-title">目录</strong>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#alphago-zero-%E7%BB%BC%E8%BF%B0"><span class="toc-text">AlphaGo Zero 综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2mcts%E6%A6%82%E8%BF%B0"><span class="toc-text">蒙特卡洛树搜索（MCTS）概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E8%A1%A1-exploration-%E5%92%8C-exploitation"><span class="toc-text">权衡 Exploration 和
Exploitation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8E%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86%E6%9D%A5%E7%90%86%E8%A7%A3alphago-zero"><span class="toc-text">从第一性原理来理解AlphaGo
Zero</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E7%A9%BA%E9%97%B4%E7%9A%84%E4%B8%A4%E4%B8%AA%E4%BC%98%E5%8C%96%E5%8E%9F%E5%88%99"><span class="toc-text">搜索空间的两个优化原则</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%88%991-%E9%80%9A%E8%BF%87value-network%E5%87%8F%E5%B0%91%E6%90%9C%E7%B4%A2%E7%9A%84%E6%B7%B1%E5%BA%A6"><span class="toc-text">原则1: 通过Value
Network减少搜索的深度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%88%992-%E9%80%9A%E8%BF%87policy-network%E5%87%8F%E5%B0%91%E6%90%9C%E7%B4%A2%E7%9A%84%E5%AE%BD%E5%BA%A6"><span class="toc-text">原则2: 通过Policy
Network减少搜索的宽度</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">神经网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#alphago-zero-mcts-%E5%85%B7%E4%BD%93%E8%BF%87%E7%A8%8B"><span class="toc-text">AlphaGo Zero MCTS 具体过程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-text">参考资料</span></a></li></ol>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            组合游戏系列4: AlphaGo Zero 强化学习算法原理深度分析
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-08-07T18:45:01.000Z" itemprop="datePublished">8月 8 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            24 分钟 读完 (约 3527 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>AlphaGo Zero是Deepmind
最后一代AI围棋算法，因为已经达到了棋类游戏AI的终极目的：给定任何游戏规则，AI从零出发只通过自我对弈的方式提高，最终可以取得超越任何对手（包括顶级人类棋手和上一代AlphaGo）的能力。换种方式说，当给定足够多的时间和计算资源，可以取得无限逼近游戏真实解的能力。这一篇，我们深入分析AlphaGo
Zero的设计理念和关键组件的细节并解释组件之间的关联。下一篇中，我们将在已有的N子棋OpenAI
Gym 环境中用Pytorch实现一个简化版的AlphaGo Zero算法。</p>
<ul>
<li><p><a href="/zh/2020/combinatorial-game-4-alphago-zero-theory/zh/combinatorial-game-1-minimax.md">第一篇:
Leetcode中的Minimax 和 Alpha Beta剪枝</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-4-alphago-zero-theory/zh/combinatorial-game-2-tictactoe.md">第二篇:
井字棋Leetcode系列题解和Minimax最佳策略实现</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-4-alphago-zero-theory/zh/combinatorial-game-3-openai-gym-pygame.md">第三篇:
井字棋、五子棋的OpenAI Gym GUI环境</a></p></li>
<li><p><strong><a href="/zh/2020/combinatorial-game-4-alphago-zero-theory/zh/combinatorial-game-4-alphago-zero-theory/index.md">第四篇:
AlphaGo Zero 强化学习算法原理深度分析</a></strong></p></li>
<li><p><a href="/zh/2020/combinatorial-game-4-alphago-zero-theory/zh/combinatorial-game-5-alphago-zero-connect-n/index.md">第五篇:
井字棋、五子棋AlphaGo Zero 算法实战</a></p></li>
</ul>
<h2 id="alphago-zero-综述">AlphaGo Zero 综述</h2>
<p>AlphaGo Zero 作为Deepmind在围棋领域的最后一代AI
Agent，已经可以达到棋类游戏的终极目标：在只给定游戏规则的情况下，AI
棋手从最初始的随机状态开始，通过不断的自我对弈的强化学习来实现超越以往任何人类棋手和上一代Alpha的能力，并且同样的算法和模型应用到了其他棋类也得出相同的效果。这一篇，从原理上来解析AlphaGo
Zero的运行方式。</p>
<p>AlphaGo Zero
算法由三种元素构成：强化学习（RL）、深度学习（DL）和蒙特卡洛树搜索（MCTS，Monte
Carlo Tree Search）。核心思想是基于神经网络的Policy
Iteration强化学习，即最终学的是一个深度学习的policy
network，输入是某棋盘局面 s，输出是此局面下可走位的概率分布：<span class="math inline">\(p(a|s)\)</span>。</p>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/policy_net.png">
</figure>
<p>在第一代AlphaGo算法中，这个初始policy
network通过收集专业人类棋手的海量棋局训练得来，再采用传统RL 的Monte
Carlo Tree Search Rollout 技术来强化现有的AI对于局面落子（Policy
Network）的判断。Monte Carlo Tree Search Rollout
简单说来就是海量棋局模拟，AI Agent在通过现有的Policy
Network策略完成一次从某局面节点到最终游戏胜负结束的对弈，这个完整的对弈叫做rollout，又称playout。完成一次rollout之后，通过局面树层层回溯到初始局面节点，并在回溯过程中同步修订所有经过的局面节点的统计指标，修正原先policy
network对于落子导致输赢的判断。通过海量并发的棋局模拟来提升基准policy
network，即在各种局面下提高好的落子的<span class="math inline">\(p(a_{win}|s)\)</span>，降低坏的落子的<span class="math inline">\(p(a_{lose}|s)\)</span></p>
举例如下井字棋局面：
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/pos.png">
<figcaption>
局面s
</figcaption>
</figure>
<p>基准policy network返回 p(s) 如下 <span class="math display">\[
p(a|s) =  
\begin{align*}
  \left\lbrace
  \begin{array}{r@{}l}
    0.1, &amp; &amp; a = (0,2) \\
    0.05, &amp; &amp; a = (1,0) \\
     0.5, &amp; &amp; a = (1,1) \\
     0.05, &amp; &amp; a = (1,2)\\
     0.2, &amp; &amp; a = (2,0) \\
    0.05, &amp; &amp; a = (2,1) \\
    0.05, &amp; &amp; a = (2,2)
  \end{array}
  \right.
\end{align*}
\]</span> 通过海量并发模拟后，修订成如下的action概率分布，然后通过policy
iteration迭代新的网络来逼近 <span class="math inline">\(p'\)</span>
就提高了棋力。 <span class="math display">\[
p'(a|s) =  
\begin{align*}
  \left\lbrace
  \begin{array}{r@{}l}
   0, &amp; &amp; a = (0,2) \\
    0, &amp; &amp; a = (1,0) \\
     0.9, &amp; &amp; a = (1,1) \\
     0, &amp; &amp; a = (1,2)\\
     0, &amp; &amp; a = (2,0) \\
    0, &amp; &amp; a = (2,1) \\
    0.1, &amp; &amp; a = (2,2)
  \end{array}
  \right.
\end{align*}
\]</span></p>
<h2 id="蒙特卡洛树搜索mcts概述">蒙特卡洛树搜索（MCTS）概述</h2>
<p>Monte Carlo Tree Search 是Monte Carlo
在棋类游戏中的变种，棋类游戏的一大特点是可以用动作(move)联系的决策树来表示，树的节点数量取决于分支的数量和树的深度。MCTS的目的是在树节点非常多的情况下，通过实验模拟（rollout,
playout）的方式来收集尽可能多的局面输赢情况，并基于这些统计信息，将搜索资源的重点均衡地放在未被探索的节点和值得探索的节点上，减少在大概率输的节点上的模拟资源投入。传统MCTS有四个过程：Selection,
Expansion, Simulation 和Backpropagation。下图是<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Wikipedia</a>
的例子：</p>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_selection.png">
</figure>
<ul>
<li>Selection：从根节点出发，根据现有统计的信息和selection规则，选择子节点递归向下做决定，后面我们会详细介绍AlphaGo的UCB规则。图中节点的数字，例如根节点11/21，分别代表赢的次数和总模拟次数。从根节点一路向下分别选择节点
7/10, 1/6直到叶子节点3/3，叶子节点表示它未被探索过。</li>
</ul>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_expansion.png">
</figure>
<ul>
<li>Expansion：由于3/3节点未被探索过，初始化其所有子节点为0/0，图中3/3只有一个子节点。后面我们会看到神经网络在初始化子节点的时候起到的指导作用，即所有子节点初始权重并非相同，而是由神经网络给出估计。</li>
</ul>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_simulation.png">
</figure>
<ul>
<li>Simulation：重复selection和expansion，根据游戏规则递归向下直至游戏结束。</li>
</ul>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_backprop.png">
</figure>
<ul>
<li>Backpropagation：游戏结束在终点节点产生游戏真实的价值，回溯向上调整所有父节点的统计状态。</li>
</ul>
<h3 id="权衡-exploration-和-exploitation">权衡 Exploration 和
Exploitation</h3>
<p>在不断扩张决策树并收集节点统计信息的同时，MCTS根据规则来权衡探索目的（采样不足）或利用目的来做决策，这个权衡规则叫做Upper
Confidence
Bound（UCB）。典型的UCB公式如下：w表示通过节点的赢的次数，n表示通过节点的总次数，N是父节点的访问次数，c是调节Exploration
和 Exploitation权重的超参。</p>
<p><span class="math display">\[
{\frac{w_i}{n_i}} + c \sqrt{\frac{\ln N_i}{n_i}}
\]</span></p>
<p>假设某节点有两个子节点s1, s2，它们的统计指标为 s1: w/n = 3/4，s2: w/n
=
6/8，由于两者输赢比率一样，因此根据公式，访问次数少的节点出于Exploration的目的胜出，MCTS最终决定从s局面走向s1。</p>
<h2 id="从第一性原理来理解alphago-zero">从第一性原理来理解AlphaGo
Zero</h2>
<p>前一代的AlphaGo已经战胜了世界冠军，取得了空前的成就，AlphaGo Zero
的设计目标变得更加General，去除围棋相关的处理和知识，用统一的框架和算法来解决棋类问题。
1. 无人工先验数据</p>
<p>改进之前需要专家棋手对弈数据来冷启动初始棋力</p>
<ol start="2" type="1">
<li><p>无特定游戏特征工程</p>
<p>无需围棋特定技巧，只包含下棋规则，可以适用到所有棋类游戏</p></li>
<li><p>单一神经网络</p>
<p>统一Policy Network和Value
Network，使用一个共享参数的双头神经网络</p></li>
<li><p>简单树搜索</p>
<p>去除传统MCTS的Rollout
方式，用神经网络来指导MCTS更有效产生搜索策略</p></li>
</ol>
<h3 id="搜索空间的两个优化原则">搜索空间的两个优化原则</h3>
尽管理论上围棋是有解的，即先手必赢、被逼平或必输，通过遍历所有可能局面可以求得解。同理，通过海量模拟所有可能游戏局面，也可以无限逼近所有局面下的真实输赢概率，直至收敛于局面落子的确切最佳结果。但由于围棋棋局的数目远远大于宇宙原子数目，3^361
&gt;&gt;
10^80，因此需要将计算资源有效的去模拟值得探索的局面，例如对于显然的被动局面减小模拟次数，所以如何有效地减小搜索空间是AlphaGo
Zero 需要解决的重大问题。David Silver 在<a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Wujy7OzvdJk&amp;t=358s">Deepmind
AlphaZero - Mastering Games Without Human Knowledge</a>中提到AlphaGo
Zero 采用两个原则来有效减小搜索空间。
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/exhaustive_search.png">
</figure>
<h4 id="原则1-通过value-network减少搜索的深度">原则1: 通过Value
Network减少搜索的深度</h4>
Value Network
通过预测给定局面的value来直接预测最终结果，思想和上一期Minimax DP
策略中直接缓存当前局面的胜负状态一样，减少每次必须靠模拟到最后才能知道当前局面的输赢概率，或者需要多层树搜索才能知道输赢概率。
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/value_net.png">
</figure>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/reduce_depth.png">
</figure>
<h4 id="原则2-通过policy-network减少搜索的宽度">原则2: 通过Policy
Network减少搜索的宽度</h4>
搜索广度的减少是由Policy
Network预估来达成的，将下一步搜索局限在高概率的动作上，大幅度提升原先MCTS新节点生成后冷启动的搜索宽度。
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/reduce_breadth.png">
</figure>
<h3 id="神经网络结构">神经网络结构</h3>
<p>AlphaGo Zero 使用一个单一的深度神经网络来完成policy
和value的预测。具体实现方式是将policy network和value
network合并成一个共享参数 $ $
的双头网络。其中z是真实游戏结局的效用，范围为[-1, 1] 。</p>
<p><span class="math display">\[
(p, v)=f_{\theta}(s)
\]</span> <span class="math display">\[
p_{a}=\operatorname{Pr}(a \mid s)
\]</span> <span class="math display">\[
v =  \mathop{\mathbb{E}}[z|s]
\]</span></p>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/policy_value_net.png">
</figure>
<p>Monte Carlo Tree Search (MCTS)
建立了棋局搜索树，节点的初始状态由神经网络输出的p和v值来估计，由此初始的动作策略和价值预判就会建立在高手的水平之上。模拟一局游戏之后向上回溯，会同步更新路径上节点的统计数值并生成更好的MCTS搜索策略
<span class="math inline">\(\vec{\pi}\)</span>。进一步来看，MCTS和神经网络互相形成了正循环。神经网络指导了未知节点的MCTS初始搜索策略，产生自我对弈游戏结局后，通过减小
<span class="math inline">\(\vec{p}\)</span> 和<span class="math inline">\(\vec{\pi}\)</span>的 Loss
，最终又提高了神经网络对于局面的估计能力。神经网络value
network的提升也是通过不断减小网络预测的结果和最终结果的差异来提升。
因此，具体神经网络的Loss函数由三部分组成，value network的损失，policy
network的损失以及正则项。 <span class="math display">\[
l=\sum_{t}\left(v_{\theta}\left(s_{t}\right)-z_{t}\right)^{2}-\vec{\pi}_{t}
\cdot \log \left(\vec{p}_{\theta}\left(s_{t}\right)\right) + c {\lVert
\theta \rVert}^2
\]</span></p>
<h3 id="alphago-zero-mcts-具体过程">AlphaGo Zero MCTS 具体过程</h3>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/self-play.png">
<figcaption>
AlphaGo Plays Games Against Itself
</figcaption>
</figure>
<p>AlphaGo Zero的MCTS和传统MCTS都有相似的四个过程，但AlphaGo
Zero的MCTS步骤相对更复杂。 首先，除了W/N统计指标之外，AlphaGo
Zero的MCTS保存了决策边 a|s 的Q(s,a)：Action
Value，也就是Q-Learning中的Q值，其初始值由神经网络给出。此外，Q
值也用于串联自底向上更新节点的Value值。具体说来，当某个新节点被Explore后，会将网络给出的Q值向上传递，并逐层更新父节点的Q值。当游戏结局产生时，也会向上更新所有父节点的Q值。
此外对于某一游戏局面s进行多次模拟，每次在局面s出发向下探索，每次探索在已知节点按Selection规则深入一步，直至达到未探索的局面或者游戏结束，产生Q值后向上回溯到最初局面s，回溯过程中更新路径上的局面的统计值或者Q值。在多次模拟结束后根据Play的算法，决定局面s的下一步行动。尽管每次模拟探索可能会深入多层，但最终play阶段的算法规则仅决定给定局面s的下一层落子动作。多次向下探索的优势在于：</p>
<ol type="1">
<li><p>探索和采样更多的叶子节点，在更多信息下做决策。</p></li>
<li><p>通过average
out多次模拟下一层落子决定，尽可能提升MCTS策略的下一步判断能力，提高
<span class="math inline">\(\pi\)</span>
能力，更有效指导神经网络，提高其学习效率。</p></li>
</ol>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/MCTS_alphago.png">
<figcaption>
New Policy Network V' is Trained to Predict Winner
</figcaption>
</figure>
<ol type="1">
<li>Selection:</li>
</ol>
<p>从游戏局面s开始，选择a向下递归，直至未展开的节点（搜索树中的叶子节点）或者游戏结局。具体在局面s下选择a的规则由以下UCB(Upper
Confidence Bound)决定<br>
<span class="math display">\[
a=\operatorname{argmax}_a(Q(s,a) + u(s,a))
\]</span></p>
<p>其中，Q(s,a) 和u(s,a) 项分别代表Exploitation
和Exploration。两项相加来均衡Exploitation和Exploration，保证初始时每个节点被explore，在有足够多的信息时逐渐偏向exploitation。</p>
<p><span class="math display">\[
u(s, a)=c_{p u c t} \cdot P(s, a) \cdot \frac{\sqrt{\Sigma_{b} N(s,
b)}}{1+N(s, a)}
\]</span></p>
<ol start="2" type="1">
<li>Expand</li>
</ol>
<p>当遇到一个未展开的节点（搜索树中的叶子节点）时，对其每个子节点使用现有网络进行预估，即</p>
<p><span class="math display">\[
(p(s), v(s))=f_{\theta}(s)
\]</span></p>
<ol start="3" type="1">
<li>Backup</li>
</ol>
<p>当新的叶子节点展开时或者到达终点局面时，向上更新父节点的Q值，具体公式为
<span class="math display">\[
Q(s, a)=\frac{1}{N(s, a)} \sum_{s^{\prime} \mid s, a \rightarrow
s^{\prime}} V\left(s^{\prime}\right)
\]</span></p>
<ol start="4" type="1">
<li>Play</li>
</ol>
<p>多次模拟结束后，使用得到搜索概率分布 $<em>{a}
$来确定最终的落子动作。正比于访问次数的某次方 $ </em>{a} N(s, a)^{1 /
}<span class="math inline">\(，其中\)</span>$为温度参数（temperature
parameter）。</p>
<figure>
<img src="/zh/2020/combinatorial-game-4-alphago-zero-theory/net_training.png">
<figcaption>
New Policy Network V' is Trained to Predict Winner
</figcaption>
</figure>
<h2 id="参考资料">参考资料</h2>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=Wujy7OzvdJk&amp;t=358s">Youtube,
Deepmind AlphaZero - Mastering Games Without Human Knowledge, David
Silver</a></p></li>
<li><p>Mastering the game of Go with deep neural networks and tree
search</p></li>
<li><p>Mastering Chess and Shogi by Self-Play with a General
Reinforcement Learning Algorithm</p></li>
<li><p><a target="_blank" rel="noopener" href="http://xtf615.com/2018/02/12/AlphaGo-Zero/">AlphaGo
Zero论文解析</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32089487">AlphaZero实战：从零学下五子棋（附代码）</a></p></li>
</ul>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Algorithm/">#Algorithm</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Gaming/">#Gaming</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Reinforcement-Learning/">#Reinforcement Learning</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Deep-Learning/">#Deep Learning</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Policy-Iteration/">#Policy Iteration</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Monte-Carlo/">#Monte Carlo</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/MCTS/">#MCTS</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/zh/2020/combinatorial-game-5-alphago-zero-connect-n/">组合游戏系列5: 井字棋、五子棋AlphaGo Zero 算法实战</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/">组合游戏系列3: 井字棋、五子棋的OpenAI Gym GUI环境</a>
            
        </span>
    </div>
    
</article>


<div>
<p class="note note-warning">
<strong>Author and License</strong> <a href="mailto:dingding303@gmail.com">Contact MyEncyclopedia to Authorize</a> <br>
<strong>myencyclopedia.top link</strong> <a target="_blank" rel="noopener" href="https://blog.myencyclopedia.top/zh/2020/combinatorial-game-4-alphago-zero-theory/">https://blog.myencyclopedia.top/zh/2020/combinatorial-game-4-alphago-zero-theory/</a> <br>
<strong>github.io link</strong> <a href="https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/">https://myencyclopedia.github.io/zh/2020/combinatorial-game-4-alphago-zero-theory/</a> <br>

<img src="/about/me_wechat_scan_search_white.png" />
</p>
</div>




<div class="sharebox">
    
<div class="notification is-danger">
    You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.
</div>

</div>



<div class="comments">
    <h3 class="title is-4">评论</h3>
    
<div id="disqus_thread">
    
    <div class="notification is-danger">
        You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.
    </div>
    
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>


    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/2020/combinatorial-game-4-alphago-zero-theory/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/2020/combinatorial-game-4-alphago-zero-theory/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>