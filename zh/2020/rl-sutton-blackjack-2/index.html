<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制 - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/2020/rl-sutton-blackjack-2/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta name="description" content="这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期21点游戏的策略蒙特卡洛值预测学习了如何用Monte Carlo来预估给定策略 \(\pi\) 的 \(V_{\pi}\) 值之后，这一期我们用Monte Carlo方法来解得21点游戏最佳策略 \(\pi_{*}\)。 蒙特卡洛策略提升 回顾一下，在Grid World 策略迭代和值迭代中由于存在Policy Improvement">
<meta property="og:type" content="article">
<meta property="og:title" content="通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制">
<meta property="og:url" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:description" content="这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期21点游戏的策略蒙特卡洛值预测学习了如何用Monte Carlo来预估给定策略 \(\pi\) 的 \(V_{\pi}\) 值之后，这一期我们用Monte Carlo方法来解得21点游戏最佳策略 \(\pi_{*}\)。 蒙特卡洛策略提升 回顾一下，在Grid World 策略迭代和值迭代中由于存在Policy Improvement">
<meta property="og:locale">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/optimal_policy_usable.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/mc_es_usable_policy.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/optimal_policy_no_usable.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/mc_es_no_usable_policy.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/mc_es_10m_usable.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/mc_es_10m_no_usable.png">
<meta property="article:published_time" content="2020-09-29T18:45:01.000Z">
<meta property="article:modified_time" content="2022-01-27T08:59:18.014Z">
<meta property="article:author" content="MyEncyclopedia">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="OpenAI Gym">
<meta property="article:tag" content="Monte Carlo">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/optimal_policy_usable.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="目录">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#蒙特卡洛策略提升">1&nbsp;&nbsp;<b>蒙特卡洛策略提升</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#算法结果的可视化和理论对比">2&nbsp;&nbsp;<b>算法结果的可视化和理论对比</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#exploring-starts-蒙特卡洛控制改进">3&nbsp;&nbsp;<b>Exploring Starts
蒙特卡洛控制改进</b></a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
            <strong class="sidebar-title">目录</strong>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87"><span class="toc-text">蒙特卡洛策略提升</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%BB%93%E6%9E%9C%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E5%92%8C%E7%90%86%E8%AE%BA%E5%AF%B9%E6%AF%94"><span class="toc-text">算法结果的可视化和理论对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#exploring-starts-%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%8E%A7%E5%88%B6%E6%94%B9%E8%BF%9B"><span class="toc-text">Exploring Starts
蒙特卡洛控制改进</span></a></li></ol>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-29T18:45:01.000Z" itemprop="datePublished">9月 30 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            15 分钟 读完 (约 2204 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼5--">21点游戏的策略蒙特卡洛值预测</a>学习了如何用Monte
Carlo来预估给定策略 <span class="math inline">\(\pi\)</span> 的 <span class="math inline">\(V_{\pi}\)</span> 值之后，这一期我们用Monte
Carlo方法来解得21点游戏最佳策略 <span class="math inline">\(\pi_{*}\)</span>。</p>
<h2 id="蒙特卡洛策略提升">蒙特卡洛策略提升</h2>
<p>回顾一下，在<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼6--">Grid World
策略迭代和值迭代</a>中由于存在Policy Improvement
Theorem，我们可以利用环境dynamics信息计算出策略v值，再选取最greedy
action的方式改进策略，形成策略提示最终能够不断逼近最佳策略。 <span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} v_{*}
\]</span> Monte Carlo Control方法搜寻最佳策略 <span class="math inline">\(\pi{*}\)</span>，是否也能沿用同样的思路呢？答案是可行的。不过，不同于第四章中我们已知环境MDP就知道状态的前后依赖关系，进而从v值中能推断出策略
<span class="math inline">\(\pi\)</span>，在Monte
Carlo方法中，环境MDP是未知的，因而我们只能从action-value中下手，通过海量Monte
Carlo试验来近似 <span class="math inline">\(q_{\pi}\)</span>。有了策略 Q
值，再和MDP策略迭代方法一样，选取最greedy
action的策略，这种策略提示方式理论上被证明了最终能够不断逼近最佳策略。
<span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} q_{*}
\]</span></p>
<p>但是此方法有一个前提要满足，由于数据是依据策略 <span class="math inline">\(\pi_{i}\)</span>
生成的，理论上需要保证在无限次的模拟过程中，每个状态都必须被无限次访问到，才能保证最终每个状态的Q估值收敛到真实的
<span class="math inline">\(q_{*}\)</span>。满足这个前提的一个简单实现是强制随机环境初始状态，保证每个状态都能有一定概率被生成。这个思路就是
Monte Carlo Control with Exploring Starts算法，伪代码如下：</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Monte Carlo ES (Exploring Starts), for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Initialize:} \\
&amp; \quad \pi(s) \in \mathcal A(s) \text{ arbitrarily for all }s \in
\mathcal{S} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Loop forever (for episode):}\\
&amp; \quad \text{Choose } S_0\in \mathcal{S}, A_0 \in \mathcal A(S_0)
\text{ randomly such that all pairs have probability &gt; 0} \\
&amp; \quad \text{Generate an episode from } S_0, A_0 \text{, following
} \pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad \pi(S_t) \leftarrow
\operatorname{argmax}_a Q(S_t, a)\\
\end{align*}
\]</span></p>
</div>
<p>下面我们实现21点游戏的Monte Carlo ES
算法。21点游戏只有200个有效的状态，可以满足算法要求的生成episode前先随机选择某一状态的前提条件。</p>
<p>相对于上一篇，我们增加 ActionValue和Policy的类型定义，ActionValue表示
<span class="math inline">\(q(s, a)\)</span>
，是一个State到动作分布的Dict，Policy
类型也一样。Actions为一维ndarray，维数是离散动作数量。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>]</span><br><span class="line">Action = <span class="hljs-built_in">bool</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">Actions = np.ndarray</span><br><span class="line">ActionValue = <span class="hljs-type">Dict</span>[State, Actions]</span><br><span class="line">Policy = <span class="hljs-type">Dict</span>[State, Actions]</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>下面代码示例如何给定
Policy后，依据指定状态state的动作分布采样，决定下一动作。
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">policy: Policy</span><br><span class="line">A: ActionValue = policy[state]</span><br><span class="line">action = np.random.choice([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], p=A/<span class="hljs-built_in">sum</span>(A))</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>整个算法的 python 代码实现如下：</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_exploring_starts</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        s0 = random.choice(states)</span><br><span class="line">        reset_env_with_s0(env, s0)</span><br><span class="line">        episode_history = gen_custom_s0_stochastic_episode(policy, env, s0)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                best_a = np.argmax(Q[s])</span><br><span class="line">                policy[s][best_a] = <span class="hljs-number">1.0</span></span><br><span class="line">                policy[s][<span class="hljs-number">1</span>-best_a] = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<p>在MC Exploring Starts
算法中，我们需要指定环境初始状态，一种做法是env.reset()时接受初始状态，但是考虑到不去修改第三方实现的
BlackjackEnv类，采用一个取巧的办法，在调用reset()后直接改写env
的私有变量，这个逻辑封装在 reset_env_with_s0 方法中。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_env_with_s0</span>(<span class="hljs-params">env: BlackjackEnv, s0: State</span>) -&gt; BlackjackEnv:</span></span><br><span class="line">    env.reset()</span><br><span class="line">    player_sum = s0[<span class="hljs-number">0</span>]</span><br><span class="line">    oppo_sum = s0[<span class="hljs-number">1</span>]</span><br><span class="line">    has_usable = s0[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    env.dealer[<span class="hljs-number">0</span>] = oppo_sum</span><br><span class="line">    <span class="hljs-keyword">if</span> has_usable:</span><br><span class="line">        env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">11</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> player_sum &gt; <span class="hljs-number">11</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">10</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">10</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">2</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">2</span></span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<h2 id="算法结果的可视化和理论对比">算法结果的可视化和理论对比</h2>
下图是有Usable Ace情况下的理论最优策略。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_usable.png">
<figcaption>
理论最佳策略（有Ace）
</figcaption>
</figure>
Monte
Carlo方法策略提示的收敛是比较慢的，下图是运行10,000,000次episode后有Usable
Ace时的策略 <span class="math inline">\(\pi_{*}^{\prime}\)</span>。对比理论最优策略，MC
ES在不少的状态下还未收敛到理论最优解。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（有Ace）
</figcaption>
</figure>
同样的，下两张图是无Usable Ace情况下的理论最优策略和试验结果的对比。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_no_usable.png">
<figcaption>
理论最佳策略（无Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_no_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（无Ace）
</figcaption>
</figure>
下面的两张图画出了运行代码10,000,000次episode后 <span class="math inline">\(\pi{*}\)</span>的V值图。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_usable.png">
<figcaption>
MC ES 10M的最佳V值（有Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_no_usable.png">
<figcaption>
MC ES 10M的最佳V值（无Ace）
</figcaption>
</figure>
<h2 id="exploring-starts-蒙特卡洛控制改进">Exploring Starts
蒙特卡洛控制改进</h2>
<p>为了避免Monte Carlo ES
Control在初始时必须访问到任意状态的限制，教材中介绍了一种改进算法，On-policy
first-visit MC control for <span class="math inline">\(\epsilon
\text{-soft policies}\)</span> ，它同样基于Monte Carlo 预估Q值，但用
<span class="math inline">\(\epsilon \text{-soft}\)</span>
策略来代替最有可能的action策略作为下一次迭代策略，<span class="math inline">\(\epsilon \text{-soft}\)</span>
本质上来说就是对于任意动作都保留 <span class="math inline">\(\epsilon\)</span>
小概率的访问可能，权衡了exploration和exploitation，由于每个动作都可能被无限次访问到，Explorting
Starts中的强制随机初始状态就可以去除了。Monte Carlo ES Control 和
On-policy first-visit MC control for <span class="math inline">\(\epsilon \text{-soft policies}\)</span>
都属于on-policy算法，其区别于off-policy的本质在于预估 <span class="math inline">\(q_{\pi}(s,a)\)</span>时是否从同策略<span class="math inline">\(\pi\)</span>生成的数据来计算。一个比较subtle的例子是著名的Q-Learning，因为根据这个定义，Q-Learning属于off-policy。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{On-policy first-visit MC control (for }\epsilon
\textbf{-soft policies), estimating } \pi \approx \pi_{*} \\
&amp; \text{Algorithm parameter: small } \epsilon &gt; 0 \\
&amp; \text{Initialize:} \\
&amp; \quad \pi \leftarrow \text{ an arbitrary } \epsilon \text{-soft
policy} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Repeat forever (for episode):}\\
&amp; \quad \text{Generate an episode following } \pi : S_0, A_0, R_1,
S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad A^{*} \leftarrow \operatorname{argmax}_a
Q(S_t, a)\\
&amp; \quad \quad \quad \quad \text{For all } a \in \mathcal A(S_t):\\
&amp; \quad \quad \quad \quad \quad \pi(a|S_t) \leftarrow
    \begin{cases}
      1 - \epsilon + \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a =
A^{*}\\
      \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a \neq A^{*}\\
    \end{cases}       \\
\end{align*}
\]</span></p>
</div>
<p>伪代码对应的 Python 实现如下。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_epsilon_greedy</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span>, epsilon=<span class="hljs-number">0.1</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_epsilon_greedy_policy</span>(<span class="hljs-params">policy: Policy, Q: ActionValue, s: State</span>):</span></span><br><span class="line">        policy[s] = np.ones(env.action_space.n, dtype=<span class="hljs-built_in">float</span>) * epsilon / env.action_space.n</span><br><span class="line">        best_action = np.argmax(Q[s])</span><br><span class="line">        policy[s][best_action] += (<span class="hljs-number">1.0</span> - epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        episode_history = gen_stochastic_episode(policy, env)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                update_epsilon_greedy_policy(policy, Q, s)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Python/">#Python</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Reinforcement-Learning/">#Reinforcement Learning</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/OpenAI-Gym/">#OpenAI Gym</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Monte-Carlo/">#Monte Carlo</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/zh/2020/leetcode-matrix-power/">Leetcode矩阵快速幂运算解法</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/zh/2020/rl-sutton-blackjack-1/">通过代码学Sutton强化学习3：21点游戏的策略蒙特卡洛值预测</a>
            
        </span>
    </div>
    
</article>


<div>
<p class="note note-warning">
<strong>Author and License</strong> <a href="mailto:dingding303@gmail.com">Contact MyEncyclopedia to Authorize</a> <br>
<strong>myencyclopedia.top link</strong> <a target="_blank" rel="noopener" href="https://blog.myencyclopedia.top/zh/2020/rl-sutton-blackjack-2/">https://blog.myencyclopedia.top/zh/2020/rl-sutton-blackjack-2/</a> <br>
<strong>github.io link</strong> <a href="https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/">https://myencyclopedia.github.io/zh/2020/rl-sutton-blackjack-2/</a> <br>

<img src="/about/me_wechat_scan_search_white.png" />
</p>
</div>




<div class="sharebox">
    
<div class="notification is-danger">
    You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.
</div>

</div>



<div class="comments">
    <h3 class="title is-4">评论</h3>
    
<div id="disqus_thread">
    
    <div class="notification is-danger">
        You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.
    </div>
    
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>


    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/2020/rl-sutton-blackjack-2/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/2020/rl-sutton-blackjack-2/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>