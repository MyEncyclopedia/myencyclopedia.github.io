<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法 - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/2020/rl-sutton-gridworld-1/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta name="description" content="经典教材Reinforcement Learning: An Introduction 第二版由强化领域权威Richard S. Sutton 和 Andrew G. Barto 完成编写，内容深入浅出，非常适合初学者。在本篇中，引入Grid World示例，结合强化学习核心概念，并用python代码实现OpenAI Gym的模拟环境，进一步实现策略评价算法。      Grid World 问题">
<meta property="og:type" content="article">
<meta property="og:title" content="通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法">
<meta property="og:url" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:description" content="经典教材Reinforcement Learning: An Introduction 第二版由强化领域权威Richard S. Sutton 和 Andrew G. Barto 完成编写，内容深入浅出，非常适合初学者。在本篇中，引入Grid World示例，结合强化学习核心概念，并用python代码实现OpenAI Gym的模拟环境，进一步实现策略评价算法。      Grid World 问题">
<meta property="og:locale">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/rl_sutton.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/grid_world.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/env_agent.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/backup_v_pi.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/backup_q_pi.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/backup_optimal.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/optimal_policy.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/value_3d.png">
<meta property="og:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/random_policy_v.png">
<meta property="article:published_time" content="2020-09-03T18:45:01.000Z">
<meta property="article:modified_time" content="2022-01-27T08:59:18.022Z">
<meta property="article:author" content="MyEncyclopedia">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="OpenAI Gym">
<meta property="article:tag" content="Dynamic Programming">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/rl_sutton.png">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            <div class="navbar-item is-hoverable has-dropdown is-hidden-mobile is-hidden-tablet-only toc">
                <a class="navbar-item" title="目录">
                    <i class="fa fa-list"></i>
                </a>
                <div class="navbar-dropdown is-right">
                    
                    
                    
                    
                    <a class="navbar-item" href="#grid-world-问题">1&nbsp;&nbsp;<b>Grid World 问题</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#finite-mdp-模型">2&nbsp;&nbsp;<b>Finite MDP 模型</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#强化学习的目的">3&nbsp;&nbsp;<b>强化学习的目的</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#bellman-最佳原则">4&nbsp;&nbsp;<b>Bellman 最佳原则</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#grid-world-最佳策略和v值">5&nbsp;&nbsp;<b>Grid World 最佳策略和V值</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#grid-world-openai-gym-环境">6&nbsp;&nbsp;<b>Grid World OpenAI Gym 环境</b></a>
                    
                    
                    <hr class="navbar-divider">
                    
                    
                    <a class="navbar-item" href="#策略评估policy-evaluation">7&nbsp;&nbsp;<b>策略评估（Policy Evaluation）</b></a>
                    
                </div>
            </div>
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section">
    <div class="container">
    <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
            <strong class="sidebar-title">目录</strong>
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#grid-world-%E9%97%AE%E9%A2%98"><span class="toc-text">Grid World 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#finite-mdp-%E6%A8%A1%E5%9E%8B"><span class="toc-text">Finite MDP 模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%9B%AE%E7%9A%84"><span class="toc-text">强化学习的目的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bellman-%E6%9C%80%E4%BD%B3%E5%8E%9F%E5%88%99"><span class="toc-text">Bellman 最佳原则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grid-world-%E6%9C%80%E4%BD%B3%E7%AD%96%E7%95%A5%E5%92%8Cv%E5%80%BC"><span class="toc-text">Grid World 最佳策略和V值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#grid-world-openai-gym-%E7%8E%AF%E5%A2%83"><span class="toc-text">Grid World OpenAI Gym 环境</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0policy-evaluation"><span class="toc-text">策略评估（Policy Evaluation）</span></a></li></ol>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-03T18:45:01.000Z" itemprop="datePublished">9月 4 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            19 分钟 读完 (约 2827 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>经典教材Reinforcement Learning: An Introduction
第二版由强化领域权威Richard S. Sutton 和 Andrew G. Barto
完成编写，内容深入浅出，非常适合初学者。在本篇中，引入Grid
World示例，结合强化学习核心概念，并用python代码实现OpenAI
Gym的模拟环境，进一步实现策略评价算法。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/rl_sutton.png">
<figcaption>
</figcaption>
</figure>
<h2 id="grid-world-问题">Grid World 问题</h2>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/grid_world.png">
<figcaption>
</figcaption>
</figure>
<p>第四章例子4.1提出了一个简单的离散空间状态问题：Grid
World，其大致意思是在4x4的网格世界中有14个格子是非终点状态，在这些非终点状态的格子中可以往上下左右四个方向走，直至走到两个终点状态格子，则游戏结束。每走一步，Agent收获reward
-1，表示Agent希望在Grid World中尽早出去。另外，Agent在Grid
World边缘时，无法继续往外只能呆在原地，reward也是-1。</p>
<h2 id="finite-mdp-模型">Finite MDP 模型</h2>
先来回顾一下强化学习的建模基础：有限马尔可夫决策过程（Finite Markov
Decision Process, Finite
MDP）。如下图，强化学习模型将世界抽象成两个实体，强化学习解决目标的主体Agent和其他外部环境。它们之间的交互过程遵从有限马尔可夫决策过程：若Agent在t时间步骤时处于状态
<span class="math inline">\(S_t\)</span>，采取动作 <span class="math inline">\(A_t\)</span>，然后环境根据自身机制，产生Reward
<span class="math inline">\(R_{t+1}\)</span> 并将Agent状态变为 <span class="math inline">\(S_{t+1}\)</span>。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/env_agent.png">
<figcaption>
</figcaption>
</figure>
<p>环境自身机制又称为dynamics，工程上可以看成一个输入(S, A)，输出(S,
R)的方法。由于MDP包含随机过程，某个输入并不能确定唯一输出，而会根据概率分布输出不同的(S,
R)。Finite MDP简化了时间对于模型的影响，因为(S, R)只和(S,
A)有关，不和时间t有关。另外，有限指的是S，A，R的状态数量是有限的。</p>
<p>数学上dynamics可以如下表示</p>
<div>
<p><span class="math display">\[
p\left(s^{\prime}, r \mid s, a\right) \doteq
\operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \mid S_{t-1}=s,
A_{t-1}=a\right\}
\]</span></p>
</div>
<p>即是四元组作为输入的概率函数 <span class="math inline">\(p: S \times
R \times S \times A \rightarrow [0, 1]\)</span>。</p>
<p>满足 <span class="math display">\[
\sum_{s^{\prime} \in \mathcal{S}} \sum_{r \in \mathcal{R}}
p\left(s^{\prime}, r \mid s, a\right)=1, \text { for all } s \in
\mathcal{S}, a \in \mathcal{A}(s)
\]</span></p>
以Grid
World为例，当Agent处于编号1的网格时，可以往四个方向走，往任意方向走都只产生一种
S,
R，因为这个简单的游戏是确定性的，不存在某一动作导致stochastic状态。例如，在1号网格往左就到了终点网格（编号0），得到Reward
-1这个规则可以如下表示 <span class="math display">\[
p\left(s^{\prime}=0, r=-1 \mid s=1, a=\text{L}\right) = 1
\]</span> 因此，状态s=1的所有dynamics概率映射为
<div>
<p><span class="math display">\[
\begin{aligned}
p\left(s^{\prime}=0, r=-1 \mid s=1, a=\text{L}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=2, r=-1 \mid s=1, a=\text{R}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=1, r=-1 \mid s=1, a=\text{U}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=5, r=-1 \mid s=1, a=\text{D}\right) &amp;=&amp; 1
\end{aligned}
\]</span></p>
</div>
<h2 id="强化学习的目的">强化学习的目的</h2>
<p>在给定了问题以及定义了强化学习的模型之后，强化学习的目的当然是通过学习让Agent能够学到最佳策略<span class="math inline">\(\pi_{*}\)</span>，也就是在某个状态下的行动分布，记成
<span class="math inline">\(\pi(a|s)\)</span>。对应在数值上的优化目标是Agent在一系列过程中采取某种策略的reward总和的期望（Expected
Return）。下面公式定义了t步往后的reward总和，其中 <span class="math inline">\(\gamma\)</span> 为discount
factor，用于权衡短期和长期reward对于当前Agent的效用影响。等式最后一步的意义是t步后的reward总和等价于t步所获的立即reward
<span class="math inline">\(R_{t+1}\)</span>，加上t+1步后的reward总和
<span class="math inline">\(\gamma G_{t+1}\)</span>。</p>
<div>
<p><span class="math display">\[
\begin{aligned}
G_{t} &amp; \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3}
R_{t+4}+\cdots \\
&amp;=R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\gamma^{2}
R_{t+4}+\cdots\right) \\
&amp;=R_{t+1}+\gamma G_{t+1}
\end{aligned}
\]</span></p>
</div>
<p>有了reward总和的定义，评价Agent策略 <span class="math inline">\(\pi\)</span> 就可以定义成Agent在状态 s
时采用此策略的Expected Return。</p>
<p><span class="math display">\[
v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]
\]</span></p>
<p>下面公式推导了 <span class="math inline">\(v_{\pi}(s)\)</span>
数值上和相关状态 <span class="math inline">\(s{\prime}\)</span>
的关系：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]
\\
&amp;=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
\mid S_{t}=s\right]\\
&amp;=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right]
\\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}} \sum_{r}
p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
\mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]\right] \\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r
\mid s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\quad \text { for all } s \in \mathcal{S}
\end{aligned}
\]</span></p>
</div>
<p>注意到如果将 <span class="math inline">\(v_{\pi}(s)\)</span>
看成未知数，上式即形成 <span class="math inline">\(\mid \mathcal{S}
\mid\)</span> 个未知变量的方程组，可以在数值上解得各个 <span class="math inline">\(v_{\pi}(s)\)</span>。</p>
书中用Backup Diagram来表示递推关系，下图是<span class="math inline">\(v_{\pi}(s)\)</span>的backup diagram。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_v_pi.png">
<figcaption>
</figcaption>
</figure>
<p>尽管v值可以来衡量策略，但由于<span class="math inline">\(v_{\pi}(s)\)</span> 是Agent在策略<span class="math inline">\(\pi(a|s)\)</span>的Expected
Return，将不同的action拆出来单独计算Expected
Return，这样的做法有时更为直接，这就是著名的Q Learning中的q
值，记成<span class="math inline">\(q_{\pi}(s, a)\)</span> 。</p>
<p><span class="math display">\[
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s,
A_{t}=a\right]
\]</span></p>
<p>下面是 $q_{}(s, a) $ 的递推 backup diagram。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_q_pi.png">
<figcaption>
</figcaption>
</figure>
<h2 id="bellman-最佳原则">Bellman 最佳原则</h2>
<p>对于所有状态集合<span class="math inline">\(\mathcal{S}\)</span>，策略<span class="math inline">\({\pi}\)</span>的评价指标 <span class="math inline">\(v_{\pi}(s)\)</span>
是一个向量，本质上是无法相互比较的。但由于存在Bellman
最佳原则（Bellman's principle of
optimality）：在有限状态情况下，一定存在一个或者多个最好的策略 <span class="math inline">\({\pi}_{*}\)</span>，它在所有状态下的v值都是最好的，即
<span class="math inline">\(v_{\pi_{*}}(s) \ge v_{\pi^{\prime}}(s) \text
{ for all } s \in \mathcal{S}\)</span>。</p>
<p>因此，最佳v值定义为最佳策略 <span class="math inline">\({\pi}_{*}\)</span> 对应的 v 值</p>
<p><span class="math display">\[
v_{*}(s) \doteq \max_{\pi} v_{\pi}(s)
\]</span></p>
<p>同理，也存在最佳q值，记为 <span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;\doteq \max_{\pi} q_{\pi}(s,a)
\end{aligned}
\]</span></p>
<p>将 <span class="math inline">\(v_{*}(s)\)</span> 改写成递推形式，称为
Bellman Optimality Equation，推导如下</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{*}(s) &amp;=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&amp;=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s,
A_{t}=a\right] \\
&amp;=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid
S_{t}=s, A_{t}=a\right] \\
&amp;=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right)
\mid S_{t}=s, A_{t}=a\right] \\
&amp;=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>直觉上可以理解为状态 s
对应的最佳v值是只采取此状态下的最佳动作后的Expected Return。</p>
<p>最佳q值递归形式的意义为最佳策略下状态s时采取行动 a 的Expected
Return，等于所有可能后续状态 s' 下采取最优行动的Expected
Return的均值。推导如下：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}}
q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\
&amp;=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma \max _{a^{\prime}} q_{*}\left(s^{\prime},
a^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<span class="math inline">\(v_{*}(s), q_{*}(s, a)\)</span> 的backup
diagram 如下图
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_optimal.png">
<figcaption>
</figcaption>
</figure>
<h2 id="grid-world-最佳策略和v值">Grid World 最佳策略和V值</h2>
<p>Grid World 的最佳策略如下：尽可能快的走出去</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/optimal_policy.png">
<figcaption>
Grid World最佳策略
</figcaption>
</figure>
<p>上面的2D图中不同颜色表示不同V值，终点格子的红色表示0，隔着一步的黄色为-1，隔两步的绿色为-2，最远的紫色为-3。下面是立体图示。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/value_3d.png">
<figcaption>
Grid World最佳策略V值
</figcaption>
</figure>
<h2 id="grid-world-openai-gym-环境">Grid World OpenAI Gym 环境</h2>
<p>下面是OpenAI Gym框架下Grid
World环境的代码实现。本质是在GridWorldEnv构造函数中构建MDP，类型定义如下</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MDP = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[Prob, State, Reward, <span class="hljs-built_in">bool</span>]]]]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># P[state][action] = [</span></span><br><span class="line"><span class="hljs-comment">#    (prob1, next_state1, reward1, is_done),</span></span><br><span class="line"><span class="hljs-comment">#    (prob2, next_state2, reward2, is_done), ...]</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Action</span>(<span class="hljs-params">Enum</span>):</span></span><br><span class="line">    UP = <span class="hljs-number">0</span></span><br><span class="line">    DOWN = <span class="hljs-number">1</span></span><br><span class="line">    LEFT = <span class="hljs-number">2</span></span><br><span class="line">    RIGHT = <span class="hljs-number">3</span></span><br><span class="line"></span><br><span class="line">State = <span class="hljs-built_in">int</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">Prob = <span class="hljs-built_in">float</span></span><br><span class="line">Policy = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, Prob]]</span><br><span class="line">Value = <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span><br><span class="line">StateSet = <span class="hljs-type">Set</span>[<span class="hljs-built_in">int</span>]</span><br><span class="line">NonTerminalStateSet = <span class="hljs-type">Set</span>[<span class="hljs-built_in">int</span>]</span><br><span class="line">MDP = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[Prob, State, Reward, <span class="hljs-built_in">bool</span>]]]]</span><br><span class="line"><span class="hljs-comment"># P[s][a] = [(prob, next_state, reward, is_done), ...]</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GridWorldEnv</span>(<span class="hljs-params">discrete.DiscreteEnv</span>):</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Grid World environment described in Sutton and Barto Reinforcement Learning 2nd, chapter 4.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, shape=[<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]</span>):</span></span><br><span class="line">        self.shape = shape</span><br><span class="line">        nS = np.prod(shape)</span><br><span class="line">        nA = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(Action))</span><br><span class="line">        MAX_R = shape[<span class="hljs-number">0</span>]</span><br><span class="line">        MAX_C = shape[<span class="hljs-number">1</span>]</span><br><span class="line">        self.grid = np.arange(nS).reshape(shape)</span><br><span class="line">        isd = np.ones(nS) / nS</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># P[s][a] = [(prob, next_state, reward, is_done), ...]</span></span><br><span class="line">        P: MDP = {}</span><br><span class="line">        action_delta = {Action.UP: (-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.DOWN: (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.LEFT: (<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), Action.RIGHT: (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)}</span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, MAX_R * MAX_C):</span><br><span class="line">            P[s] = {a.value : [] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action)}</span><br><span class="line">            is_terminal = self.is_terminal(s)</span><br><span class="line">            <span class="hljs-keyword">if</span> is_terminal:</span><br><span class="line">                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action):</span><br><span class="line">                    P[s][a.value] = [(<span class="hljs-number">1.0</span>, s, <span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)]</span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                r = s // MAX_R</span><br><span class="line">                c = s % MAX_R</span><br><span class="line">                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action):</span><br><span class="line">                    neighbor_r = <span class="hljs-built_in">min</span>(MAX_R-<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, r + action_delta[a][<span class="hljs-number">0</span>]))</span><br><span class="line">                    neighbor_c = <span class="hljs-built_in">min</span>(MAX_C-<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, c + action_delta[a][<span class="hljs-number">1</span>]))</span><br><span class="line">                    s_ = neighbor_r * MAX_R + neighbor_c</span><br><span class="line">                    P[s][a.value] = [(<span class="hljs-number">1.0</span>, s_, -<span class="hljs-number">1</span>, <span class="hljs-literal">False</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-built_in">super</span>(GridWorldEnv, self).__init__(nS, nA, P, isd)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="策略评估policy-evaluation">策略评估（Policy Evaluation）</h2>
<p>策略评估需要解决在给定环境dynamics和Agent策略 <span class="math inline">\(\pi\)</span>下，计算策略的v值 <span class="math inline">\(v_{\pi}\)</span>。由于所有数量关系都已知，可以通过解方程组的方式求得，但通常会通过数值迭代的方式来计算，即通过一系列
<span class="math inline">\(v_{0}, v_{1}, ..., v_{k}\)</span> 收敛至
<span class="math inline">\(v_{\pi}\)</span>。如下迭代方式已经得到证明，当
<span class="math inline">\(k \rightarrow \infty\)</span> 一定收敛至
<span class="math inline">\(v_{\pi}\)</span>。</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{k+1}(s) &amp; \doteq \mathbb{E}_{\pi}\left[R_{t+1}+\gamma
v_{k}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r
\mid s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>书中具体伪代码如下</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Iterative Policy Evaluation, for estimating } V\approx
v_{\pi} \\
&amp; \text{Input } {\pi}, \text{the policy to be evaluated} \\
&amp; \text{Algorithm parameter: a small threshold } \theta &gt; 0
\text{ determining accuracy of estimation} \\
&amp; \text{Initialize } V(s), \text{for all } s \in \mathcal{S}^{+}
\text{, arbitrarily except that } V (terminal) = 0\\
&amp; \\
&amp;1: \text{Loop:}\\
&amp;2: \quad \quad \Delta \leftarrow 0\\
&amp;3: \quad \quad \text{Loop for each } s \in \mathcal{S}:\\
&amp;4: \quad \quad \quad \quad v \leftarrow V(s) \\
&amp;5: \quad \quad \quad \quad V(s) \leftarrow \sum_{a} \pi(a \mid s)
\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
V\left(s^{\prime}\right)\right] \\
&amp;6: \quad \quad \quad \quad \Delta \leftarrow \max(\Delta, |v-V(s)|)
\\
&amp;7: \text{until } \Delta &lt; \theta
\end{align*}
\]</span></p>
</div>
<p>下面是python
代码实现，注意这里单run迭代时，新的v值直接覆盖数组里的旧v值，这种做法在书中被证明不仅有效，甚至更为高效。这种做法称为原地（in
place）更新。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">policy_evaluate</span>(<span class="hljs-params">policy: Policy, env: GridWorldEnv, gamma=<span class="hljs-number">1.0</span>, theta=<span class="hljs-number">0.0001</span></span>):</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">        delta = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">            v = <span class="hljs-number">0</span></span><br><span class="line">            <span class="hljs-keyword">for</span> a, action_prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(policy[s]):</span><br><span class="line">                <span class="hljs-keyword">for</span> prob, next_state, reward, done <span class="hljs-keyword">in</span> env.P[s][a]:</span><br><span class="line">                    v += action_prob * prob * (reward + gamma * V[next_state])</span><br><span class="line">            delta = <span class="hljs-built_in">max</span>(delta, np.<span class="hljs-built_in">abs</span>(v - V[s]))</span><br><span class="line">            V[s] = v</span><br><span class="line">        <span class="hljs-keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">    <span class="hljs-keyword">return</span> np.array(V)</span><br></pre></td></tr></tbody></table></figure>
<p>输入策略为随机选择方向，运行上面的policy_evaluate最终多轮收敛后的V值输出为</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[  <span class="hljs-number">0.</span>         -<span class="hljs-number">13.99931242</span> -<span class="hljs-number">19.99901152</span> -<span class="hljs-number">21.99891199</span>]</span><br><span class="line"> [-<span class="hljs-number">13.99931242</span> -<span class="hljs-number">17.99915625</span> -<span class="hljs-number">19.99908389</span> -<span class="hljs-number">19.99909436</span>]</span><br><span class="line"> [-<span class="hljs-number">19.99901152</span> -<span class="hljs-number">19.99908389</span> -<span class="hljs-number">17.99922697</span> -<span class="hljs-number">13.99942284</span>]</span><br><span class="line"> [-<span class="hljs-number">21.99891199</span> -<span class="hljs-number">19.99909436</span> -<span class="hljs-number">13.99942284</span>   <span class="hljs-number">0.</span>        ]]</span><br></pre></td></tr></tbody></table></figure>
在3D V值图中可以发现，由于是随机选择方向的策略，
Agent在每个格子的V值绝对数值要比最佳V值大，意味着随机策略下Agent在Grid
World会得到更多的负reward。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/random_policy_v.png">
<figcaption>
Grid World随机策略V值
</figcaption>
</figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    <div class="columns is-variable is-1 is-multiline is-mobile">
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Python/">#Python</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Reinforcement-Learning/">#Reinforcement Learning</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/OpenAI-Gym/">#OpenAI Gym</a></span>
    
        <span class="column is-narrow"><a class="tag is-light article-tag" href="/tags/Dynamic-Programming/">#Dynamic Programming</a></span>
    
    </div>
    
    
    <div class="columns is-mobile is-multiline article-nav">
        <span class="column is-12-mobile is-half-desktop  article-nav-prev">
            
            <a href="/zh/2020/leetcode-679-24-game/">Leetcode 679 24 Game 的 Python 函数式实现</a>
            
        </span>
        <span class="column is-12-mobile is-half-desktop  article-nav-next">
            
            <a href="/zh/2020/tsp-1-dp-alg/">TSP问题从DP算法到深度学习1： 递归DP方法 AC AIZU TSP问题</a>
            
        </span>
    </div>
    
</article>


<div>
<p class="note note-warning">
<strong>Author and License</strong> <a href="mailto:dingding303@gmail.com">Contact MyEncyclopedia to Authorize</a> <br>
<strong>myencyclopedia.top link</strong> <a target="_blank" rel="noopener" href="https://blog.myencyclopedia.top/zh/2020/rl-sutton-gridworld-1/">https://blog.myencyclopedia.top/zh/2020/rl-sutton-gridworld-1/</a> <br>
<strong>github.io link</strong> <a href="https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/">https://myencyclopedia.github.io/zh/2020/rl-sutton-gridworld-1/</a> <br>

<img src="/about/me_wechat_scan_search_white.png" />
</p>
</div>




<div class="sharebox">
    
<div class="notification is-danger">
    You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.
</div>

</div>



<div class="comments">
    <h3 class="title is-4">评论</h3>
    
<div id="disqus_thread">
    
    <div class="notification is-danger">
        You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.
    </div>
    
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
</div>


    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/2020/rl-sutton-gridworld-1/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/2020/rl-sutton-gridworld-1/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>