{"pages":[],"posts":[{"title":"关于我","text":"资深老程序员，兴趣广泛，精通各个方面的计算机技术，包括 深度学习：RL，DNN，CV，NLP 统计和数学 算法 计算机工程 多线程编程 分布式计算和大数据 欢迎关注我的公众号 我的微信号，注明目的","link":"/about/"},{"title":"组合游戏系列1: Leetcode中的Minimax 和 Alpha Beta剪枝","text":"本系列，我们来看看在一种常见的组合游戏——回合制棋盘类游戏中，如何用算法来解决问题。首先，我们会介绍并解决搜索空间较小的问题，引入经典的博弈算法和相关理论，最终实现在大搜索空间中的Deep RL近似算法。在此基础上可以理解AlphaGo的原理和工作方式。 本系列的第一篇，我们介绍3个Leetcode中的零和回合制游戏，从最初的暴力解法，到动态规划最终演变成博弈论里的经典算法： minimax 以及 alpha beta 剪枝。 第一篇: Leetcode中的Minimax 和 Alpha Beta剪枝 第二篇: 井字棋Leetcode系列题解和Minimax最佳策略实现 第三篇: 井字棋、五子棋的OpenAI Gym GUI环境 第四篇: AlphaGo Zero 强化学习算法原理深度分析 第五篇: 井字棋、五子棋AlphaGo Zero 算法实战 Leetcode 292 Nim Game (简单) 简单题 Leetcode 292 Nim Game。 你和你的朋友，两个人一起玩 Nim游戏：桌子上有一堆石头，每次你们轮流拿掉 1 - 3 块石头。 拿掉最后一块石头的人就是获胜者。你作为先手。 你们是聪明人，每一步都是最优解。 编写一个函数，来判断你是否可以在给定石头数量的情况下赢得游戏。 示例: 输入: 4 输出: false 解释: 如果堆中有 4 块石头，那么你永远不会赢得比赛；因为无论你拿走 1 块、2 块 还是 3 块石头，最后一块石头总是会被你的朋友拿走。 定义 \\(f(n)\\) 为有\\(n\\)个石头并采取最优策略的游戏结果， \\(f(n)\\)的值只有可能是赢或者输。考察前几个结果：\\(f(1) = f(2) = f(3) = Win\\)，然后来计算\\(f(4)\\)。因为玩家采取最优策略（只要有一种走法让对方必输，玩家获胜），对于4来说，玩家能走的可能是拿掉1块、2块或3块，但是无论剩余何种局面，对方都是必赢，因此，4就是必输。总的说来，递归关系如下： \\[ f(n) = \\neg (f(n-1) \\land f(n-2) \\land f(n-3)) \\] 这个递归式可以直接翻译成Python 3代码 {linenos1234567891011# TLE# Time Complexity: O(exponential)class Solution_BruteForce: def canWinNim(self, n: int) -&gt; bool: if n &lt;= 3: return True for i in range(1, 4): if not self.canWinNim(n - i): return True return False 以上的递归公式和代码很像fibonacci数的递归定义和暴力解法，因此对应的时间复杂度也是指数级的，提交代码以后会TLE。下图画出了当n=7时的递归调用，注意 5 被扩展向下重复执行了两次，4重复了4次。 292 Nim Game 暴力解法调用图 n=7 我们采用和fibonacci一样的方式来优化算法：缓存较小n的结果以此来计算较大n的结果。 Python 中，我们可以只加一行lru_cache decorator，来取得这种动态规划效果，下面的代码将复杂度降到了 \\(O(N)\\)。 {linenos123456789101112# RecursionError: maximum recursion depth exceeded in comparison n=1348820612# Time Complexity: O(N)class Solution_DP: from functools import lru_cache @lru_cache(maxsize=None) def canWinNim(self, n: int) -&gt; bool: if n &lt;= 3: return True for i in range(1, 4): if not self.canWinNim(n - i): return True return False 再来画出调用图：这次5和4就不再被展开重复计算，图中绿色的节点表示缓存命中。 292 Nim Game 动归解法调用图 n=7 但还是没有AC，因为当n=1348820612时，这种方式会导致栈溢出。再改成下面的循环版本，可惜还是TLE。 {linenos1234567891011# TLE for 1348820612# Time Complexity: O(N)class Solution: def canWinNim(self, n: int) -&gt; bool: if n &lt;= 3: return True last3, last2, last1 = True, True, True for i in range(4, n+1): this = not (last3 and last2 and last1) last3, last2, last1 = last2, last1, this return last1 由此看来，AC 版本需要低于\\(O(n)\\)的算法复杂度。上面的写法似乎暗示输赢有周期性的规律。事实上，如果将输赢按照顺序画出来，就马上得出规律了：只要\\(n \\mod 4 = 0\\) 就是输，否则赢。原因如下：当面临不能被4整除的数量时 \\(4k+i (i=1,2,3)\\) ，一方总是可以拿走 \\(i\\) 个，将\\(4k\\) 留给对手，而对方下轮又将返回不能被4整除的数，如此循环往复，直到这一方有1, 2, 3 个，最终获胜。 输赢分布 最终AC版本，只有一句语句。 {linenos12345# AC# Time Complexity: O(1)class Solution: def canWinNim(self, n: int) -&gt; bool: return not (n % 4 == 0) Leetcode 486 Predict the Winner (中等) 中等难度题目： Leetcode 486 Predict the Winner. 给定一个表示分数的非负整数数组。 玩家1从数组任意一端拿取一个分数，随后玩家2继续从剩余数组任意一端拿取分数，然后玩家1拿，……。每次一个玩家只能拿取一个分数，分数被拿取之后不再可取。直到没有剩余分数可取时游戏结束。最终获得分数总和最多的玩家获胜。 给定一个表示分数的数组，预测玩家1是否会成为赢家。你可以假设每个玩家的玩法都会使他的分数最大化。 示例 1: 输入: [1, 5, 2] 输出: False 解释: 一开始，玩家1可以从1和2中进行选择。 如果他选择2（或者1），那么玩家2可以从1（或者2）和5中进行选择。如果玩家2选择了5，那么玩家1则只剩下1（或者2）可选。 所以，玩家1的最终分数为 1 + 2 = 3，而玩家2为 5。 因此，玩家1永远不会成为赢家，返回 False。 示例 2: 输入: [1, 5, 233, 7] 输出: True 解释: 玩家1一开始选择1。然后玩家2必须从5和7中进行选择。无论玩家2选择了哪个，玩家1都可以选择233。 最终，玩家1（234分）比玩家2（12分）获得更多的分数，所以返回 True，表示玩家1可以成为赢家。 对于当前玩家，他有两种选择：左边或者右边的数。定义 maxDiff(l, r) 为剩余子数组\\([l,r]\\)时，当前玩家能取得的最大分差，那么 \\[ \\begin{equation*} \\operatorname{maxDiff}(l, r) = \\max \\begin{cases} nums[l] - \\operatorname{maxDiff}(l + 1, r)\\\\\\\\ nums[r] - \\operatorname{maxDiff}(l, r - 1) \\end{cases} \\end{equation*} \\] 对应的时间复杂度可以写出递归式，显然是指数级的： \\[ f(n) = 2f(n-1) = O(2^n) \\] 采用暴力解法可以AC，但运算时间很长，接近TLE边缘 (6300ms)。 {linenos123456789101112131415# AC# Time Complexity: O(2^N)# Slow: 6300msfrom typing import Listclass Solution: def maxDiff(self, l: int, r:int) -&gt; int: if l == r: return self.nums[l] return max(self.nums[l] - self.maxDiff(l + 1, r), self.nums[r] - self.maxDiff(l, r - 1)) def PredictTheWinner(self, nums: List[int]) -&gt; bool: self.nums = nums return self.maxDiff(0, len(nums) - 1) &gt;= 0 从调用图也很容易看出是指数级的复杂度 486 Predict the Winner 暴力解法调用图 n=4 上图中我们有重复计算的节点，例如[1-2]节点被计算了两次。使用 lru_cache 大法，在maxDiff 上仅加了一句，就能以复杂度 \\(O(n^2)\\)和运行时间 43ms AC。 {linenos1234567891011121314151617# AC# Time Complexity: O(N^2)# Fast: 43msfrom functools import lru_cachefrom typing import Listclass Solution: @lru_cache(maxsize=None) def maxDiff(self, l: int, r:int) -&gt; int: if l == r: return self.nums[l] return max(self.nums[l] - self.maxDiff(l + 1, r), self.nums[r] - self.maxDiff(l, r - 1)) def PredictTheWinner(self, nums: List[int]) -&gt; bool: self.nums = nums return self.maxDiff(0, len(nums) - 1) &gt;= 0 动态规划解法调用图可以看出节点 [1-2] 这次没有被计算两次。 486 Predict the Winner 动归解法调用图 n=4 Leetcode 464 Can I Win (中等) 类似但稍有难度的题目 Leetcode 464 Can I Win。难点在于使用了位的状态压缩。 在 \"100 game\" 这个游戏中，两名玩家轮流选择从 1 到 10 的任意整数，累计整数和，先使得累计整数和达到 100 的玩家，即为胜者。 如果我们将游戏规则改为 “玩家不能重复使用整数” 呢？ 例如，两个玩家可以轮流从公共整数池中抽取从 1 到 15 的整数（不放回），直到累计整数和 &gt;= 100。 给定一个整数 maxChoosableInteger （整数池中可选择的最大数）和另一个整数 desiredTotal（累计和），判断先出手的玩家是否能稳赢（假设两位玩家游戏时都表现最佳）？ 你可以假设 maxChoosableInteger 不会大于 20， desiredTotal 不会大于 300。 示例： 输入： maxChoosableInteger = 10 desiredTotal = 11 输出： false 解释： 无论第一个玩家选择哪个整数，他都会失败。 第一个玩家可以选择从 1 到 10 的整数。 如果第一个玩家选择 1，那么第二个玩家只能选择从 2 到 10 的整数。 第二个玩家可以通过选择整数 10（那么累积和为 11 &gt;= desiredTotal），从而取得胜利. 同样地，第一个玩家选择任意其他整数，第二个玩家都会赢。 {linenos123456789101112131415161718192021222324# AC# Time Complexity: O:(2^m*m), m: maxChoosableIntegerclass Solution: from functools import lru_cache @lru_cache(maxsize=None) def recurse(self, status: int, currentTotal: int) -&gt; bool: for i in range(1, self.maxChoosableInteger + 1): if not (status &gt;&gt; i &amp; 1): new_status = 1 &lt;&lt; i | status if currentTotal + i &gt;= self.desiredTotal: return True if not self.recurse(new_status, currentTotal + i): return True return False def canIWin(self, maxChoosableInteger: int, desiredTotal: int) -&gt; bool: self.maxChoosableInteger = maxChoosableInteger self.desiredTotal = desiredTotal sum = maxChoosableInteger * (maxChoosableInteger + 1) / 2 if sum &lt; desiredTotal: return False return self.recurse(0, 0) 上面的代码算法复杂度为\\(O(m 2^m)\\)，m是maxChoosableInteger。由于所有状态的数量是\\(2^m\\)，对于每个状态，最多会尝试 \\(m\\) 走法。 Minimax 算法 至此，我们AC了leetcode中的几道零和回合制博弈游戏。事实上，在这个领域有通用的算法：回合制博弈下的minimax。算法背景如下，两个玩家轮流玩，第一个玩家max的目的是将游戏的效用最大化，第二个玩家min则是最小化效用。比如，下面的节点表示玩家选取节点后游戏的效用，当两个玩家都能采取最优策略，Minimax 算法从底层节点来计算，游戏的结果是最终max 玩家会得到-7。 Wikipedia Minimax 例子 Minimax Python 3伪代码如下。 {linenos12345678910111213def minimax(node: Node, depth: int, maximizingPlayer: bool) -&gt; int: if depth == 0 or is_terminal(node): return evaluate_terminal(node) if maximizingPlayer: value:int = −∞ for child in node: value = max(value, minimax(child, depth − 1, False)) return value else: # minimizing player value := +∞ for child in node: value = min(value, minimax(child, depth − 1, True)) return value Minimax: 486 Predict the Winner 我们知道486 Predict the Winner 是有minimax解法的，但如何具体实现，其难点在于如何定义合适的游戏价值或者效用。之前的解法中，我们定义maxDiff(l, r) 来表示当前玩家面临子区间 \\([l, r]\\) 时能取得的最大分差。对于minimax算法，max 玩家要最大化游戏价值，min玩家要最小化游戏价值。先考虑最简单情况即只有一个数x时，若定义max玩家在此局面下得到这个数时游戏价值为 +x，则min玩家为-x，即max玩家得到的所有数为正（\\(+a_1 + a_2 + ... = A\\)），min玩家得到的所有数为负（\\(-b_1 - b_2 - ... = -B\\)）。至此，max玩家的目标就是 \\(max(A-B)\\) ，min玩家是 \\(min(A-B)\\)。有了精确的定义和优化目标，代码只需要套一下上面的模版。 {linenos12345678910111213141516171819202122232425# ACfrom functools import lru_cachefrom typing import Listclass Solution: # max_player: max(A - B) # min_player: min(A - B) @lru_cache(maxsize=None) def minimax(self, l: int, r: int, isMaxPlayer: bool) -&gt; int: if l == r: return self.nums[l] * (1 if isMaxPlayer else -1) if isMaxPlayer: return max( self.nums[l] + self.minimax(l + 1, r, not isMaxPlayer), self.nums[r] + self.minimax(l, r - 1, not isMaxPlayer)) else: return min( -self.nums[l] + self.minimax(l + 1, r, not isMaxPlayer), -self.nums[r] + self.minimax(l, r - 1, not isMaxPlayer)) def PredictTheWinner(self, nums: List[int]) -&gt; bool: self.nums = nums v = self.minimax(0, len(nums) - 1, True) return v &gt;= 0 Minimax 486 调用图 nums=[1, 5, 2, 4] Minimax: 464 Can I Win 该题目是很典型的此类游戏，即结果为赢输平，但是中间的状态没有直接对应的游戏价值。对于这样的问题，一般定义为，max玩家胜，价值 +1，min玩家胜，价值-1，平则0。下面的AC代码实现了 Minimax 算法。算法中针对两个玩家都有剪枝（没有剪枝无法AC）。具体来说，max玩家一旦在某一节点取得胜利(value=1)，就停止继续向下搜索，因为这是他能取得的最好分数。同理，min玩家一旦取得-1也直接返回上层节点。这个剪枝可以泛化成 alpha beta剪枝算法。 {linenos1234567891011121314151617181920212223242526272829303132# ACclass Solution: from functools import lru_cache @lru_cache(maxsize=None) # currentTotal &lt; desiredTotal def minimax(self, status: int, currentTotal: int, isMaxPlayer: bool) -&gt; int: import math if status == self.allUsed: return 0 # draw: no winner if isMaxPlayer: value = -math.inf for i in range(1, self.maxChoosableInteger + 1): if not (status &gt;&gt; i &amp; 1): new_status = 1 &lt;&lt; i | status if currentTotal + i &gt;= self.desiredTotal: return 1 # shortcut value = max(value, self.minimax(new_status, currentTotal + i, not isMaxPlayer)) if value == 1: return 1 return value else: value = math.inf for i in range(1, self.maxChoosableInteger + 1): if not (status &gt;&gt; i &amp; 1): new_status = 1 &lt;&lt; i | status if currentTotal + i &gt;= self.desiredTotal: return -1 # shortcut value = min(value, self.minimax(new_status, currentTotal + i, not isMaxPlayer)) if value == -1: return -1 return value Alpha-Beta 剪枝 在464 Can I Win minimax 算法代码实现中，我们发现有剪枝优化空间。对于每个节点，定义两个值alpha 和 beta，表示从根节点到目前局面时，max玩家保证能取得的最小值以及min玩家能保证取得的最大值。初始时，根节点alpha = −∞ ， beta = +∞，表示游戏最终的价值在区间 [−∞, +∞]中。在向下遍历的过程中，子节点先继承父节点的 alpha beta 值进而继承区间 [alpha, beta]。当子节点在向下遍历的时候同步更新alpha 或者 beta，一旦区间[alpha, beta]非法就立即向上返回。举个Wikimedia的例子来进一步说明： 根节点初始时： alpha = −∞, beta = +∞ 根节点，最左边子节点返回4后： alpha = 4, beta = +∞ 根节点，中间子节点返回5后： alpha = 5, beta = +∞ 最右Min节点（标1节点），初始时： alpha = 5, beta = +∞ 最右Min节点（标1节点），第一个子节点返回1后： alpha = 5, beta = 1 此时，最右Min节点的alpha, beta形成了无效区间[5, 1]，满足了剪枝条件，因此可以不用计算它的第二个和第三个子节点。如果剩余子节点返回值 &gt; 1，比如2，由于这是个min节点，将会被已经到手的1替换。若其他子节点返回值 &lt; 1，但由于min的父节点有效区间是[5, +∞]，已经保证了&gt;=5，小于5的值也会被忽略。 Wikimedia Alpha Beta 剪枝例子 Alpha Beta 剪枝 Python 3伪代码如下 {linenos12345678910111213141516171819def alpha_beta(node: Node, depth: int, α: int, β: int, maximizingPlayer: bool) -&gt; int: if depth == 0 or is_terminal(node): return evaluate_terminal(node) if maximizingPlayer: value: int = −∞ for child in node: value = max(value, alphabeta(child, depth − 1, α, β, False)) α = max(α, value) if α &gt;= β: break # β cut-off return value else: value: int = +∞ for child in node: value = min(value, alphabeta(child, depth − 1, α, β, True)) β = min(β, value) if β &lt;= α: break # α cut-off return value Alpha-Beta Pruning: 486 Predict the Winner 用 Alpha-Beta 剪枝 再次AC 486。 {linenos1234567891011121314151617181920212223242526272829# ACimport mathfrom functools import lru_cachefrom typing import Listclass Solution: def alpha_beta(self, l: int, r: int, curr: int, isMaxPlayer: bool, alpha: int, beta: int) -&gt; int: if l == r: return curr + self.nums[l] * (1 if isMaxPlayer else -1) if isMaxPlayer: ret = self.alpha_beta(l + 1, r, curr + self.nums[l], not isMaxPlayer, alpha, beta) alpha = max(alpha, ret) if alpha &gt;= beta: return alpha ret = max(ret, self.alpha_beta(l, r - 1, curr + self.nums[r], not isMaxPlayer, alpha, beta)) return ret else: ret = self.alpha_beta(l + 1, r, curr - self.nums[l], not isMaxPlayer, alpha, beta) beta = min(beta, ret) if alpha &gt;= beta: return beta ret = min(ret, self.alpha_beta(l, r - 1, curr - self.nums[r], not isMaxPlayer, alpha, beta)) return ret def PredictTheWinner(self, nums: List[int]) -&gt; bool: self.nums = nums v = self.alpha_beta(0, len(nums) - 1, 0, True, -math.inf, math.inf) return v &gt;= 0 Alpha-Beta Pruning: 464 Can I Win 464 Alpha-Beta 剪枝版本。 {linenos12345678910111213141516171819202122232425262728293031323334# ACclass Solution: from functools import lru_cache @lru_cache(maxsize=None) # currentTotal &lt; desiredTotal def alpha_beta(self, status: int, currentTotal: int, isMaxPlayer: bool, alpha: int, beta: int) -&gt; int: import math if status == self.allUsed: return 0 # draw: no winner if isMaxPlayer: value = -math.inf for i in range(1, self.maxChoosableInteger + 1): if not (status &gt;&gt; i &amp; 1): new_status = 1 &lt;&lt; i | status if currentTotal + i &gt;= self.desiredTotal: return 1 # shortcut value = max(value, self.alpha_beta(new_status, currentTotal + i, not isMaxPlayer, alpha, beta)) alpha = max(alpha, value) if alpha &gt;= beta: return value return value else: value = math.inf for i in range(1, self.maxChoosableInteger + 1): if not (status &gt;&gt; i &amp; 1): new_status = 1 &lt;&lt; i | status if currentTotal + i &gt;= self.desiredTotal: return -1 # shortcut value = min(value, self.alpha_beta(new_status, currentTotal + i, not isMaxPlayer, alpha, beta)) beta = min(beta, value) if alpha &gt;= beta: return value return value C++, Java, Javascript AC 486 Predict the Winner 最后介绍一种不同的DP实现：用C++, Java, Javascript 实现自底向上的DP解法来AC leetcode 486，当然其他语言没有Python的lru_cache大法。以下实现中，注意DP解的构建顺序，先解决小规模的问题，并在此基础上计算稍大的问题。值得一提的是，以下的循环写法严格保证了 \\(n^2\\) 次循环，但是自顶向下的计划递归可能会少于 \\(n^2\\)次循环。 Java AC Code {linenos12345678910111213141516171819// ACclass Solution { public boolean PredictTheWinner(int[] nums) { int n = nums.length; int[][] dp = new int[n][n]; for (int i = 0; i &lt; n; i++) { dp[i][i] = nums[i]; } for (int l = n - 1; l &gt;= 0; l--) { for (int r = l + 1; r &lt; n; r++) { dp[l][r] = Math.max( nums[l] - dp[l + 1][r], nums[r] - dp[l][r - 1]); } } return dp[0][n - 1] &gt;= 0; }} C++ AC Code {linenos1234567891011121314151617// ACclass Solution {public: bool PredictTheWinner(vector&lt;int&gt;&amp; nums) { int n = nums.size(); vector&lt;vector&lt;int&gt;&gt; dp(n, vector&lt;int&gt;(n, 0)); for (int i = 0; i &lt; n; i++) { dp[i][i] = nums[i]; } for (int l = n - 1; l &gt;= 0; l--) { for (int r = l + 1; r &lt; n; r++) { dp[l][r] = max(nums[l] - dp[l + 1][r], nums[r] - dp[l][r - 1]); } } return dp[0][n - 1] &gt;= 0; }}; Javascript AC Code {linenos1234567891011121314151617181920/** * @param {number[]} nums * @return {boolean} */var PredictTheWinner = function(nums) { const n = nums.length; const dp = new Array(n).fill().map(() =&gt; new Array(n)); for (let i = 0; i &lt; n; i++) { dp[i][i] = nums[i]; } for (let l = n - 1; l &gt;=0; l--) { for (let r = i + 1; r &lt; n; r++) { dp[l][r] = Math.max(nums[l] - dp[l + 1][r],nums[r] - dp[l][r - 1]); } } return dp[0][n-1] &gt;=0;};","link":"/zh/2020/combinatorial-game-1-minimax/"},{"title":"组合游戏系列2: 井字棋Leetcode系列题解和Minimax最佳策略实现","text":"继上一篇介绍了Minimax 和Alpha Beta 剪枝算法之后，本篇选择了Leetcode中的井字棋游戏题目，积累相关代码后实现井字棋游戏并扩展到五子棋和N子棋（战略井字棋），随后用Minimax和Alpha Beta剪枝算法解得小规模下N子棋的游戏结局，并分析其状态数量和每一步的最佳策略。后续篇章中，我们基于本篇代码完成一个N子棋的OpenAI Gym 图形环境，可用于人机对战或机器对战，并最终实现棋盘规模稍大的五子棋或者N子棋中的蒙特卡洛树搜索（MCTS）算法。 第一篇: Leetcode中的Minimax 和 Alpha Beta剪枝 第二篇: 井字棋Leetcode系列题解和Minimax最佳策略实现 第三篇: 井字棋、五子棋的OpenAI Gym GUI环境 第四篇: AlphaGo Zero 强化学习算法原理深度分析 第五篇: 井字棋、五子棋AlphaGo Zero 算法实战 Leetcode 上的井字棋系列 Leetcode 1275. 找出井字棋的获胜者 (简单) A 和&nbsp;B&nbsp;在一个&nbsp;3&nbsp;x&nbsp;3&nbsp;的网格上玩井字棋。 井字棋游戏的规则如下： 玩家轮流将棋子放在空方格 (\" \") 上。 第一个玩家 A 总是用&nbsp;\"X\" 作为棋子，而第二个玩家 B 总是用 \"O\" 作为棋子。 \"X\" 和 \"O\" 只能放在空方格中，而不能放在已经被占用的方格上。 只要有 3 个相同的（非空）棋子排成一条直线（行、列、对角线）时，游戏结束。 如果所有方块都放满棋子（不为空），游戏也会结束。 游戏结束后，棋子无法再进行任何移动。 给你一个数组 moves，其中每个元素是大小为 2 的另一个数组（元素分别对应网格的行和列），它按照 A 和 B 的行动顺序（先 A 后 B）记录了两人各自的棋子位置。 如果游戏存在获胜者（A 或 B），就返回该游戏的获胜者；如果游戏以平局结束，则返回 \"Draw\"；如果仍会有行动（游戏未结束），则返回 \"Pending\"。 你可以假设&nbsp;moves&nbsp;都 有效（遵循井字棋规则），网格最初是空的，A 将先行动。 示例 1： 输入：moves = [[0,0],[2,0],[1,1],[2,1],[2,2]] 输出：\"A\" 解释：\"A\" 获胜，他总是先走。 \"X \" \"X \" \"X \" \"X \" \"X \" \" \" -&gt; \" \" -&gt; \" X \" -&gt; \" X \" -&gt; \" X \" \" \" \"O \" \"O \" \"OO \" \"OOX\" 示例 2： 输入：moves = [[0,0],[1,1],[0,1],[0,2],[1,0],[2,0]] 输出：\"B\" 解释：\"B\" 获胜。 \"X \" \"X \" \"XX \" \"XXO\" \"XXO\" \"XXO\" \" \" -&gt; \" O \" -&gt; \" O \" -&gt; \" O \" -&gt; \"XO \" -&gt; \"XO \" \" \" \" \" \" \" \" \" \" \" \"O \" 第一种解法，检查A或者B赢的所有可能情况：某玩家占据8种连线的任意一种情况则胜利，我们使用八个变量来保存所有情况。下面的代码使用了一个小技巧，将moves转换成3x3的棋盘状态数组，元素的值为1，-1和0。1，-1代表两个玩家，0代表空的棋盘格子，其优势在于后续我们只需累加棋盘的值到八个变量中关联的若干个，再检查这八个变量是否满足取胜条件。例如，row[0]表示第一行的状态，当遍历一次所有棋盘格局后，row[0]为第一行的3个格子的总和，只有当row[0] == 3 才表明玩家A占据了第一行，-3表明玩家B占据了第一行。 {linenos12345678910111213141516171819202122232425262728# ACfrom typing import Listclass Solution: def tictactoe(self, moves: List[List[int]]) -&gt; str: board = [[0] * 3 for _ in range(3)] for idx, xy in enumerate(moves): player = 1 if idx % 2 == 0 else -1 board[xy[0]][xy[1]] = player turn = 0 row, col = [0, 0, 0], [0, 0, 0] diag1, diag2 = False, False for r in range(3): for c in range(3): turn += board[r][c] row[r] += board[r][c] col[c] += board[r][c] if r == c: diag1 += board[r][c] if r + c == 2: diag2 += board[r][c] oWin = any(row[r] == 3 for r in range(3)) or any(col[c] == 3 for c in range(3)) or diag1 == 3 or diag2 == 3 xWin = any(row[r] == -3 for r in range(3)) or any(col[c] == -3 for c in range(3)) or diag1 == -3 or diag2 == -3 return \"A\" if oWin else \"B\" if xWin else \"Draw\" if len(moves) == 9 else \"Pending\" 下面我们给出另一种解法，这种解法虽然代码较多，但可以不必遍历棋盘每个格子，比上一种严格遍历一次棋盘的解法略为高效。原理如下，题目保证了moves过程中不会产生输赢结果，因此我们直接检查最后一个棋子向外的八个方向，若任意方向有三连子，则此玩家获胜。这种解法主要是为后续井字棋扩展到五子棋时判断每个落子是否产生输赢做代码准备。 {linenos1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# ACfrom typing import Listclass Solution: def checkWin(self, r: int, c: int) -&gt; bool: north = self.getConnectedNum(r, c, -1, 0) south = self.getConnectedNum(r, c, 1, 0) east = self.getConnectedNum(r, c, 0, 1) west = self.getConnectedNum(r, c, 0, -1) south_east = self.getConnectedNum(r, c, 1, 1) north_west = self.getConnectedNum(r, c, -1, -1) north_east = self.getConnectedNum(r, c, -1, 1) south_west = self.getConnectedNum(r, c, 1, -1) if (north + south + 1 &gt;= 3) or (east + west + 1 &gt;= 3) or \\ (south_east + north_west + 1 &gt;= 3) or (north_east + south_west + 1 &gt;= 3): return True return False def getConnectedNum(self, r: int, c: int, dr: int, dc: int) -&gt; int: player = self.board[r][c] result = 0 i = 1 while True: new_r = r + dr * i new_c = c + dc * i if 0 &lt;= new_r &lt; 3 and 0 &lt;= new_c &lt; 3: if self.board[new_r][new_c] == player: result += 1 else: break else: break i += 1 return result def tictactoe(self, moves: List[List[int]]) -&gt; str: self.board = [[0] * 3 for _ in range(3)] for idx, xy in enumerate(moves): player = 1 if idx % 2 == 0 else -1 self.board[xy[0]][xy[1]] = player # only check last move r, c = moves[-1] win = self.checkWin(r, c) if win: return \"A\" if len(moves) % 2 == 1 else \"B\" return \"Draw\" if len(moves) == 9 else \"Pending\" Leetcode 794. 有效的井字游戏 (中等) 用字符串数组作为井字游戏的游戏板 board。当且仅当在井字游戏过程中，玩家有可能将字符放置成游戏板所显示的状态时，才返回 true。 该游戏板是一个 3 x 3 数组，由字符 \" \"，\"X\" 和 \"O\" 组成。字符 \" \" 代表一个空位。 以下是井字游戏的规则： 玩家轮流将字符放入空位（\" \"）中。 第一个玩家总是放字符 “X”，且第二个玩家总是放字符 “O”。 “X” 和 “O” 只允许放置在空位中，不允许对已放有字符的位置进行填充。 当有 3 个相同（且非空）的字符填充任何行、列或对角线时，游戏结束。 当所有位置非空时，也算为游戏结束。 如果游戏结束，玩家不允许再放置字符。 示例 1: 输入: board = [\"O \", \" \", \" \"] 输出: false 解释: 第一个玩家总是放置“X”。 示例 2: 输入: board = [\"XOX\", \" X \", \" \"] 输出: false 解释: 玩家应该是轮流放置的。 示例 3: 输入: board = [\"XXX\", \" \", \"OOO\"] 输出: false 示例 4: 输入: board = [\"XOX\", \"O O\", \"XOX\"] 输出: true 说明: 游戏板 board 是长度为 3 的字符串数组，其中每个字符串 board[i] 的长度为 3。 board[i][j] 是集合 {\" \", \"X\", \"O\"} 中的一个字符。 这道题第一反应是需要DFS来判断给定状态是否可达，但其实可以用上面1275的思路，即通过检验最终棋盘的一些特点来判断给定状态是否合法。比如，X和O的数量只有可能相同，或X比O多一个。其关键在于需要找到判断状态合法的充要条件，就可以在\\(O(1)\\) 时间复杂度完成判断。 此外，这道题给了我们井字棋所有可能状态数量的启示。 {linenos123456789101112131415161718192021222324252627# ACfrom typing import Listclass Solution: def convertCell(self, c:str): return 1 if c == 'X' else -1 if c == 'O' else 0 def validTicTacToe(self, board: List[str]) -&gt; bool: turn = 0 row, col = [0, 0, 0], [0, 0, 0] diag1, diag2 = False, False for r in range(3): for c in range(3): turn += self.convertCell(board[r][c]) row[r] += self.convertCell(board[r][c]) col[c] += self.convertCell(board[r][c]) if r == c: diag1 += self.convertCell(board[r][c]) if r + c == 2: diag2 += self.convertCell(board[r][c]) xWin = any(row[r] == 3 for r in range(3)) or any(col[c] == 3 for c in range(3)) or diag1 == 3 or diag2 == 3 oWin = any(row[r] == -3 for r in range(3)) or any(col[c] == -3 for c in range(3)) or diag1 == -3 or diag2 == -3 if (xWin and turn == 0) or (oWin and turn == 1): return False return (turn == 0 or turn == 1) and (not xWin or not oWin) Leetcode 348. 判定井字棋胜负 (中等，加锁) 请在 n × n 的棋盘上，实现一个判定井字棋（Tic-Tac-Toe）胜负的神器，判断每一次玩家落子后，是否有胜出的玩家。 在这个井字棋游戏中，会有 2 名玩家，他们将轮流在棋盘上放置自己的棋子。 在实现这个判定器的过程中，你可以假设以下这些规则一定成立： 每一步棋都是在棋盘内的，并且只能被放置在一个空的格子里； 一旦游戏中有一名玩家胜出的话，游戏将不能再继续； 一个玩家如果在同一行、同一列或者同一斜对角线上都放置了自己的棋子，那么他便获得胜利。 示例： 给定棋盘边长 n = 3, 玩家 1 的棋子符号是 \"X\"，玩家 2 的棋子符号是 \"O\"。 TicTacToe toe = new TicTacToe(3); toe.move(0, 0, 1); -&gt; 函数返回 0 (此时，暂时没有玩家赢得这场对决) |X| | | | | | | // 玩家 1 在 (0, 0) 落子。 | | | | toe.move(0, 2, 2); -&gt; 函数返回 0 (暂时没有玩家赢得本场比赛) |X| |O| | | | | // 玩家 2 在 (0, 2) 落子。 | | | | toe.move(2, 2, 1); -&gt; 函数返回 0 (暂时没有玩家赢得比赛) |X| |O| | | | | // 玩家 1 在 (2, 2) 落子。 | | |X| toe.move(1, 1, 2); -&gt; 函数返回 0 (暂没有玩家赢得比赛) |X| |O| | |O| | // 玩家 2 在 (1, 1) 落子。 | | |X| toe.move(2, 0, 1); -&gt; 函数返回 0 (暂无玩家赢得比赛) |X| |O| | |O| | // 玩家 1 在 (2, 0) 落子。 |X| |X| toe.move(1, 0, 2); -&gt; 函数返回 0 (没有玩家赢得比赛) |X| |O| |O|O| | // 玩家 2 在 (1, 0) 落子. |X| |X| toe.move(2, 1, 1); -&gt; 函数返回 1 (此时，玩家 1 赢得了该场比赛) |X| |O| |O|O| | // 玩家 1 在 (2, 1) 落子。 |X|X|X| 348 是道加锁题，对于每次玩家的move，可以用1275第二种解法中的checkWin 函数。下面代码给出了另一种基于1275解法一的方法：保存八个关键变量，每次落子后更新这个子所关联的某几个变量。 {linenos1234567891011121314151617181920212223242526272829303132333435363738# ACclass TicTacToe: def __init__(self, n:int): \"\"\" Initialize your data structure here. :type n: int \"\"\" self.row, self.col, self.diag1, self.diag2, self.n = [0] * n, [0] * n, 0, 0, n def move(self, row:int, col:int, player:int) -&gt; int: \"\"\" Player {player} makes a move at ({row}, {col}). @param row The row of the board. @param col The column of the board. @param player The player, can be either 1 or 2. @return The current winning condition, can be either: 0: No one wins. 1: Player 1 wins. 2: Player 2 wins. \"\"\" if player == 2: player = -1 self.row[row] += player self.col[col] += player if row == col: self.diag1 += player if row + col == self.n - 1: self.diag2 += player if self.n in [self.row[row], self.col[col], self.diag1, self.diag2]: return 1 if -self.n in [self.row[row], self.col[col], self.diag1, self.diag2]: return 2 return 0 井字棋最佳策略 井字棋的规模可以很自然的扩展成四子棋或五子棋等，区别在于棋盘大小和胜利时的连子数量。这类游戏最一般的形式为 M,n,k-game，中文可能翻译为战略井字游戏，表示棋盘大小为M x N，当k连子时获胜。 下面的ConnectNGame类实现了战略井字游戏（M=N）中，两个玩家轮流下子、更新棋盘状态和判断每次落子输赢等逻辑封装。其中undo方法用于撤销最后一个落子，方便在后续寻找最佳策略时回溯。 ConnectNGame {linenos12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class ConnectNGame: PLAYER_A = 1 PLAYER_B = -1 AVAILABLE = 0 RESULT_TIE = 0 RESULT_A_WIN = 1 RESULT_B_WIN = -1 def __init__(self, N:int = 3, board_size:int = 3): assert N &lt;= board_size self.N = N self.board_size = board_size self.board = [[ConnectNGame.AVAILABLE] * board_size for _ in range(board_size)] self.gameOver = False self.gameResult = None self.currentPlayer = ConnectNGame.PLAYER_A self.remainingPosNum = board_size * board_size self.actionStack = [] def move(self, r: int, c: int) -&gt; int: \"\"\" :param r: :param c: :return: None: game ongoing \"\"\" assert self.board[r][c] == ConnectNGame.AVAILABLE self.board[r][c] = self.currentPlayer self.actionStack.append((r, c)) self.remainingPosNum -= 1 if self.checkWin(r, c): self.gameOver = True self.gameResult = self.currentPlayer return self.currentPlayer if self.remainingPosNum == 0: self.gameOver = True self.gameResult = ConnectNGame.RESULT_TIE return ConnectNGame.RESULT_TIE self.currentPlayer *= -1 def undo(self): if len(self.actionStack) &gt; 0: lastAction = self.actionStack.pop() r, c = lastAction self.board[r][c] = ConnectNGame.AVAILABLE self.currentPlayer = ConnectNGame.PLAYER_A if len(self.actionStack) % 2 == 0 else ConnectNGame.PLAYER_B self.remainingPosNum += 1 self.gameOver = False self.gameResult = None else: raise Exception('No lastAction') def getAvailablePositions(self) -&gt; List[Tuple[int, int]]: return [(i,j) for i in range(self.board_size) for j in range(self.board_size) if self.board[i][j] == ConnectNGame.AVAILABLE] def getStatus(self) -&gt; Tuple[Tuple[int, ...]]: return tuple([tuple(self.board[i]) for i in range(self.board_size)]) 其中checkWin和1275解法二中的逻辑一致。 Minimax 算法 此战略井字游戏的逻辑代码，结合之前的minimax算法，可以实现游戏最佳策略。 先定义一个通用的策略基类和抽象方法 action。action表示给定一个棋盘状态，返回一个动作决定。返回Tuple的第一个int值表示估计走这一步的结局，第二个值类型是Tuple[int, int]，表示这次落子的位置，例如（1，1）。 {linenos12345678class Strategy(ABC): def __init__(self): super().__init__() @abstractmethod def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: pass MinimaxStrategy 的逻辑和之前的minimax模版算法大致相同，多了保存最佳move对应的动作，用于最后返回。 {linenos1234567891011121314151617181920212223242526272829303132333435363738class MinimaxStrategy(Strategy): def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: self.game = copy.deepcopy(game) result, move = self.minimax() return result, move def minimax(self) -&gt; Tuple[int, Tuple[int, int]]: game = self.game bestMove = None assert not game.gameOver if game.currentPlayer == ConnectNGame.PLAYER_A: ret = -math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.minimax() game.undo() ret = max(ret, result) bestMove = move if ret == result else bestMove if ret == 1: return 1, move return ret, bestMove else: ret = math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.minimax() game.undo() ret = min(ret, result) bestMove = move if ret == result else bestMove if ret == -1: return -1, move return ret, bestMove 通过上面的代码可以画出初始两步的井字棋最终结局。对于先手O来说可以落9个位置，排除对称位置后只有三种，分别为角落，边上和正中。但无论哪一个位置作为先手，最好的结局都是被对方逼平，不存在必赢的开局。所以井字棋的结局是：如果两个玩家都采用最优策略（无失误），游戏结果为双方逼平。 井字棋第一步结局 下面分别画出三种开局后进一步的游戏结局。 井字棋角落开局 井字棋边上开局 井字棋中间开局 井字棋游戏状态数和解 有趣的是井字棋游戏的状态数量，简单的上限估算是\\(3^9=19683\\)。这显然是个较宽泛的上限，因为很多状态在游戏结束后无法达到。 这篇文章 Tic-Tac-Toe (Naughts and Crosses, Cheese and Crackers, etc 中列出了每一步的状态数，合计5478个。 Moves Positions Terminal Positions 0 1 1 9 2 72 3 252 4 756 5 1260 120 6 1520 148 7 1140 444 8 390 168 9 78 78 Total 5478 958 我们已经实现了井字棋的minimax策略，算法本质上遍历了所有情况，稍加改造后增加dp数组，就可以确认上面的总状态数。 {linenos12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class CountingMinimaxStrategy(Strategy): def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: self.game = copy.deepcopy(game) self.dpMap = {} result, move = self.minimax(game.getStatus()) return result, move def minimax(self, gameStatus: Tuple[Tuple[int, ...]]) -&gt; Tuple[int, Tuple[int, int]]: # print(f'Current {len(strategy.dpMap)}') if gameStatus in self.dpMap: return self.dpMap[gameStatus] game = self.game bestMove = None assert not game.gameOver if game.currentPlayer == ConnectNGame.PLAYER_A: ret = -math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.minimax(game.getStatus()) self.dpMap[game.getStatus()] = result, oppMove else: self.dpMap[game.getStatus()] = result, move game.undo() ret = max(ret, result) bestMove = move if ret == result else bestMove self.dpMap[gameStatus] = ret, bestMove return ret, bestMove else: ret = math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.minimax(game.getStatus()) self.dpMap[game.getStatus()] = result, oppMove else: self.dpMap[game.getStatus()] = result, move game.undo() ret = min(ret, result) bestMove = move if ret == result else bestMove self.dpMap[gameStatus] = ret, bestMove return ret, bestMoveif __name__ == '__main__': tic_tac_toe = ConnectNGame(N=3, board_size=3) strategy = CountingMinimaxStrategy() strategy.action(tic_tac_toe) print(f'Game States Number {len(strategy.dpMap)}') 运行程序证实了井字棋状态数为5478，下面是一些极小规模时代码运行结果： 3x3 4x4 k=3 5478 （Draw) 6035992 （Win） k=4 9722011 （Draw） k=5 根据 Wikipedia M,n,k-game, 列出了一些小规模下的游戏解： 3x3 4x4 5x5 6x6 k=3 Draw Win Win Win k=4 Draw Draw Win k=5 Draw Draw 值得一提的是，五子棋（棋盘15x15或以上）被 L. Victor Allis证明是先手赢。 Alpha-Beta剪枝策略 Alpha Beta 剪枝策略的代码如下（和之前代码比较类似，不再赘述）： {linenos1234567891011121314151617181920212223242526272829303132333435363738394041class AlphaBetaStrategy(Strategy): def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: self.game = game result, move = self.alpha_beta(self.game.getStatus(), -math.inf, math.inf) return result, move def alpha_beta(self, gameStatus: Tuple[Tuple[int, ...]], alpha:int=None, beta:int=None) -&gt; Tuple[int, Tuple[int, int]]: game = self.game bestMove = None assert not game.gameOver if game.currentPlayer == ConnectNGame.PLAYER_A: ret = -math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.alpha_beta(game.getStatus(), alpha, beta) game.undo() alpha = max(alpha, result) ret = max(ret, result) bestMove = move if ret == result else bestMove if alpha &gt;= beta or ret == 1: return ret, move return ret, bestMove else: ret = math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver result, oppMove = self.alpha_beta(game.getStatus(), alpha, beta) game.undo() beta = min(beta, result) ret = min(ret, result) bestMove = move if ret == result else bestMove if alpha &gt;= beta or ret == -1: return ret, move return ret, bestMove Alpha Beta 的DP版本中，由于lru_cache无法指定cache的有效参数，递归函数并没有传入alpha, beta。因此我们将alpha，beta参数隐式放入自己维护的栈中，并保证栈的状态和alpha_beta_dp函数调用状态一致。 {linenos1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class AlphaBetaDPStrategy(Strategy): def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: self.game = game self.alphaBetaStack = [(-math.inf, math.inf)] result, move = self.alpha_beta_dp(self.game.getStatus()) return result, move @lru_cache(maxsize=None) def alpha_beta_dp(self, gameStatus: Tuple[Tuple[int, ...]]) -&gt; Tuple[int, Tuple[int, int]]: alpha, beta = self.alphaBetaStack[-1] game = self.game bestMove = None assert not game.gameOver if game.currentPlayer == ConnectNGame.PLAYER_A: ret = -math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver self.alphaBetaStack.append((alpha, beta)) result, oppMove = self.alpha_beta_dp(game.getStatus()) self.alphaBetaStack.pop() game.undo() alpha = max(alpha, result) ret = max(ret, result) bestMove = move if ret == result else bestMove if alpha &gt;= beta or ret == 1: return ret, move return ret, bestMove else: ret = math.inf for pos in game.getAvailablePositions(): move = pos result = game.move(*pos) if result is None: assert not game.gameOver self.alphaBetaStack.append((alpha, beta)) result, oppMove = self.alpha_beta_dp(game.getStatus()) self.alphaBetaStack.pop() game.undo() beta = min(beta, result) ret = min(ret, result) bestMove = move if ret == result else bestMove if alpha &gt;= beta or ret == -1: return ret, move return ret, bestMove","link":"/zh/2020/combinatorial-game-2-tictactoe/"},{"title":"组合游戏系列3: 井字棋、五子棋的OpenAI Gym GUI环境","text":"继上一篇完成了井字棋（N子棋）的minimax 最佳策略后，我们基于Pygame来创造一个图形游戏环境，可供人机和机器对弈，为后续模拟AlphaGo的自我强化学习算法做环境准备。OpenAI Gym 在强化学习领域是事实标准，我们最终封装成OpenAI Gym的接口。本篇所有代码都在github.com/MyEncyclopedia/ConnectNGym。 第一篇: Leetcode中的Minimax 和 Alpha Beta剪枝 第二篇: 井字棋Leetcode系列题解和Minimax最佳策略实现 第三篇: 井字棋、五子棋的OpenAI Gym GUI环境 第四篇: AlphaGo Zero 强化学习算法原理深度分析 第五篇: 井字棋、五子棋AlphaGo Zero 算法实战 井字棋、五子棋 Pygame 实现 Pygame 井字棋玩家对弈效果 Python 上有Tkinter，PyQt等跨平台GUI类库，主要用于桌面程序编程，但此类库容量较大，编程也相对麻烦。Pygame具有代码少，开发快的优势，比较适合快速开发五子棋这类桌面小游戏。 ### Pygame 极简入门 与所有的GUI开发相同，Pygame也是基于事件的单线程编程模型。下面的例子包含了显示一个最简单GUI窗口，操作系统产生事件并发送到Pygame窗口，while True 控制了python主线程永远轮询事件。我们在这里仅仅判断了当前是否是关闭应用程序事件，如果是则退出进程。此外，clock 用于控制FPS。 {linenos12345678910111213import sysimport pygamepygame.init()display = pygame.display.set_mode((800,600))clock = pygame.time.Clock()while True: for event in pygame.event.get(): if event.type == pygame.QUIT: sys.exit(0) else: pygame.display.update() clock.tick(1) PyGameBoard 主体代码 PyGameBoard类封装了Pygame实现游戏交互和显示的逻辑。上一篇中，我们完成了ConnectNGame逻辑，这里PyGameBoard需要在初始化时，指定传入ConnectNGame 实例（见下图），支持通过API 方式改变其状态，也支持GUI交互方式等待人类玩家的输入。next_user_input(self)实现了等待人类玩家输入的逻辑，本质上是循环检查GUI事件直到有合法的落子产生。 PyGameBoard Class Diagram {linenos12345678910111213141516171819202122232425class PyGameBoard: def __init__(self, connectNGame: ConnectNGame): self.connectNGame = connectNGame pygame.init() def next_user_input(self) -&gt; Tuple[int, int]: self.action = None while not self.action: self.check_event() self._render() self.clock.tick(60) return self.action def move(self, r: int, c: int) -&gt; int: return self.connectNGame.move(r, c) if __name__ == '__main__': connectNGame = ConnectNGame() pygameBoard = PyGameBoard(connectNGame) while not pygameBoard.isGameOver(): pos = pygameBoard.next_user_input() pygameBoard.move(*pos) pygame.quit() check_event 较之极简版本增加了处理用户输入事件，这里我们仅支持人类玩家鼠标输入。方法_handle_user_input 将鼠标点击事件转换成棋盘行列值，并判断点击位置是否合法，合法则返回落子位置，类型为Tuple[int, int]，例如(0, 0)表示棋盘最左上角位置。 {linenos1234567891011121314151617181920212223def check_event(self): for e in pygame.event.get(): if e.type == pygame.QUIT: pygame.quit() sys.exit(0) elif e.type == pygame.MOUSEBUTTONDOWN: self._handle_user_input(e) def _handle_user_input(self, e: Event) -&gt; Tuple[int, int]: origin_x = self.start_x - self.edge_size origin_y = self.start_y - self.edge_size size = (self.board_size - 1) * self.grid_size + self.edge_size * 2 pos = e.pos if origin_x &lt;= pos[0] &lt;= origin_x + size and origin_y &lt;= pos[1] &lt;= origin_y + size: if not self.connectNGame.gameOver: x = pos[0] - origin_x y = pos[1] - origin_y r = int(y // self.grid_size) c = int(x // self.grid_size) valid = self.connectNGame.checkAction(r, c) if valid: self.action = (r, c) return self.action OpenAI Gym 接口规范 OpenAI Gym规范了Agent和环境（Env）之间的互动，核心抽象接口类是gym.Env，自定义的游戏环境需要继承Env，并实现 reset、step和render方法。下面我们看一下如何具体实现ConnectNGym的这几个方法： {linenos12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758class ConnectNGym(gym.Env): def reset(self) -&gt; ConnectNGame: \"\"\"Resets the state of the environment and returns an initial observation. Returns: observation (object): the initial observation. \"\"\" raise NotImplementedError def step(self, action: Tuple[int, int]) -&gt; Tuple[ConnectNGame, int, bool, None]: \"\"\"Run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling `reset()` to reset this environment's state. Accepts an action and returns a tuple (observation, reward, done, info). Args: action (object): an action provided by the agent Returns: observation (object): agent's observation of the current environment reward (float) : amount of reward returned after previous action done (bool): whether the episode has ended, in which case further step() calls will return undefined results info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning) \"\"\" raise NotImplementedError def render(self, mode='human'): \"\"\" Renders the environment. The set of supported modes varies per environment. (And some environments do not support rendering at all.) By convention, if mode is: - human: render to the current display or terminal and return nothing. Usually for human consumption. - rgb_array: Return an numpy.ndarray with shape (x, y, 3), representing RGB values for an x-by-y pixel image, suitable for turning into a video. - ansi: Return a string (str) or StringIO.StringIO containing a terminal-style text representation. The text can include newlines and ANSI escape sequences (e.g. for colors). Note: Make sure that your class's metadata 'render.modes' key includes the list of supported modes. It's recommended to call super() in implementations to use the functionality of this method. Args: mode (str): the mode to render with \"\"\" raise NotImplementedError reset 方法 1def reset(self) -&gt; ConnectNGame 重置环境状态，并返回给Agent重置后环境下观察到的状态。ConnectNGym内部维护了ConnectNGame实例作为自身状态，每个agent落子后会更新这个实例。由于棋类游戏对于玩家来说是完全信息的，我们直接返回ConnectNGame的deepcopy。 step 方法 1def step(self, action: Tuple[int, int]) -&gt; Tuple[ConnectNGame, int, bool, None] Agent 选择了某一action后，由环境来执行这个action并返回4个值：1. 执行后的环境Agent观察到的状态；2. 环境执行了这个action回馈给agent的reward；3. 环境是否结束；4. 其余信息。 step方法是最核心的接口，因此举例来说明ConnectNGym中的输入和输出： 初始状态 状态 ((0, 0, 0), (0, 0, 0), (0, 0, 0)) Agent A 选择action = (0, 0)，执行ConnectNGym.step 后返回值：status = ((1, 0, 0), (0, 0, 0), (0, 0, 0))，reward = 0，game_end = False 状态 ((1, 0, 0), (0, 0, 0), (0, 0, 0)) Agent B 选择action = (1, 1)，执行ConnectNGym.step 后返回值：status = ((1, 0, 0), (0, -1, 0), (0, 0, 0))，reward = 0，game_end = False 状态 ((1, 0, 0), (0, -1, 0), (0, 0, 0)) 重复此过程直至游戏结束，下面是5步后游戏可能达到的最终状态 终结状态 ((1, 1, 1), (-1, -1, 0), (0, 0, 0)) 此时step的返回值为：status = ((1, 1, 1), (-1, -1, 0), (0, 0, 0))，reward = 1，game_end = True render 方法 1def render(self, mode='human') 展现环境，通过mode区分是否是人类玩家。 ConnectNGym 代码 {linenos1234567891011121314151617181920212223242526272829303132333435class ConnectNGym(gym.Env): def __init__(self, pygameBoard: PyGameBoard, isGUI=True, displaySec=2): self.pygameBoard = pygameBoard self.isGUI = isGUI self.displaySec = displaySec self.action_space = spaces.Discrete(pygameBoard.board_size * pygameBoard.board_size) self.observation_space = spaces.Discrete(pygameBoard.board_size * pygameBoard.board_size) self.seed() self.reset() def reset(self) -&gt; ConnectNGame: self.pygameBoard.connectNGame.reset() return copy.deepcopy(self.pygameBoard.connectNGame) def step(self, action: Tuple[int, int]) -&gt; Tuple[ConnectNGame, int, bool, None]: # assert self.action_space.contains(action) r, c = action reward = REWARD_NONE result = self.pygameBoard.move(r, c) if self.pygameBoard.isGameOver(): reward = result return copy.deepcopy(self.pygameBoard.connectNGame), reward, not result is None, None def render(self, mode='human'): if not self.isGUI: self.pygameBoard.connectNGame.drawText() time.sleep(self.displaySec) else: self.pygameBoard.display(sec=self.displaySec) def get_available_actions(self) -&gt; List[Tuple[int, int]]: return self.pygameBoard.getAvailablePositions() 井字棋（N子棋）Minimax策略玩家 图中当k=3,m=n=3即井字棋游戏中，两个minimax策略玩家的对弈效果，游戏结局符合已知的结论：井字棋的解是先手被对方逼平。 Minimax策略AI对弈 镜像游戏状态的DP处理 上一篇中，我们确认了井字棋的总状态数是5478。当k=3, m=n=4时是6035992，k=4, m=n=4时是9722011，总的来说游戏状态数是以指数级增长的。上一版minimax DP策略还有改善的空间，第一种是旋转格局的处理。对于任意一种棋盘格局可以得到90度旋转后的另外三种格局，它们的最佳结局是一致的。因此，我们在递归过程中解得某一棋盘格局后，将其另外三种旋转后格局的解也一起缓存起来。例如： 游戏状态1 旋转后的三种游戏状态 {linenos1234567891011121314151617def similarStatus(self, status: Tuple[Tuple[int, ...]]) -&gt; List[Tuple[Tuple[int, ...]]]: ret = [] rotatedS = status for _ in range(4): rotatedS = self.rotate(rotatedS) ret.append(rotatedS) return retdef rotate(self, status: Tuple[Tuple[int, ...]]) -&gt; Tuple[Tuple[int, ...]]: N = len(status) board = [[ConnectNGame.AVAILABLE] * N for _ in range(N)] for r in range(N): for c in range(N): board[c][N - 1 - r] = status[r][c] return tuple([tuple(board[i]) for i in range(N)]) Minimax 策略预计算 之前我们对每个棋局去计算最佳的下一步，并在此过程中做了剪枝，即当已经找到当前玩家必胜落子时直接返回。这对于单一局面的计算是较优的，但是AI Agent 需要在每一步都重复这个过程，当棋盘大小&gt;3时运算非常耗时，因此我们来做第二种优化。初始空棋盘时使用Minimax来保证遍历所有状态，缓存所有棋局的最佳结果。对于AI Agent面临的每个棋局只需查找此棋局下所有的可能落子位置，并返回最佳决定，这样大大减少了每次棋局下重复的minimax递归计算。相关代码如下。 {linenos123456789101112131415161718192021222324252627282930class PlannedMinimaxStrategy(Strategy): def __init__(self, game: ConnectNGame): super().__init__() self.game = copy.deepcopy(game) self.dpMap = {} # game_status =&gt; result, move self.result = self.minimax(game.getStatus()) def action(self, game: ConnectNGame) -&gt; Tuple[int, Tuple[int, int]]: game = copy.deepcopy(game) player = game.currentPlayer bestResult = player * -1 # assume opponent win as worst result bestMove = None for move in game.getAvailablePositions(): game.move(*move) status = game.getStatus() game.undo() result = self.dpMap[status] if player == ConnectNGame.PLAYER_A: bestResult = max(bestResult, result) else: bestResult = min(bestResult, result) # update bestMove if any improvement bestMove = move if bestResult == result else bestMove print(f'move {move} =&gt; {result}') return bestResult, bestMove Agent 类和对弈逻辑 Agent 类的抽象并不是 OpenAI Gym的规范，出于代码扩展性，我们也封装了Agent基类及其子类，包括AI玩家和人类玩家。BaseAgent需要子类实现 act方法，默认实现为随机决定。 {linenos123456class BaseAgent(object): def __init__(self): pass def act(self, game: PyGameBoard, available_actions): return random.choice(available_actions) AIAgent 实现act并代理给 strategy 的action方法。 {linenos12345678class AIAgent(BaseAgent): def __init__(self, strategy: Strategy): self.strategy = strategy def act(self, game: PyGameBoard, available_actions): result, move = self.strategy.action(game.connectNGame) assert move in available_actions return move HumanAgent 实现act并代理给 PyGameBoard 的next_user_input方法。 {linenos123456class HumanAgent(BaseAgent): def __init__(self): pass def act(self, game: PyGameBoard, available_actions): return game.next_user_input() Agent Class Diagram 下面代码展示如何将Agent，ConnectNGym，PyGameBoard 等所有上述类串联起来，完成人人对弈，人机对弈。 {linenos1234567891011121314151617181920212223242526272829303132def play_ai_vs_ai(env: ConnectNGym): plannedMinimaxAgent = AIAgent(PlannedMinimaxStrategy(env.pygameBoard.connectNGame)) play(env, plannedMinimaxAgent, plannedMinimaxAgent)def play(env: ConnectNGym, agent1: BaseAgent, agent2: BaseAgent): agents = [agent1, agent2] while True: env.reset() done = False agent_id = -1 while not done: agent_id = (agent_id + 1) % 2 available_actions = env.get_available_actions() agent = agents[agent_id] action = agent.act(pygameBoard, available_actions) _, reward, done, info = env.step(action) env.render(True) if done: print(f'result={reward}') time.sleep(3) breakif __name__ == '__main__': pygameBoard = PyGameBoard(connectNGame=ConnectNGame(board_size=3, N=3)) env = ConnectNGym(pygameBoard) env.render(True) play_ai_vs_ai(env) Class Diagram 总览","link":"/zh/2020/combinatorial-game-3-openai-gym-pygame/"},{"title":"组合游戏系列4: AlphaGo Zero 强化学习算法原理深度分析","text":"AlphaGo Zero是Deepmind 最后一代AI围棋算法，因为已经达到了棋类游戏AI的终极目的：给定任何游戏规则，AI从零出发只通过自我对弈的方式提高，最终可以取得超越任何对手（包括顶级人类棋手和上一代AlphaGo）的能力。换种方式说，当给定足够多的时间和计算资源，可以取得无限逼近游戏真实解的能力。这一篇，我们深入分析AlphaGo Zero的设计理念和关键组件的细节并解释组件之间的关联。下一篇中，我们将在已有的N子棋OpenAI Gym 环境中用Pytorch实现一个简化版的AlphaGo Zero算法。 第一篇: Leetcode中的Minimax 和 Alpha Beta剪枝 第二篇: 井字棋Leetcode系列题解和Minimax最佳策略实现 第三篇: 井字棋、五子棋的OpenAI Gym GUI环境 第四篇: AlphaGo Zero 强化学习算法原理深度分析 第五篇: 井字棋、五子棋AlphaGo Zero 算法实战 AlphaGo Zero 综述 AlphaGo Zero 作为Deepmind在围棋领域的最后一代AI Agent，已经可以达到棋类游戏的终极目标：在只给定游戏规则的情况下，AI 棋手从最初始的随机状态开始，通过不断的自我对弈的强化学习来实现超越以往任何人类棋手和上一代Alpha的能力，并且同样的算法和模型应用到了其他棋类也得出相同的效果。这一篇，从原理上来解析AlphaGo Zero的运行方式。 AlphaGo Zero 算法由三种元素构成：强化学习（RL）、深度学习（DL）和蒙特卡洛树搜索（MCTS，Monte Carlo Tree Search）。核心思想是基于神经网络的Policy Iteration强化学习，即最终学的是一个深度学习的policy network，输入是某棋盘局面 s，输出是此局面下可走位的概率分布：\\(p(a|s)\\)。 在第一代AlphaGo算法中，这个初始policy network通过收集专业人类棋手的海量棋局训练得来，再采用传统RL 的Monte Carlo Tree Search Rollout 技术来强化现有的AI对于局面落子（Policy Network）的判断。Monte Carlo Tree Search Rollout 简单说来就是海量棋局模拟，AI Agent在通过现有的Policy Network策略完成一次从某局面节点到最终游戏胜负结束的对弈，这个完整的对弈叫做rollout，又称playout。完成一次rollout之后，通过局面树层层回溯到初始局面节点，并在回溯过程中同步修订所有经过的局面节点的统计指标，修正原先policy network对于落子导致输赢的判断。通过海量并发的棋局模拟来提升基准policy network，即在各种局面下提高好的落子的\\(p(a_{win}|s)\\)，降低坏的落子的\\(p(a_{lose}|s)\\) 举例如下井字棋局面： 局面s 基准policy network返回 p(s) 如下 \\[ p(a|s) = \\begin{align*} \\left\\lbrace \\begin{array}{r@{}l} 0.1, &amp; &amp; a = (0,2) \\\\ 0.05, &amp; &amp; a = (1,0) \\\\ 0.5, &amp; &amp; a = (1,1) \\\\ 0.05, &amp; &amp; a = (1,2)\\\\ 0.2, &amp; &amp; a = (2,0) \\\\ 0.05, &amp; &amp; a = (2,1) \\\\ 0.05, &amp; &amp; a = (2,2) \\end{array} \\right. \\end{align*} \\] 通过海量并发模拟后，修订成如下的action概率分布，然后通过policy iteration迭代新的网络来逼近 \\(p'\\) 就提高了棋力。 \\[ p'(a|s) = \\begin{align*} \\left\\lbrace \\begin{array}{r@{}l} 0, &amp; &amp; a = (0,2) \\\\ 0, &amp; &amp; a = (1,0) \\\\ 0.9, &amp; &amp; a = (1,1) \\\\ 0, &amp; &amp; a = (1,2)\\\\ 0, &amp; &amp; a = (2,0) \\\\ 0, &amp; &amp; a = (2,1) \\\\ 0.1, &amp; &amp; a = (2,2) \\end{array} \\right. \\end{align*} \\] 蒙特卡洛树搜索（MCTS）概述 Monte Carlo Tree Search 是Monte Carlo 在棋类游戏中的变种，棋类游戏的一大特点是可以用动作(move)联系的决策树来表示，树的节点数量取决于分支的数量和树的深度。MCTS的目的是在树节点非常多的情况下，通过实验模拟（rollout, playout）的方式来收集尽可能多的局面输赢情况，并基于这些统计信息，将搜索资源的重点均衡地放在未被探索的节点和值得探索的节点上，减少在大概率输的节点上的模拟资源投入。传统MCTS有四个过程：Selection, Expansion, Simulation 和Backpropagation。下图是Wikipedia 的例子： Selection：从根节点出发，根据现有统计的信息和selection规则，选择子节点递归向下做决定，后面我们会详细介绍AlphaGo的UCB规则。图中节点的数字，例如根节点11/21，分别代表赢的次数和总模拟次数。从根节点一路向下分别选择节点 7/10, 1/6直到叶子节点3/3，叶子节点表示它未被探索过。 Expansion：由于3/3节点未被探索过，初始化其所有子节点为0/0，图中3/3只有一个子节点。后面我们会看到神经网络在初始化子节点的时候起到的指导作用，即所有子节点初始权重并非相同，而是由神经网络给出估计。 Simulation：重复selection和expansion，根据游戏规则递归向下直至游戏结束。 Backpropagation：游戏结束在终点节点产生游戏真实的价值，回溯向上调整所有父节点的统计状态。 权衡 Exploration 和 Exploitation 在不断扩张决策树并收集节点统计信息的同时，MCTS根据规则来权衡探索目的（采样不足）或利用目的来做决策，这个权衡规则叫做Upper Confidence Bound（UCB）。典型的UCB公式如下：w表示通过节点的赢的次数，n表示通过节点的总次数，N是父节点的访问次数，c是调节Exploration 和 Exploitation权重的超参。 \\[ {\\frac{w_i}{n_i}} + c \\sqrt{\\frac{\\ln N_i}{n_i}} \\] 假设某节点有两个子节点s1, s2，它们的统计指标为 s1: w/n = 3/4，s2: w/n = 6/8，由于两者输赢比率一样，因此根据公式，访问次数少的节点出于Exploration的目的胜出，MCTS最终决定从s局面走向s1。 从第一性原理来理解AlphaGo Zero 前一代的AlphaGo已经战胜了世界冠军，取得了空前的成就，AlphaGo Zero 的设计目标变得更加General，去除围棋相关的处理和知识，用统一的框架和算法来解决棋类问题。 1. 无人工先验数据 改进之前需要专家棋手对弈数据来冷启动初始棋力 无特定游戏特征工程 无需围棋特定技巧，只包含下棋规则，可以适用到所有棋类游戏 单一神经网络 统一Policy Network和Value Network，使用一个共享参数的双头神经网络 简单树搜索 去除传统MCTS的Rollout 方式，用神经网络来指导MCTS更有效产生搜索策略 搜索空间的两个优化原则 尽管理论上围棋是有解的，即先手必赢、被逼平或必输，通过遍历所有可能局面可以求得解。同理，通过海量模拟所有可能游戏局面，也可以无限逼近所有局面下的真实输赢概率，直至收敛于局面落子的确切最佳结果。但由于围棋棋局的数目远远大于宇宙原子数目，3^361 &gt;&gt; 10^80，因此需要将计算资源有效的去模拟值得探索的局面，例如对于显然的被动局面减小模拟次数，所以如何有效地减小搜索空间是AlphaGo Zero 需要解决的重大问题。David Silver 在Deepmind AlphaZero - Mastering Games Without Human Knowledge中提到AlphaGo Zero 采用两个原则来有效减小搜索空间。 原则1: 通过Value Network减少搜索的深度 Value Network 通过预测给定局面的value来直接预测最终结果，思想和上一期Minimax DP 策略中直接缓存当前局面的胜负状态一样，减少每次必须靠模拟到最后才能知道当前局面的输赢概率，或者需要多层树搜索才能知道输赢概率。 原则2: 通过Policy Network减少搜索的宽度 搜索广度的减少是由Policy Network预估来达成的，将下一步搜索局限在高概率的动作上，大幅度提升原先MCTS新节点生成后冷启动的搜索宽度。 神经网络结构 AlphaGo Zero 使用一个单一的深度神经网络来完成policy 和value的预测。具体实现方式是将policy network和value network合并成一个共享参数 $ $ 的双头网络。其中z是真实游戏结局的效用，范围为[-1, 1] 。 \\[ (p, v)=f_{\\theta}(s) \\] \\[ p_{a}=\\operatorname{Pr}(a \\mid s) \\] \\[ v = \\mathop{\\mathbb{E}}[z|s] \\] Monte Carlo Tree Search (MCTS) 建立了棋局搜索树，节点的初始状态由神经网络输出的p和v值来估计，由此初始的动作策略和价值预判就会建立在高手的水平之上。模拟一局游戏之后向上回溯，会同步更新路径上节点的统计数值并生成更好的MCTS搜索策略 \\(\\vec{\\pi}\\)。进一步来看，MCTS和神经网络互相形成了正循环。神经网络指导了未知节点的MCTS初始搜索策略，产生自我对弈游戏结局后，通过减小 \\(\\vec{p}\\) 和\\(\\vec{\\pi}\\)的 Loss ，最终又提高了神经网络对于局面的估计能力。神经网络value network的提升也是通过不断减小网络预测的结果和最终结果的差异来提升。 因此，具体神经网络的Loss函数由三部分组成，value network的损失，policy network的损失以及正则项。 \\[ l=\\sum_{t}\\left(v_{\\theta}\\left(s_{t}\\right)-z_{t}\\right)^{2}-\\vec{\\pi}_{t} \\cdot \\log \\left(\\vec{p}_{\\theta}\\left(s_{t}\\right)\\right) + c {\\lVert \\theta \\rVert}^2 \\] AlphaGo Zero MCTS 具体过程 AlphaGo Plays Games Against Itself AlphaGo Zero的MCTS和传统MCTS都有相似的四个过程，但AlphaGo Zero的MCTS步骤相对更复杂。 首先，除了W/N统计指标之外，AlphaGo Zero的MCTS保存了决策边 a|s 的Q(s,a)：Action Value，也就是Q-Learning中的Q值，其初始值由神经网络给出。此外，Q 值也用于串联自底向上更新节点的Value值。具体说来，当某个新节点被Explore后，会将网络给出的Q值向上传递，并逐层更新父节点的Q值。当游戏结局产生时，也会向上更新所有父节点的Q值。 此外对于某一游戏局面s进行多次模拟，每次在局面s出发向下探索，每次探索在已知节点按Selection规则深入一步，直至达到未探索的局面或者游戏结束，产生Q值后向上回溯到最初局面s，回溯过程中更新路径上的局面的统计值或者Q值。在多次模拟结束后根据Play的算法，决定局面s的下一步行动。尽管每次模拟探索可能会深入多层，但最终play阶段的算法规则仅决定给定局面s的下一层落子动作。多次向下探索的优势在于： 探索和采样更多的叶子节点，在更多信息下做决策。 通过average out多次模拟下一层落子决定，尽可能提升MCTS策略的下一步判断能力，提高 \\(\\pi\\) 能力，更有效指导神经网络，提高其学习效率。 New Policy Network V' is Trained to Predict Winner Selection: 从游戏局面s开始，选择a向下递归，直至未展开的节点（搜索树中的叶子节点）或者游戏结局。具体在局面s下选择a的规则由以下UCB(Upper Confidence Bound)决定 \\[ a=\\operatorname{argmax}_a(Q(s,a) + u(s,a)) \\] 其中，Q(s,a) 和u(s,a) 项分别代表Exploitation 和Exploration。两项相加来均衡Exploitation和Exploration，保证初始时每个节点被explore，在有足够多的信息时逐渐偏向exploitation。 \\[ u(s, a)=c_{p u c t} \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\Sigma_{b} N(s, b)}}{1+N(s, a)} \\] Expand 当遇到一个未展开的节点（搜索树中的叶子节点）时，对其每个子节点使用现有网络进行预估，即 \\[ (p(s), v(s))=f_{\\theta}(s) \\] Backup 当新的叶子节点展开时或者到达终点局面时，向上更新父节点的Q值，具体公式为 \\[ Q(s, a)=\\frac{1}{N(s, a)} \\sum_{s^{\\prime} \\mid s, a \\rightarrow s^{\\prime}} V\\left(s^{\\prime}\\right) \\] Play 多次模拟结束后，使用得到搜索概率分布 ${a} $来确定最终的落子动作。正比于访问次数的某次方 $ {a} N(s, a)^{1 / }\\(，其中\\)$为温度参数（temperature parameter）。 New Policy Network V' is Trained to Predict Winner 参考资料 Youtube, Deepmind AlphaZero - Mastering Games Without Human Knowledge, David Silver Mastering the game of Go with deep neural networks and tree search Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm AlphaGo Zero论文解析 AlphaZero实战：从零学下五子棋（附代码）","link":"/zh/2020/combinatorial-game-4-alphago-zero-theory/"},{"title":"组合游戏系列5: 井字棋、五子棋AlphaGo Zero 算法实战","text":"上一篇我们从原理层面解析了AlphaGo Zero如何改进MCTS算法，通过不断自我对弈，最终实现从零棋力开始训练直至能够打败任何高手。在本篇中，我们在已有的N子棋OpenAI Gym 环境中用Pytorch实现一个简化版的AlphaGo Zero算法。本篇所有代码在 github MyEncyclopedia/ConnectNGym 中，其中部分参考了SongXiaoJun 的 AlphaZero_Gomoku。 第一篇: Leetcode中的Minimax 和 Alpha Beta剪枝 第二篇: 井字棋Leetcode系列题解和Minimax最佳策略实现 第三篇: 井字棋、五子棋的OpenAI Gym GUI环境 第四篇: AlphaGo Zero 强化学习算法原理深度分析 第五篇: 井字棋、五子棋AlphaGo Zero 算法实战 AlphaGo Zero MCTS 树节点 上一篇中，我们知道AlphaGo Zero 的MCTS树搜索是基于传统MCTS 的UCT （UCB for Tree）的改进版PUCT（Polynomial Upper Confidence Trees）。局面节点的PUCT值由两部分组成，分别是代表Exploitation的action value Q值，和代表Exploration的U值。 \\[ PUCT(s, a) =Q(s,a) + U(s,a) \\] U值计算由这些参数决定：系数\\(c_{puct}\\)，节点先验概率P(s, a) ，父节点访问次数，本节点的访问次数。具体公式如下 \\[ U(s, a)=c_{p u c t} \\cdot P(s, a) \\cdot \\frac{\\sqrt{\\Sigma_{b} N(s, b)}}{1+N(s, a)} \\] 因此在实现过程中，对于一个树节点来说，需要保存其Q值、节点访问次数 _visit_num和先验概率 _prior。其中，_prior在节点初始化后不变，Q值和 visit_num随着游戏MCTS模拟进程而改变。此外，节点保存了 parent和_children变量，用于维护父子关系。c_puct为class variable，作为全局参数。 {linenos123456789101112class TreeNode: \"\"\" MCTS Tree Node \"\"\" c_puct: ClassVar[int] = 5 # class-wise global param c_puct, exploration weight factor. _parent: TreeNode _children: Dict[int, TreeNode] # map from action to TreeNode _visit_num: int _Q: float # Q value of the node, which is the mean action value. _prior: float 和上面的计算公式相对应，下列代码根据节点状态计算PUCT(s, a)。 {linenos12345678910class TreeNode: def get_puct(self) -&gt; float: \"\"\" Computes AlphaGo Zero PUCT (polynomial upper confidence trees) of the node. :return: Node PUCT value. \"\"\" U = (TreeNode.c_puct * self._prior * np.sqrt(self._parent._visit_num) / (1 + self._visit_num)) return self._Q + U AlphaGo Zero MCTS在playout时遇到已经被展开的节点，会根据selection规则选择子节点，该规则本质上是在所有子节点中选择最大的PUCT值的节点。 \\[ a=\\operatorname{argmax}_a(PUCT(s, a))=\\operatorname{argmax}_a(Q(s,a) + U(s,a)) \\] {linenos123456789class TreeNode: def select(self) -&gt; Tuple[Pos, TreeNode]: \"\"\" Selects an action(Pos) having max UCB value. :return: Action and corresponding node \"\"\" return max(self._children.items(), key=lambda act_node: act_node[1].get_puct()) 新的叶节点一旦在playout时产生，关联的 v 值会一路向上更新至根节点，具体新节点的v值将在下一节中解释。 {linenos1234567891011121314151617181920212223class TreeNode: def propagate_to_root(self, leaf_value: float): \"\"\" Updates current node with observed leaf_value and propagates to root node. :param leaf_value: :return: \"\"\" if self._parent: self._parent.propagate_to_root(-leaf_value) self._update(leaf_value) def _update(self, leaf_value: float): \"\"\" Updates the node by newly observed leaf_value. :param leaf_value: :return: \"\"\" self._visit_num += 1 # new Q is updated towards deviation from existing Q self._Q += 0.5 * (leaf_value - self._Q) AlphaGo Zero MCTS Player 实现 AlphaGo Zero MCTS 在训练阶段分为如下几个步骤。游戏初始局面下，整个局面树的建立由子节点的不断被探索而丰富起来。AlphaGo Zero对弈一次即产生了一次完整的游戏开始到结束的动作系列。在对弈过程中的某一游戏局面，需要采样海量的playout，又称MCTS模拟，以此来决定此局面的下一步动作。一次playout可视为在真实游戏状态树的一种特定采样，playout可能会产生游戏结局，生成真实的v值；也可能explore 到新的叶子节点，此时v值依赖策略价值网络的输出，目的是利用训练的神经网络来产生高质量的游戏对战局面。每次playout会从当前给定局面递归向下，向下的过程中会遇到下面三种节点情况。 若局面节点是游戏结局（叶子节点），可以得到游戏的真实价值 z。从底部节点带着z向上更新沿途节点的Q值，直至根节点（初始局面）。 若局面节点从未被扩展过（叶子节点），此时会将局面编码输入到策略价值双头网络，输出结果为网络预估的action分布和v值。Action分布作为节点先验概率P(s, a)来初始化子节点，预估的v值和上面真实游戏价值z一样，从叶子节点向上沿途更新到根节点。 若局面节点已经被扩展过，则根据PUCT的select规则继续选择下一节点。 海量的playout模拟后，建立了游戏状态树的节点信息。但至此，AI玩家只是收集了信息，还仍未给定局面落子，而落子的决定由Play规则产生。下图展示了给定局面（Current节点）下，MCST模拟进行的多次playout探索后生成的局面树，play规则根据这些节点信息，产生Current 节点的动作分布 \\(\\pi\\) ，确定下一步落子。 MCTS Playout和Play关系 Play 给定局面 对于当前需要做落子决定的某游戏局面\\(s_0\\)，根据如下play公式生成落子分布 $$ ，子局面的落子概率正比于其访问次数的某次方。其中，某次方的倒数称为温度参数（Temperature）。 \\[ \\pi\\left(a \\mid s_{0}\\right)=\\frac{N\\left(s_{0}, a\\right)^{1 / \\tau}}{\\sum_{b} N\\left(s_{0}, b\\right)^{1 / \\tau}} \\] {linenos12345678910111213141516171819class MCTSAlphaGoZeroPlayer(BaseAgent): def _next_step_play_act_probs(self, game: ConnectNGame) -&gt; Tuple[List[Pos], ActionProbs]: \"\"\" For the given game status, run playouts number of times specified by self._playout_num. Returns the action distribution according to AlphaGo Zero MCTS play formula. :param game: :return: actions and their probability \"\"\" for n in range(self._playout_num): self._playout(copy.deepcopy(game)) act_visits = [(act, node._visit_num) for act, node in self._current_root._children.items()] acts, visits = zip(*act_visits) act_probs = softmax(1.0 / MCTSAlphaGoZeroPlayer.temperature * np.log(np.array(visits) + 1e-10)) return acts, act_probs 在训练模式时，考虑到偏向exploration的目的，在\\(\\pi\\) 落子分布的基础上增加了 Dirichlet 分布。 \\[ P(s,a) = (1-\\epsilon)*\\pi(a \\mid s) + \\epsilon * \\boldsymbol{\\eta} \\quad (\\boldsymbol{\\eta} \\sim \\operatorname{Dir}(0.3)) \\] {linenos123456789101112131415161718192021222324252627282930class MCTSAlphaGoZeroPlayer(BaseAgent): def get_action(self, board: PyGameBoard) -&gt; Pos: \"\"\" Method defined in BaseAgent. :param board: :return: next move for the given game board. \"\"\" return self._get_action(copy.deepcopy(board.connect_n_game))[0] def _get_action(self, game: ConnectNGame) -&gt; Tuple[MoveWithProb]: epsilon = 0.25 avail_pos = game.get_avail_pos() move_probs: ActionProbs = np.zeros(game.board_size * game.board_size) assert len(avail_pos) &gt; 0 # the pi defined in AlphaGo Zero paper acts, act_probs = self._next_step_play_act_probs(game) move_probs[list(acts)] = act_probs if self._is_training: # add Dirichlet Noise when training in favour of exploration p_ = (1-epsilon) * act_probs + epsilon * np.random.dirichlet(0.3 * np.ones(len(act_probs))) move = np.random.choice(acts, p=p_) assert move in game.get_avail_pos() else: move = np.random.choice(acts, p=act_probs) self.reset() return move, move_probs 一次完整的对弈 一次完整的AI对弈就是从初始局面迭代play直至游戏结束，对弈生成的数据是一系列的 $(s, , z) $。 如下图 s0 到 s5 是某次井字棋的对弈。最终结局是先手黑棋玩家赢，即对于黑棋玩家 z = +1。需要注意的是：z = +1 是对于所有黑棋面临的局面，即s0, s2, s4，而对应的其余白棋玩家来说 z = -1。 一局完整对弈 \\[ \\begin{align*} &amp;0: (s_0, \\vec{\\pi_0}, +1) \\\\ &amp;1: (s_1, \\vec{\\pi_1}, -1) \\\\ &amp;2: (s_2, \\vec{\\pi_2}, +1) \\\\ &amp;3: (s_3, \\vec{\\pi_3}, -1) \\\\ &amp;4: (s_4, \\vec{\\pi_4}, +1) \\end{align*} \\] 以下代码展示如何在AI对弈时收集数据 $(s, , z) $ {linenos12345678910111213141516171819202122232425262728class MCTSAlphaGoZeroPlayer(BaseAgent): def self_play_one_game(self, game: ConnectNGame) \\ -&gt; List[Tuple[NetGameState, ActionProbs, NDArray[(Any), np.float]]]: \"\"\" :param game: :return: Sequence of (s, pi, z) of a complete game play. The number of list is the game play length. \"\"\" states: List[NetGameState] = [] probs: List[ActionProbs] = [] current_players: List[np.float] = [] while not game.game_over: move, move_probs = self._get_action(game) states.append(convert_game_state(game)) probs.append(move_probs) current_players.append(game.current_player) game.move(move) current_player_z = np.zeros(len(current_players)) current_player_z[np.array(current_players) == game.game_result] = 1.0 current_player_z[np.array(current_players) == -game.game_result] = -1.0 self.reset() return list(zip(states, probs, current_player_z)) Playout 代码实现 一次playout会从当前局面根据PUCT selection规则下沉到叶子节点，如果此叶子节点非游戏终结点，则会扩展当前节点生成下一层新节点，其先验分布由策略价值网络输出的action分布决定。一次playout最终会得到叶子节点的 v 值，并沿着MCTS树向上更新沿途的所有父节点 Q值。 从上一篇文章已知，游戏节点的数量随着参数而指数级增长，举例来说，井字棋（k=3，m=n=3）的状态数量是5478，k=3，m=n=4时是6035992 ，k=m=n=4时是9722011 。如果我们将初始局面节点作为根节点，同时保存海量playout探索得到的局面节点，实现时会发现我们无法将所有探索到的局面节点都保存在内存中。这里的一种解决方法是在一次self play中每轮playout之后，将根节点重置成落子的节点，从而有效控制整颗局面树中的节点数量。 {linenos123456789101112131415161718192021222324252627282930313233343536373839class MCTSAlphaGoZeroPlayer(BaseAgent): def _playout(self, game: ConnectNGame): \"\"\" From current game status, run a sequence down to a leaf node, either because game ends or unexplored node. Get the leaf value of the leaf node, either the actual reward of game or action value returned by policy net. And propagate upwards to root node. :param game: \"\"\" player_id = game.current_player node = self._current_root while True: if node.is_leaf(): break act, node = node.select() game.move(act) # now game state is a leaf node in the tree, either a terminal node or an unexplored node act_and_probs: Iterator[MoveWithProb] act_and_probs, leaf_value = self._policy_value_net.policy_value_fn(game) if not game.game_over: # case where encountering an unexplored leaf node, update leaf_value estimated by policy net to root for act, prob in act_and_probs: game.move(act) child_node = node.expand(act, prob) game.undo() else: # case where game ends, update actual leaf_value to root if game.game_result == ConnectNGame.RESULT_TIE: leaf_value = ConnectNGame.RESULT_TIE else: leaf_value = 1 if game.game_result == player_id else -1 leaf_value = float(leaf_value) # Update leaf_value and propagate up to root node node.propagate_to_root(-leaf_value) 编码游戏局面 为了将信息有效的传递给策略神经网络，必须从当前玩家的角度编码游戏局面。局面不仅要反映棋盘上黑白棋子的位置，也需要考虑最后一个落子的位置以及是否为当前玩家棋局。因此，我们将某局面按照当前玩家来编码，返回类型为4个棋盘大小组成的ndarray，即shape [4, board_size, board_size]，其中 第一个数组编码当前玩家的棋子位置 第二个数组编码对手玩家棋子位置 第三个表示最后落子位置 第四个全1表示此局面为先手（黑棋）局面，全0表示白棋局面 例如之前游戏对弈中的前四步： s1-&gt;s2 后局面s2的编码：当前玩家为黑棋玩家，编码局面s2 返回如下ndarray，数组[0] 为s2黑子位置，[1]为白子位置，[2]表示最后一个落子(1, 1) ，[3] 全1表示当前是黑棋落子的局面。 编码黑棋玩家局面 s2 s2-&gt;s3 后局面s3的编码：当前玩家为白棋玩家，编码返回如下，数组[0] 为s3白子位置，[1]为黑子位置，[2]表示最后一个落子(1, 0) ，[3] 全0表示当前是白棋落子的局面。 编码白棋玩家局面 s3 具体代码实现如下。 {linenos123456789101112131415161718192021222324252627282930NetGameState = NDArray[(4, Any, Any), np.int]def convert_game_state(game: ConnectNGame) -&gt; NetGameState: \"\"\" Converts game state to type NetGameState as ndarray. :param game: :return: Of shape 4 * board_size * board_size. [0] is current player positions. [1] is opponent positions. [2] is last move location. [3] all 1 meaning move by black player, all 0 meaning move by white. \"\"\" state_matrix = np.zeros((4, game.board_size, game.board_size)) if game.action_stack: actions = np.array(game.action_stack) move_curr = actions[::2] move_oppo = actions[1::2] for move in move_curr: state_matrix[0][move] = 1.0 for move in move_oppo: state_matrix[1][move] = 1.0 # indicate the last move location state_matrix[2][actions[-1]] = 1.0 if len(game.action_stack) % 2 == 0: state_matrix[3][:, :] = 1.0 # indicate the colour to play return state_matrix[:, ::-1, :] 策略价值网络训练 策略价值网络是一个共享参数 \\(\\theta\\) 的双头网络，给定上面的游戏局面编码会产生预估的p和v。 \\[ \\vec{p_{\\theta}}, v_{\\theta}=f_{\\theta}(s) \\] 结合真实游戏对弈后产生三元组数据 $(s, , z) $ ，按照论文中的loss 来训练神经网络。 \\[ l=\\sum_{t}\\left(v_{\\theta}\\left(s_{t}\\right)-z_{t}\\right)^{2}-\\vec{\\pi_{t}} \\cdot \\log \\left(\\vec{p_{\\theta}}\\left(s_{t}\\right)\\right) + c {\\lVert \\theta \\rVert}^2 \\] 下面代码为Pytorch backward部分。 {linenos123456789101112131415161718192021222324def backward_step(self, state_batch: List[NetGameState], probs_batch: List[ActionProbs], value_batch: List[NDArray[(Any), np.float]], lr) -&gt; Tuple[float, float]: if self.use_gpu: state_batch = Variable(torch.FloatTensor(state_batch).cuda()) probs_batch = Variable(torch.FloatTensor(probs_batch).cuda()) value_batch = Variable(torch.FloatTensor(value_batch).cuda()) else: state_batch = Variable(torch.FloatTensor(state_batch)) probs_batch = Variable(torch.FloatTensor(probs_batch)) value_batch = Variable(torch.FloatTensor(value_batch)) self.optimizer.zero_grad() for param_group in self.optimizer.param_groups: param_group['lr'] = lr log_act_probs, value = self.policy_value_net(state_batch) # loss = (z - v)^2 - pi*T * log(p) + c||theta||^2 value_loss = F.mse_loss(value.view(-1), value_batch) policy_loss = -torch.mean(torch.sum(probs_batch * log_act_probs, 1)) loss = value_loss + policy_loss loss.backward() self.optimizer.step() entropy = -torch.mean(torch.sum(torch.exp(log_act_probs) * log_act_probs, 1)) return loss.item(), entropy.item() 参考资料 Youtube, Deepmind AlphaZero - Mastering Games Without Human Knowledge, David Silver Mastering the game of Go with deep neural networks and tree search Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm AlphaGo Zero论文解析 AlphaZero实战：从零学下五子棋（附代码）","link":"/zh/2020/combinatorial-game-5-alphago-zero-connect-n/"},{"title":"从零构建统计随机变量生成器之离散基础篇","text":"在本系列中，我们会从第一性原理出发，从零开始构建统计学中的常见分布的随机变量生成器，包括二项分布，泊松分布，高斯分布等。在实现这些基础常见分布的过程中，会展示如何使用统计模拟的通用技术，包括 inverse CDF，Box-Muller，分布转换等。本期通过伯努利试验串联起来基础离散分布并通过代码来实现这些分布的生成函数，从零开始构建的原则是随机变量生成器实现只依赖 random() 产生 [0, 1.0] 之间的浮点数，不依赖于其他第三方API来完成。 从零构建统计随机变量生成器之 离散基础篇 从零构建统计随机变量生成器之 用逆变换采样方法构建随机变量生成器 深入 LeetCode 470 拒绝采样，状态转移图求期望和一道经典统计求期望题目 从零构建统计随机变量生成器之 正态分布 Box-Muller方法 均匀分布（离散） 离散均匀分布（Discrete Uniform Distribution）的随机变量是最为基本的，图中为 [0, 6] 七个整数的离散均匀分布。算法实现为，使用 [0, 1] 之间的随机数 u，再将 u 等比例扩展到指定的整数上下界。 实现代码 1234567import randomfrom math import floordef uniform(a: int, b: int) -&gt; int: assert a &lt;= b u = random.random() return a + floor((b - a + 1) * u) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_uniform.py 伯努利分布 伯努利分布（Bernoulli Distribution）是support为0或者1的离散分布，0和1可以看成失败和成功两种可能。伯努利分布指定了成功的概率p，例如，下图是 p=0.4 的伯努利分布。 伯努利分布随机数实现也很直接，将随机值 u 根据 p 决定成功或者失败。 实现代码 123456import randomdef bernoulli(p: float) -&gt; int: assert 0 &lt;= p &lt;= 1 u = random.random() return 1 if u &lt;= p else 0 Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_bernoulli.py 类别分布 类别分布（Categorical Distribution）是在伯努利分布的基础上扩展到了多个点，每个点同样由参数指定了其概率，因此，其参数从 p 扩展到了向量 \\(\\vec p\\)，如图所示为 $p = [0.2, 0.3, 0.1, 0.4] $ 时的类别分布。 实现代码 类别分布生成函数也扩展了伯努利分布的实现算法，将随机数 u 和累计概率向量作比较。在这个例子中， $p = [0.2, 0.3, 0.1, 0.4] $ 转换成 $c = [0.2, 0.5, 0.6, 1] $，再将 u 和 \\(\\vec c\\)数组匹配，返回结果为第一个大于 u 的元素 index。实现上，我们可以以线性复杂度遍历数组，更好一点的方法是，用 python bisect函数通过二分法找到index，将时间复杂度降到 \\(O(log(n))\\)。 12345678910111213import bisectimport randomfrom typing import Listdef categorical(probs: List[float]) -&gt; int: assert abs(sum(probs) - 1.0) &lt; 0.001 cum = probs.copy() for i in range(1, len(cum)): cum[i] = cum[i-1] + probs[i] u = random.random() return bisect.bisect(cum, u) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_categorical.py 二项分布 二项分布（Binomial Distribution）有两个参数 n 和 p，表示伯努利实验做n次后成功的次数。图中为 n=6，p=0.5的二项分布。 实现代码 二项分布生成算法可以通过伯努利试验的故事来实现，即调用 n 次伯努利分布生成函数，返回总的成功次数。 12def binomial(n: int, p: float) -&gt; int: return sum(bernoulli(p) for _ in range(n)) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_binomial.py 概率质量函数（PMF） \\[ \\operatorname{Pr}_\\text{Binomial}(X=k)=\\left(\\begin{array}{c}n \\\\ k\\end{array}\\right)p^{k}(1- p)^{n-k} \\] 几何分布 几何分布（Geometric Distribution）和伯努利实验的关系是：几何分布是反复伯努利实验直至第一次成功时的失败次数。如图，当成功概率 p=0.4时的几何分布。 实现代码 1234567from discrete_bernoulli import bernoullidef geometric(p: float) -&gt; int: fail_num = 0 while not bernoulli(p): fail_num += 1 return fail_num Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_geometric.py 概率质量函数（PMF） \\[ \\operatorname{Pr}_\\text{Geometric}(X=k)=(1-p)^{k-1} p \\] 负二项分布 负二项分布（Negative Binomial Distribution）是尝试伯努利试验直至成功 r 次的失败次数。 实现代码 1234567891011from discrete_bernoulli import bernoullidef negative_binomial(r: int, p: float) -&gt; int: failures = 0 while r: success = bernoulli(p) if success: r -= 1 else: failures += 1 return failures Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_nagative_binomial.py 概率质量函数（PMF） \\[ \\operatorname{Pr}_\\text{NegBinomial}(X=k)=\\left(\\begin{array}{c}k+r-1 \\\\ r-1\\end{array}\\right)(1-p)^{k} p^{r} \\] 超几何分布 超几何分布（HyperGeometric Distribution）的意义是从总数为N的集合抽取n次后成功的次数。具体来说，集合由K个表示成功的元素和N-K个表示失败的元素组成，并且抽取时没有替换（without replacement）情况下的成功次数。注意，超几何分布和二项分布的区别仅在于有无替换。 实现代码 12345678910111213141516from discrete_bernoulli import bernoullidef hypergeometric(N: int, K_succ_num: int, n_trial_num: int) -&gt; int: x = N - K_succ_num n_hit = 0 while n_trial_num: hit = bernoulli(K_succ_num / (K_succ_num + x)) n_hit += hit if hit: K_succ_num -= 1 else: x -= 1 if K_succ_num == 0: return n_hit n_trial_num -= 1 return n_hit Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_hypergeometric.py 概率质量函数（PMF） \\[ \\operatorname{Pr}_\\text{Hypergeo}(X=k)=\\frac{\\left(\\begin{array}{c}K \\\\ k\\end{array}\\right)\\left(\\begin{array}{c}N-k \\\\ n-k\\end{array}\\right)}{\\left(\\begin{array}{l}N \\\\ n\\end{array}\\right)} \\] 负超几何分布 负超几何分布（Negative Hypergeometric Distribution）的意义是从总数为N的集合中，无替换下抽取直至 r 次失败时，成功的次数。 实现代码 123456789101112131415161718from discrete_bernoulli import bernoullidef negative_hypergeometric(N: int, K_success_num: int, r_fail_times: int) -&gt; int: fail_num = N - K_success_num succ_trials = 0 while r_fail_times: success = bernoulli(K_success_num / (K_success_num + fail_num)) if success: K_success_num -= 1 succ_trials += 1 if K_success_num == 0: # no more success elements return succ_trials else: fail_num -= 1 r_fail_times -= 1 return succ_trials Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_negative_hypergeometric.py 概率质量函数（PMF） \\[ \\operatorname{Pr}_\\text{NegHypergeo}(X=k)=\\frac{\\left(\\begin{array}{c}k+r-1 \\\\ k\\end{array}\\right)\\left(\\begin{array}{c}N-r-k \\\\ K-k\\end{array}\\right)}{\\left(\\begin{array}{l}N \\\\ K\\end{array}\\right)} \\quad \\text{for } k=0,1,2, \\ldots, K \\] 伯努利试验总结 下表总结了上面四种和伯努利试验有关的离散分布的具体区别。 有替换 无替换 固定尝试次数 二项 Binomial 超几何 Hypergeometric 固定成功次数 负二项 Negative Binomial 负超几何 Negative Hypergeometric","link":"/zh/2020/distribution-discrete-generator/"},{"title":"从蒙特卡罗模拟，数学递推到直觉模式来思考 Leetcode 1227 飞机座位分配概率","text":"Leetcode 1227 是一道有意思的概率题，本篇将从多个角度来讨论这道题。题目如下 有 n 位乘客即将登机，飞机正好有 n 个座位。第一位乘客的票丢了，他随便选了一个座位坐下。 剩下的乘客将会： 如果他们自己的座位还空着，就坐到自己的座位上， 当他们自己的座位被占用时，随机选择其他座位，第 n 位乘客坐在自己的座位上的概率是多少？ 示例 1： 输入：n = 1 输出：1.00000 解释：第一个人只会坐在自己的位置上。 示例 2： 输入: n = 2 输出: 0.50000 解释：在第一个人选好座位坐下后，第二个人坐在自己的座位上的概率是 0.5。 提示： 1 &lt;= n &lt;= 10^5 假设规模为n时答案为f(n)，一般来说，这种递推问题在数学形式上可能有关于n的简单数学表达式（closed form），或者肯定有f(n)关于f(n-k)的递推表达式。工程上，我们可以通过通过多次模拟即蒙特卡罗模拟来算得近似的数值解。 Monte Carlo 模拟发现规律 首先，我们先来看看如何高效的用代码来模拟。根据题意的描述过程，直接可以写出下面代码。seats为n大小的bool 数组，每个位置表示此位置是否已经被占据。然后依次给第i个人按题意分配座位。注意，每次参数随机数范围在[0,n-1]，因此，会出现已经被占据的情况，此时需要再次随机，直至分配到空位。 暴力直接模拟 {linenos12345678910111213141516171819202122def simulate_bruteforce(n: int) -&gt; bool: \"\"\" Simulates one round. Unbounded time complexity. :param n: total number of seats :return: True if last one has last seat, otherwise False \"\"\" seats = [False for _ in range(n)] for i in range(n-1): if i == 0: # first one, always random seats[random.randint(0, n - 1)] = True else: if not seats[i]: # i-th has his seat seats[i] = True else: while True: rnd = random.randint(0, n - 1) # random until no conflicts if not seats[rnd]: seats[rnd] = True break return not seats[n-1] 运行上面的代码来模拟 n 从 2 到10 的情况，每种情况跑500次模拟，输出如下 123456789101 =&gt; 1.02 =&gt; 0.553 =&gt; 0.544 =&gt; 0.4865 =&gt; 0.4886 =&gt; 0.4987 =&gt; 0.5268 =&gt; 0.5049 =&gt; 0.48210 =&gt; 0.494 发现当 n&gt;=2 时，似乎概率都是0.5。 标准答案 其实，这道题的标准答案就是 n=1 为1，n&gt;=2 为0.5。下面是 python 3 标准答案。本篇后面会从多个角度来探讨为什么是0.5 。 {linenos123class Solution: def nthPersonGetsNthSeat(self, n: int) -&gt; float: return 1.0 if n == 1 else 0.5 O(n) 改进算法 上面的暴力直接模拟版本有个最大的问题是当n很大时，随机分配座位会产生大量冲突，因此，最坏复杂度是没有任何上限的。解决方法是每次发生随机分配时保证不冲突，能直接选到空位。下面是一种最坏复杂度O(n)的模拟过程，seats数组初始话成 0，1，...，n-1，表示座位号。当第i个人登机时，seats[i:n] 的值为他可以选择的座位集合，而seats[0:i]为已经被占据的座位集合。由于[i: n]是连续空间，产生随机数就能保证不冲突。当第i个人选完座位时，将他选中的seats[k]和seats[i] 交换，保证第i+i个人面临的seats[i+1:n]依然为可选座位集合。 {linenos1234567891011121314151617181920212223242526def simulate_online(n: int) -&gt; bool: \"\"\" Simulates one round of complexity O(N). :param n: total number of seats :return: True if last one has last seat, otherwise False \"\"\" seats = [i for i in range(n)] def swap(i, j): tmp = seats[i] seats[i] = seats[j] seats[j] = tmp # for each person, the seats array idx available are [i, n-1] for i in range(n-1): if i == 0: # first one, always random rnd = random.randint(0, n - 1) swap(rnd, 0) else: if seats[i] == i: # i-th still has his seat pass else: rnd = random.randint(i, n - 1) # selects idx from [i, n-1] swap(rnd, i) return seats[n-1] == n - 1 递推思维 这一节我们用数学递推思维来解释0.5的解。令f(n) 为第 n 位乘客坐在自己的座位上的概率，考察第一个人的情况（first step analysis），有三种可能 第一个人选了第一个即自己的座位，那么最后一个人一定能保证坐在自己的座位。 第一个人选了最后一个人的座位，无论中间什么过程，最后一个人无法坐到自己座位 第一个人选了第i个座位，(1&lt;i&lt;n)，那么第i个人前面的除了第一个外的人都会坐在自己位置上，第i个人由于没有自己座位，随机在剩余的座位1，座位 [i+1,n] 中随机选择，此时，问题转变为f(n-i+1)，如下图所示。 第一个人选了位置i 第i个人将问题转换成f(n-i+1) 通过上面分析，得到概率递推关系如下 \\[ f(n) = \\begin{align*} \\left\\lbrace \\begin{array}{r@{}l} 1 &amp; &amp; p=\\frac{1}{n} \\quad \\text{选了第一个位置} \\\\\\\\\\\\ f(n-i+1) &amp; &amp; p=\\frac{1}{n} \\quad \\text{选了第i个位置，1&lt;i&lt;n} \\\\\\\\\\\\ 0 &amp; &amp; p=\\frac{1}{n} \\quad \\text{选了第n个位置} \\end{array} \\right. \\end{align*} \\] 即f(n)的递推式为： \\[ f(n) = \\frac{1}{n} + \\frac{1}{n} \\times [ f(n-1) + f(n-2) + ...+ f(2)], \\quad n&gt;=2 \\] 同理，f(n+1)递推式如下 \\[ f(n+1) = \\frac{1}{n+1} + \\frac{1}{n+1} \\times [ f(n) + f(n-1) + ...+ f(2)] \\] \\((n+1)f(n+1) - nf(n)\\) 抵消 \\(f(n-1) + ...f(2)\\) 项，可得 \\[ (n+1)f(n+1) - nf(n) = f(n) \\] 即 \\[ f(n+1) = f(n) = \\frac{1}{2} \\quad n&gt;=2 \\] 用数学归纳法也可以证明 n&gt;=2 时 f(n)=0.5。 简化的思考方式 我们再仔细思考一下上面的第三种情况，就是第一个人坐了第i个座位，1&lt;i&lt;n，此时，程序继续，不产生结果，直至产生结局1或者2，也就是case 1和2是真正的结局节点，它们产生的概率相同，因此答案是1/2。 从调用图可以看出这种关系，由于中间节点 f(4)，f(3)，f(2)生成Case 1和2的概率一样，因此无论它们之间是什么关系，最后结果都是1/2. 知乎上有个很形象的类比理解方式 考虑一枚硬币，正面向上的概率为 1/n，反面也是，立起来的概率为 (n-2)/n 。我们规定硬币立起来重新抛，但重新抛时，n会至少减小1。求结果为反面的概率。这样很显然结果为 1/2 。 这里，正面向上对应Case 2，反面对应Case 1。 这种思想可以写出如下代码，seats为 n 大小的bool 数组，当第i个人（0&lt;i&lt;n）发现自己座位被占的话，此时必然seats[0]没有被占，同时seats[i+1:]都是空的。假设seats[0]被占的话，要么是第一个人占的，要么是第p个人（p&lt;i）坐了，两种情况下乱序都已经恢复了，此时第i个座位一定是空的。 {linenos123456789101112131415161718192021222324def simulate(n: int) -&gt; bool: \"\"\" Simulates one round of complexity O(N). :param n: total number of seats :return: True if last one has last seat, otherwise False \"\"\" seats = [False for _ in range(n)] for i in range(n-1): if i == 0: # first one, always random rnd = random.randint(0, n - 1) seats[rnd] = True else: if not seats[i]: # i-th still has his seat seats[i] = True else: # 0 must not be available, now we have 0 and [i+1, n-1], rnd = random.randint(i, n - 1) if rnd == i: seats[0] = True else: seats[rnd] = True return not seats[n-1]","link":"/zh/2020/leetcode-1227-airplane-seat-assignment-probability/"},{"title":"Leetcode 679 24 Game 的 Python 函数式实现","text":"Leetcode 679 24 Game (Hard) 先来介绍一下24点游戏题目，大家一定都玩过，就是给定4个牌面数字，用加减乘除计算24点。 本篇会用两种偏函数式的 Python 3解法来AC 24 Game。 Leetcode 679 24 Game (Hard) &gt; You have 4 cards each containing a number from 1 to 9. You need to judge whether they could operated through *, /, +, -, (, ) to get the value of 24. Example 1: Input: [4, 1, 8, 7] Output: True Explanation: (8-4) * (7-1) = 24 Example 2: Input: [1, 2, 1, 2] Output: False itertools.permutations 先来介绍一下Python itertools.permutations 的用法，正好用Leetcode 中的Permutation问题来示例。Permutations 的输入可以是List，返回是 generator 实例，用于生成所有排列。简而言之，python 的 generator 可以和List一样，用 for 语句来全部遍历产生的值。和List不同的是，generator 的所有值并不必须全部初始化，一般按需产生从而大量减少内存占用。下面在介绍 yield 时我们会看到如何合理构造 generator。 Leetcode 46 Permutations (Medium) &gt; Given a collection of distinct integers, return all possible permutations. Example: Input: [1,2,3] Output: [ [1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]] 用 permutations 很直白，代码只有一行。 {linenos1234567891011# AC# Runtime: 36 ms, faster than 91.78% of Python3 online submissions for Permutations.# Memory Usage: 13.9 MB, less than 66.52% of Python3 online submissions for Permutations.from itertools import permutationsfrom typing import Listclass Solution: def permute(self, nums: List[int]) -&gt; List[List[int]]: return [p for p in permutations(nums)] itertools.combinations 有了排列就少不了组合，itertools.combinations 可以产生给定List的k个元素组合 \\(\\binom{n}{k}\\)，用一道算法题来举例，同样也是一句语句就可以AC。 Leetcode 77 Combinations (Medium) Given two integers n and k, return all possible combinations of k numbers out of 1 ... n. You may return the answer in any order. Example 1: Input: n = 4, k = 2 Output: [ [2,4], [3,4], [2,3], [1,2], [1,3], [1,4],] Example 2: Input: n = 1, k = 1 Output: [[1]] {linenos123456789# AC# Runtime: 84 ms, faster than 95.43% of Python3 online submissions for Combinations.# Memory Usage: 15.2 MB, less than 68.98% of Python3 online submissions for Combinations.from itertools import combinationsfrom typing import Listclass Solution: def combine(self, n: int, k: int) -&gt; List[List[int]]: return [c for c in combinations(list(range(1, n + 1)), k)] itertools.product 当有多维度的对象需要迭代笛卡尔积时，可以用 product(iter1, iter2, ...)来生成generator，等价于多重 for 循环。 12[lst for lst in product([1, 2, 3], ['a', 'b'])][(i, s) for i in [1, 2, 3] for s in ['a', 'b']] 这两种方式都生成了如下结果 1[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b'), (3, 'a'), (3, 'b')] 再举一个Leetcode的例子来实战product generator。 Leetcode 17. Letter Combinations of a Phone Number (Medium) Given a string containing digits from 2-9 inclusive, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below. Note that 1 does not map to any letters. Example: Input: \"23\" Output: [\"ad\", \"ae\", \"af\", \"bd\", \"be\", \"bf\", \"cd\", \"ce\", \"cf\"]. 举例来说，下面的代码当输入 digits 是 '352' 时，iter_dims 的值是 ['def', 'jkl', 'abc']，再输入给 product 后会产生 'dja', 'djb', 'djc', 'eja', 共 3 x 3 x 3 = 27个组合的值。 {linenos1234567891011121314151617181920# AC# Runtime: 24 ms, faster than 94.50% of Python3 online submissions for Letter Combinations of a Phone Number.# Memory Usage: 13.7 MB, less than 83.64% of Python3 online submissions for Letter Combinations of a Phone Number.from itertools import productfrom typing import Listclass Solution: def letterCombinations(self, digits: str) -&gt; List[str]: if digits == \"\": return [] mapping = {'2':'abc', '3':'def', '4':'ghi', '5':'jkl', '6':'mno', '7':'pqrs', '8':'tuv', '9':'wxyz'} iter_dims = [mapping[i] for i in digits] result = [] for lst in product(*iter_dims): result.append(''.join(lst)) return result yield 示例 Python具有独特的itertools generator，可以花式AC代码，接下来讲解如何进一步构造 generator。Python 定义只要函数中使用了yield关键字，这个函数就是 generator。Generator 在计算机领域的标准名称是 coroutine，即协程，是一种特殊的函数：当返回上层调用时自身能够保存调用栈状态，并在上层函数处理完逻辑后跳入到这个 generator，恢复之前的状态再继续运行下去。Yield语句也举一道经典的Fibonacci 问题。 Leetcode 509. Fibonacci Number (Easy) The Fibonacci numbers, commonly denoted F(n) form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is, F(0) = 0, F(1) = 1 F(N) = F(N - 1) + F(N - 2), for N &gt; 1. Given N, calculate F(N). Example 1: Input: 2 Output: 1 Explanation: F(2) = F(1) + F(0) = 1 + 0 = 1. Example 2: Input: 3 Output: 2 Explanation: F(3) = F(2) + F(1) = 1 + 1 = 2. Example 3: Input: 4 Output: 3 Explanation: F(4) = F(3) + F(2) = 2 + 1 = 3. Fibonacci 的一般标准解法是循环迭代方式，可以以O(n)时间复杂度和O(1) 空间复杂度来AC。下面的 yield 版本中，我们构造了fib_next generator，它保存了最后两个值作为内部迭代状态，外部每调用一次可以得到下一个fib(n)，如此外部只需不断调用直到满足题目给定次数。 {linenos1234567891011121314151617181920# AC# Runtime: 28 ms, faster than 85.56% of Python3 online submissions for Fibonacci Number.# Memory Usage: 13.8 MB, less than 58.41% of Python3 online submissions for Fibonacci Number.class Solution: def fib(self, N: int) -&gt; int: if N &lt;= 1: return N i = 2 for fib in self.fib_next(): if i == N: return fib i += 1 def fib_next(self): f_last2, f_last = 0, 1 while True: f = f_last2 + f_last f_last2, f_last = f_last, f yield f yield from 示例 上述yield用法之后，再来演示 yield from 的用法。Yield from 始于Python 3.3，用于嵌套generator时的控制转移，一种典型的用法是有多个generator嵌套时，外层的outer_generator 用 yield from 这种方式等价代替如下代码。 123def outer_generator(): for i in inner_generator(): yield i 用一道算法题目来具体示例。 Leetcode 230. Kth Smallest Element in a BST (Medium) Given a binary search tree, write a function kthSmallest to find the kth smallest element in it. Example 1: Input: root = [3,1,4,null,2], k = 1 12345 3 / \\1 4 \\ 2 Output: 1 Example 2: Input: root = [5,3,6,2,4,null,null,1], k = 3 1234567 5 / \\ 3 6 / \\ 2 4 /1 Output: 3 直觉思路上，我们只要从小到大有序遍历每个节点直至第k个。因为给定的树是Binary Search Tree，有序遍历意味着以左子树、节点本身和右子树的访问顺序递归下去就行。由于ordered_iter是generator，递归调用自己的过程就是嵌套使用generator的过程。下面是yield版本。 {linenos123456789101112131415161718# AC# Runtime: 48 ms, faster than 90.31% of Python3 online submissions for Kth Smallest Element in a BST.# Memory Usage: 17.9 MB, less than 14.91% of Python3 online submissions for Kth Smallest Element in a BST.class Solution: def kthSmallest(self, root: TreeNode, k: int) -&gt; int: def ordered_iter(node): if node: for sub_node in ordered_iter(node.left): yield sub_node yield node for sub_node in ordered_iter(node.right): yield sub_node for node in ordered_iter(root): k -= 1 if k == 0: return node.val 等价于如下 yield from 版本： {linenos12345678910111213141516# AC# Runtime: 56 ms, faster than 63.74% of Python3 online submissions for Kth Smallest Element in a BST.# Memory Usage: 17.7 MB, less than 73.33% of Python3 online submissions for Kth Smallest Element in a BST.class Solution: def kthSmallest(self, root: TreeNode, k: int) -&gt; int: def ordered_iter(node): if node: yield from ordered_iter(node.left) yield node yield from ordered_iter(node.right) for node in ordered_iter(root): k -= 1 if k == 0: return node.val 24 点问题之函数式枚举解法 看明白了itertools.permuations，combinations，product，yield以及yield from，我们回到本篇最初的24点游戏问题。 24点游戏的本质是枚举出所有可能运算，如果有一种方式得到24返回True，否则返回Flase。进一步思考所有可能的运算，包括下面三个维度： 4个数字的所有排列，比如给定 [1, 2, 3, 4]，可以用permutations([1, 2, 3, 4]) 生成这个维度的所有可能 三个位置的操作符号的全部可能，可以用 product([+, -, *, /], repeat=3) 生成，具体迭代结果为：[+, +, +]，[+, +, -]，... 给定了前面两个维度后，还有一个比较不容易察觉但必要的维度：运算优先级。比如在给定数字顺序 [1, 2, 3, 4]和符号顺序 [+, *, -]之后可能的四种操作树 四种运算优先级 能否算得24点只需要枚举这三个维度笛卡尔积的运算结果 (维度1：数字组合) x (维度2：符号组合) x (维度3：优先级组合) {linenos123456789101112131415161718192021222324252627282930# AC# Runtime: 112 ms, faster than 57.59% of Python3 online submissions for 24 Game.# Memory Usage: 13.7 MB, less than 85.60% of Python3 online submissions for 24 Game.import mathfrom itertools import permutations, productfrom typing import Listclass Solution: def iter_trees(self, op1, op2, op3, a, b, c, d): yield op1(op2(a, b), op3(c, d)) yield op1(a, op2(op3(b, c), d)) yield op1(a, op2(b, op3(c, d))) yield op1(op2(a, op3(b, c)), d) def judgePoint24(self, nums: List[int]) -&gt; bool: mul = lambda x, y: x * y plus = lambda x, y: x + y div = lambda x, y: x / y if y != 0 else math.inf minus = lambda x, y: x - y op_lst = [plus, minus, mul, div] for ops in product(op_lst, repeat=3): for val in permutations(nums): for v in self.iter_trees(ops[0], ops[1], ops[2], val[0], val[1], val[2], val[3]): if abs(v - 24) &lt; 0.0001: return True return False 24 点问题之 DFS yield from 解法 一种常规的思路是，在四个数组成的集合中先选出任意两个数，枚举所有可能的计算，再将剩余的三个数组成的集合递归调用下去，直到叶子节点只剩一个数，如下图所示。 DFS 调用示例 下面的代码是这种思路的 itertools + yield from 解法，recurse方法是generator，会自我递归调用。当只剩下两个数时，用 yield 返回两个数的所有可能运算得出的值，其他非叶子情况下则自我调用使用yield from，例如4个数任选2个先计算再合成3个数的情况。这种情况下，比较麻烦的是由于4个数可能有相同值，若用 combinations(lst, 2) 先任选两个数，后续要生成剩余两个数加上第三个计算的数的集合代码会繁琐。因此，我们改成任选4个数index中的两个，剩余的indices 可以通过集合操作来完成。 {linenos12345678910111213141516171819202122232425262728293031323334353637383940# AC# Runtime: 116 ms, faster than 56.23% of Python3 online submissions for 24 Game.# Memory Usage: 13.9 MB, less than 44.89% of Python3 online submissions for 24 Game.import mathfrom itertools import combinations, product, permutationsfrom typing import Listclass Solution: def judgePoint24(self, nums: List[int]) -&gt; bool: mul = lambda x, y: x * y plus = lambda x, y: x + y div = lambda x, y: x / y if y != 0 else math.inf minus = lambda x, y: x - y op_lst = [plus, minus, mul, div] def recurse(lst: List[int]): if len(lst) == 2: for op, values in product(op_lst, permutations(lst)): yield op(values[0], values[1]) else: # choose 2 indices from lst of length n for choosen_idx_lst in combinations(list(range(len(lst))), 2): # remaining indices not choosen (of length n-2) idx_remaining_set = set(list(range(len(lst)))) - set(choosen_idx_lst) # remaining values not choosen (of length n-2) value_remaining_lst = list(map(lambda x: lst[x], idx_remaining_set)) for op, idx_lst in product(op_lst, permutations(choosen_idx_lst)): # 2 choosen values are lst[idx_lst[0]], lst[idx_lst[1] value_remaining_lst.append(op(lst[idx_lst[0]], lst[idx_lst[1]])) yield from recurse(value_remaining_lst) value_remaining_lst = value_remaining_lst[:-1] for v in recurse(nums): if abs(v - 24) &lt; 0.0001: return True","link":"/zh/2020/leetcode-679-24-game/"},{"title":"Leetcode矩阵快速幂运算解法","text":"快速幂运算是一种利用位运算和DP思想求的\\(x^n\\)的数值算法，它将时间复杂度\\(O(n)\\)降到\\(O(log(n))\\)。快速幂运算结合矩阵乘法，可以巧解不少DP问题。本篇会由浅入深，从最基本的快速幂运算算法，到应用矩阵快速幂运算解DP问题，结合三道Leetcode题目来具体讲解。 Leetcode 50. Pow(x, n) (Medium) Leetcode 50. Pow(x, n) 是实数的快速幂运算问题，题目如下。 Implement pow(x, n), which calculates x raised to the power n (i.e. \\(x^n\\)). Example 1: 12Input: x = 2.00000, n = 10Output: 1024.00000 Example 2: 12Input: x = 2.10000, n = 3Output: 9.26100 Example 3: 123Input: x = 2.00000, n = -2Output: 0.25000Explanation: 2-2 = 1/22 = 1/4 = 0.25 快速幂运算解法分析 假设n是32位的int类型，将n写成二进制形式，那么n可以写成最多32个某位为 1（第k位为1则值为\\(2^k\\)）的和。那么\\(x^n\\)最多可以由32个 \\(x^{2^k}\\)的乘积组合，例如： \\[ x^{\\text{10011101}_{2}} = x^{1} \\times x^{\\text{100}_{2}} \\times x^{\\text{1000}_{2}} \\times x^{\\text{10000}_{2}} \\times x^{\\text{10000000}_{2}} \\] 快速幂运算的特点就是通过32次循环，每次循环根据上轮\\(x^{2^k}\\)的值进行平方后得出这一轮的值：\\(x^{2^k} \\times x^{2^k} = x^{2^{k+1}}\\)，即循环计算出如下数列 \\[ x^{1}, x^2=x^{\\text{10}_{2}}, x^4=x^{\\text{100}_{2}}, x^8=x^{\\text{1000}_{2}}, x^{16}=x^{\\text{10000}_{2}}, ..., x^{128} = x^{\\text{10000000}_{2}} \\] 在循环时，如果n的二进制形式在本轮对应的位的值是1，则将这次结果累乘计入最终结果。 下面是python 3 的代码，由于循环为32次，所以容易看出算法复杂度为 \\(O(log(n))\\)。 {linenos1234567891011121314# AC# Runtime: 32 ms, faster than 54.28% of Python3 online submissions for Pow(x, n).# Memory Usage: 14.2 MB, less than 5.04% of Python3 online submissions for Pow(x, n).class Solution: def myPow(self, x: float, n: int) -&gt; float: ret = 1.0 i = abs(n) while i != 0: if i &amp; 1: ret *= x x *= x i = i &gt;&gt; 1 return 1.0 / ret if n &lt; 0 else ret 对应的 Java 的代码。 {linenos12345678910111213141516171819// AC// Runtime: 1 ms, faster than 42.98% of Java online submissions for Pow(x, n).// Memory Usage: 38.7 MB, less than 48.31% of Java online submissions for Pow(x, n).class Solution { public double myPow(double x, int n) { double ret = 1.0; long i = Math.abs((long) n); while (i != 0) { if ((i &amp; 1) &gt; 0) { ret *= x; } x *= x; i = i &gt;&gt; 1; } return n &lt; 0 ? 1.0 / ret : ret; }} 矩阵快速幂运算 快速幂运算也可以应用到计算矩阵的幂，即上面的x从实数变为方形矩阵。实现上，矩阵的幂需要矩阵乘法：$ A_{r c} B_{c p}$ ，Python中可以用numpy的 np.matmul(A, B)来完成，而Java版本中我们手动实现简单的矩阵相乘算法，从三重循环看出其算法复杂度为\\(O(r \\times c \\times p)\\)。 {linenos1234567891011121314public int[][] matrixProd(int[][] A, int[][] B) { int R = A.length; int C = B[0].length; int P = A[0].length; int[][] ret = new int[R][C]; for (int r = 0; r &lt; R; r++) { for (int c = 0; c &lt; C; c++) { for (int p = 0; p &lt; P; p++) { ret[r][c] += A[r][p] * B[p][c]; } } } return ret;} Leetcode 509. Fibonacci Number (Easy) 有了快速矩阵幂运算，我们来看看如何具体解题。Fibonacci问题作为最基本的DP问题，在上一篇Leetcode 679 24 Game 的 Python 函数式实现中我们用python独有的yield来巧解，这次再拿它来做演示。 The Fibonacci numbers, commonly denoted F(n) form a sequence, called the Fibonacci sequence, such that each number is the sum of the two preceding ones, starting from 0 and 1. That is, 12F(0) = 0, F(1) = 1F(N) = F(N - 1) + F(N - 2), for N &gt; 1. Given N, calculate F(N). Example 1: 123Input: 2Output: 1Explanation: F(2) = F(1) + F(0) = 1 + 0 = 1. Example 2: 123Input: 3Output: 2Explanation: F(3) = F(2) + F(1) = 1 + 1 = 2. Example 3: 123Input: 4Output: 3Explanation: F(4) = F(3) + F(2) = 2 + 1 = 3. 转换为矩阵幂运算 Fibonacci的二阶递推式如下： \\[ \\begin{align*} F(n) =&amp; F(n-1) + F(n-2) \\\\ F(n-1) =&amp; F(n-1) \\end{align*} \\] 等价的矩阵递推形式为： \\[ \\begin{bmatrix}F(n)\\\\F(n-1)\\end{bmatrix} = \\begin{bmatrix}1 &amp; 1\\\\1 &amp; 0\\end{bmatrix} \\begin{bmatrix}F(n-1)\\\\F(n-2)\\end{bmatrix} \\] 也就是每轮左乘一个2维矩阵。其循环形式为，即矩阵幂的形式： \\[ \\begin{bmatrix}F(n)\\\\F(n-1)\\end{bmatrix} = \\begin{bmatrix}1 &amp; 1\\\\1 &amp; 0\\end{bmatrix}^{n-1} \\begin{bmatrix}F(1)\\\\F(0)\\end{bmatrix} \\] AC代码 有了上面的矩阵幂公式，代码稍作改动即可。Java 版本代码。 {linenos123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * AC * Runtime: 0 ms, faster than 100.00% of Java online submissions for Fibonacci Number. * Memory Usage: 37.9 MB, less than 18.62% of Java online submissions for Fibonacci Number. * * Method: Matrix Fast Power Exponentiation * Time Complexity: O(log(N)) **/class Solution { public int fib(int N) { if (N &lt;= 1) { return N; } int[][] M = {{1, 1}, {1, 0}}; // powers = M^(N-1) N--; int[][] powerDouble = M; int[][] powers = {{1, 0}, {0, 1}}; while (N &gt; 0) { if (N % 2 == 1) { powers = matrixProd(powers, powerDouble); } powerDouble = matrixProd(powerDouble, powerDouble); N = N / 2; } return powers[0][0]; } public int[][] matrixProd(int[][] A, int[][] B) { int R = A.length; int C = B[0].length; int P = A[0].length; int[][] ret = new int[R][C]; for (int r = 0; r &lt; R; r++) { for (int c = 0; c &lt; C; c++) { for (int p = 0; p &lt; P; p++) { ret[r][c] += A[r][p] * B[p][c]; } } } return ret; }} Python 3的numpy.matmul() 版本代码。 {linenos1234567891011121314151617181920212223# AC# Runtime: 256 ms, faster than 26.21% of Python3 online submissions for Fibonacci Number.# Memory Usage: 29.4 MB, less than 5.25% of Python3 online submissions for Fibonacci Number.class Solution: def fib(self, N: int) -&gt; int: if N &lt;= 1: return N import numpy as np F = np.array([[1, 1], [1, 0]]) N -= 1 powerDouble = F powers = np.array([[1, 0], [0, 1]]) while N &gt; 0: if N % 2 == 1: powers = np.matmul(powers, powerDouble) powerDouble = np.matmul(powerDouble, powerDouble) N = N // 2 return powers[0][0] 或者也可以直接调用numpy.matrix_power() 代替手动的快速矩阵幂运算。 {linenos12345678910111213141516# AC# Runtime: 116 ms, faster than 26.25% of Python3 online submissions for Fibonacci Number.# Memory Usage: 29.2 MB, less than 5.25% of Python3 online submissions for Fibonacci Number.class Solution: def fib(self, N: int) -&gt; int: if N &lt;= 1: return N from numpy.linalg import matrix_power import numpy as np F = np.array([[1, 1], [1, 0]]) F = matrix_power(F, N - 1) return F[0][0] Leetcode 1411. Number of Ways to Paint N × 3 Grid (Hard) 下面来看一道稍难一点的DP问题，1411. Number of Ways to Paint N × 3 Grid。 You have a grid of size n x 3 and you want to paint each cell of the grid with exactly one of the three colours: Red, Yellow or Green while making sure that no two adjacent cells have the same colour (i.e no two cells that share vertical or horizontal sides have the same colour). You are given n the number of rows of the grid. Return the number of ways you can paint this grid. As the answer may grow large, the answer must be computed modulo 10^9 + 7. Example 1: 123Input: n = 1Output: 12Explanation: There are 12 possible way to paint the grid as shown: Example 2: 12Input: n = 2Output: 54 Example 3: 12Input: n = 3Output: 246 Example 4: 12Input: n = 7Output: 106494 Example 5: 12Input: n = 5000Output: 30228214 标准DP解法 分析题目容易发现第i行的状态只取决于第i-1行的状态，第i行会有两种不同状态：三种颜色都有或者只有两种颜色。这个问题容易识别出是经典的双状态DP问题，那么我们定义dp2[i]为第i行只有两种颜色的数量，dp3[i]为第i行有三种颜色的数量。 先考虑dp3[i]和i-1行的关系。假设第i行包含3种颜色，即dp3[i]，假设具体颜色为红，绿，黄，若i-1行包含两种颜色（即dp2[i-1]），此时dp2[i-1]只有以下2种可能： dp2[i-1] -&gt; dp3[i] 还是dp3[i] 红，绿，黄情况，若i-1行包含三种颜色（从dp3[i-1]转移过来），此时dp3[i-1]也只有以下2种可能： dp3[i-1] -&gt; dp3[i] 因此，dp3[i]= dp2[i-1] * 2 + dp3[i-1] * 2。 同理，若第i行包含两种颜色，即dp2[i]，假设具体颜色为绿，黄，绿，若i-1行是两种颜色（dp2[i-1]），此时dp2[i-1]有如下3种可能： dp2[i-1] -&gt; dp2[i] dp2[i]的另一种情况是由dp3[i-1]转移过来，则dp3[i-1]有2种可能，枚举如下： dp3[i-1] -&gt; dp2[i] 因此，dp2[i] = dp2[i-1] * 3 + dp3[i-1] * 2。 初始值dp2[1] = 6，dp3[1] = 6，最终答案为dp2[i] + dp3[i]。 很容易写出普通DP版本的Python 3代码，时间复杂度为\\(O(n)\\)。 {linenos12345678910111213# AC# Runtime: 36 ms, faster than 98.88% of Python3 online submissions for Number of Ways to Paint N × 3 Grid.# Memory Usage: 13.9 MB, less than 58.66% of Python3 online submissions for Number of Ways to Paint N × 3 Grid.class Solution: def numOfWays(self, n: int) -&gt; int: MOD = 10 ** 9 + 7 dp2, dp3 = 6, 6 n -= 1 while n &gt; 0: dp2, dp3 = (dp2 * 3 + dp3 * 2) % MOD, (dp2 * 2 + dp3 * 2) % MOD n -= 1 return (dp2 + dp3) % MOD 快速矩阵幂运算解法 和Fibonacci一样，我们将DP状态转移方程转换成矩阵乘法： \\[ \\begin{bmatrix}dp2(n)\\\\dp3(n)\\end{bmatrix} = \\begin{bmatrix}3 &amp; 2\\\\2 &amp; 2\\end{bmatrix} \\begin{bmatrix}dp2(n-1)\\\\dp3(n-1)\\end{bmatrix} \\] 代入初始值，转换成矩阵幂形式 \\[ \\begin{bmatrix}dp2(n)\\\\dp3(n)\\end{bmatrix} = \\begin{bmatrix}3 &amp; 2\\\\2 &amp; 2\\end{bmatrix}^{n-1}\\begin{bmatrix}6\\\\6\\end{bmatrix} \\] 代码几乎和Fibonacci一模一样，仅仅多了mod 计算。下面是Java版本。 {linenos1234567891011121314151617181920212223242526272829303132333435363738394041/**ACRuntime: 0 ms, faster than 100.00% of Java online submissions for Number of Ways to Paint N × 3 Grid.Memory Usage: 35.7 MB, less than 97.21% of Java online submissions for Number of Ways to Paint N × 3 Grid.**/class Solution { public int numOfWays(int n) { long MOD = (long) (1e9 + 7); long[][] ret = {{6, 6}}; long[][] m = {{3, 2}, {2, 2}}; n -= 1; while(n &gt; 0) { if ((n &amp; 1) &gt; 0) { ret = matrixProd(ret, m, MOD); } m = matrixProd(m, m, MOD); n &gt;&gt;= 1; } return (int) ((ret[0][0] + ret[0][1]) % MOD); } public long[][] matrixProd(long[][] A, long[][] B, long MOD) { int R = A.length; int C = B[0].length; int P = A[0].length; long[][] ret = new long[R][C]; for (int r = 0; r &lt; R; r++) { for (int c = 0; c &lt; C; c++) { for (int p = 0; p &lt; P; p++) { ret[r][c] += A[r][p] * B[p][c]; ret[r][c] = ret[r][c] % MOD; } } } return ret; }} Python 3实现为 {linenos12345678910111213141516171819# AC# Runtime: 88 ms, faster than 39.07% of Python3 online submissions for Number of Ways to Paint N × 3 Grid.# Memory Usage: 30.2 MB, less than 11.59% of Python3 online submissions for Number of Ways to Paint N × 3 Grid.class Solution: def numOfWays(self, n: int) -&gt; int: import numpy as np MOD = int(1e9 + 7) ret = np.array([[6, 6]]) m = np.array([[3, 2], [2, 2]]) n -= 1 while n &gt; 0: if n % 2 == 1: ret = np.matmul(ret, m) % MOD m = np.matmul(m, m) % MOD n = n // 2 return int((ret[0][0] + ret[0][1]) % MOD)","link":"/zh/2020/leetcode-matrix-power/"},{"title":"解读深度强化学习基石论文：函数近似的策略梯度方法","text":"导读：这篇式1999 年Richard Sutton 在强化学习领域中的经典论文，论文证明了策略梯度定理和在用函数近似 Q 值时策略梯度定理依然成立，本文奠定了后续以深度强化学习策略梯度方法的基石。理解熟悉本论文对 Policy Gradient，Actor Critic 方法有很好的指导意义。 论文分成四部分。第一部分指出策略梯度在两种期望回报定义下都成立（定理一）。第二部分提出，如果 \\(Q^{\\pi}\\) 被函数 \\(f_w\\) 近似时且满足兼容（compatible）条件，以 \\(f_w\\) 替换策略梯度中的 \\(Q^{\\pi}\\)公式也成立（定理二）。第三部分举Gibbs分布的策略为例，如何应用 \\(Q^{\\pi}\\)近似函数来实现策略梯度算法。第四部分证明了近似函数的策略梯度迭代法一定能收敛到局部最优解。附录部分证明了两种定义下的策略梯度定理。 1. 策略梯度定理 对于Agent和环境而言，可以分成episode和non-episode，后者的时间步骤可以趋近于无穷大，但一般都可以适用两种期望回报定义。一种是单步平均reward ，另一种是指定唯一开始状态并对trajectory求 \\(\\gamma\\)-discounted 之和，称为开始状态定义。两种定义都考虑到了reward的sum会趋近于无穷大，通过不同的方式降低了此问题的概率。 A. 平均reward定义 目标函数 \\(\\rho(\\pi)\\) 定义成单步的平均reward，这种情况下等价于稳定状态分布下期望值。 稳定状态分布定义成无限次数后状态的分布。 此时，\\(Q^{\\pi}\\) 定义为无限步的reward sum 减去累积的单步平均 reward \\(\\rho(\\pi)\\)，这里减去\\(\\rho(\\pi)\\)是为了一定程度防止 \\(Q^{\\pi}\\)没有上界。 B. 开始状态定义 在开始状态定义方式中，某指定状态\\(s_0\\)作为起始状态，\\(\\rho(\\pi)\\) 的定义为 trajectory 的期望回报，注意由于时间步骤 t 趋近于无穷大，必须要乘以discount 系数 \\(\\gamma &lt; 1\\) 保证期望不会趋近无穷大。 \\(Q^{\\pi}\\) 也直接定义成 trajectory 的期望回报。 \\(d^{\\pi}\\) 依然为无限次数后状态的稳定分布。 策略梯度定理 论文指出上述两种定义都满足策略梯度定理，即目标 \\(\\rho\\) 对于参数 \\(\\theta\\) 的偏导不依赖于 \\(d^{\\pi}\\) 对于 \\(\\theta\\) 偏导，仅取决 关于策略梯度定理的一些综述，可以参考。 论文中还提到策略梯度定理公式和经典的William REINFORCE算法之间的联系。REINFORCE算法即策略梯度的蒙特卡洛实现。 联系如下： 首先，根据策略梯度定理，如果状态 s 是通过 \\(\\pi\\) 采样得到，则下式是$$ 的无偏估计。注意，这里action的summation和 \\(\\pi\\) 是无关的。 在William REINFORCE算法中，采用\\(R_t\\) 作为 \\(Q^{\\pi}(s_t, a_t)\\)的近似，但是 \\(R_t\\) 取决于 on-policy \\(\\pi\\) 的动作分布，因此必须除掉 \\(\\pi(s_t, a_t)\\)项，去除引入\\(R_t\\) 后导致oversample动作空间。 2. 函数近似的策略梯度 论文第二部分，进一步引入 \\(Q_{\\pi}\\) 的近似函数 \\(f_w\\): $ $。 如果我们有\\(Q_{\\pi}(s_t, a_t)\\)的无偏估计，例如 \\(R_t\\)，很自然，可以让 \\(\\partial f_w \\over \\partial w\\) 通过最小化 \\(R_t\\) 和 \\(f_w\\)之间的差距来计算。 当拟合过程收敛到局部最优时，策略梯度定理中右边项对于 \\(w\\) 求导为0，可得(3)式。 至此，引出策略梯度定理的延续，即定理2：当 \\(f_w\\) 满足(3)式同时满足(4)式（称为compatible条件时），可以用 \\(f_w(s, a)\\)替换原策略梯度中的 \\(Q_{\\pi}(s,a)\\) 3. 一个应用示例 假设一个策略用features的线性组合后的 Gibbs分布来生成，即： 注意，\\(\\phi_{sa}\\) 和 \\(\\theta\\) 都是 \\(l\\) 维的。 当 \\(f_w\\) 满足compatible 条件，由公式（4）可得\\(\\partial f_w \\over \\partial w\\) 注意，\\(\\partial f_w \\over \\partial w\\) 也是 \\(l\\)维。\\(f_w\\) 可以很自然的参数化为 即 \\(f_w\\) 和 策略 \\(\\pi\\) 一样是features的线性关系。当然 \\(f_w\\) 还满足对于所有状态，在 \\(\\pi\\) 动作分布下均值为0。 上式和advantage 函数 \\(A^{\\pi}(s, a)\\) 定义一致，因此可以认为 \\(f_w\\) 的意义是 \\(A^{\\pi}\\) 的近似。 \\(A^{\\pi}\\)具体定义如下 4. 函数近似的策略梯度收敛性证明 这一部分证明了在满足一定条件后，\\(\\theta\\) 可以收敛到局部最优点。 条件为 Compatible 条件，公式（4） 任意两个 \\(\\partial \\pi \\over \\partial \\theta\\) 偏导是有限的，即 步长数列满足如下条件 环境的 reward 是有限的 此时，当 \\(w_k\\) 和 \\(\\theta_k\\) 按如下方式迭代一定能收敛到局部最优。 收敛到局部最优，即 5. 策略梯度定理的两种情况下的证明 下面简单分解策略梯度的证明步骤。 A. 平均reward 定义下的证明 根据定义，将 \\(\\theta\\) 导数放入求和号中，并分别对乘积中的每项求导。 将\\(Q_{\\pi}\\)的定义代入第二项 \\(Q^{\\pi}\\) 对 \\(\\theta\\) 求偏导中，引入环境reward 随机变量 \\(R^a_s\\)，环境dynamics \\(P\\) 和 \\(\\rho(\\pi)\\) \\(\\theta\\) 偏导进一步移入，\\(R^a_s\\)， \\(P\\) 不依赖于\\(\\theta\\)。 \\(\\rho(\\pi)\\) 对于 \\(\\theta\\) 偏导整理到等式左边 两边同时乘以 \\(\\sum d^{\\pi}\\) 由于 \\(d^{\\pi}\\) 是状态在 \\(\\pi\\) 下的平稳分布，\\(\\sum \\pi \\sum P\\) 项表示 agent 主观 \\(\\pi\\) 和环境客观 \\(P\\) 对于状态分布的影响，因此可以直接去除。 整理证得。 B. Start-state 定义下的证明 根据定义，将 \\(\\theta\\) 导数放入求和号中，并分别对乘积中的每项求导。 将\\(Q_{\\pi}\\)的定义代入第二项 \\(Q^{\\pi}\\) 对 \\(\\theta\\) 求偏导中，引入环境reward 随机变量 \\(R^a_s\\)，环境dynamics \\(P\\) \\(\\theta\\) 偏导进一步移入，\\(R^a_s\\)， \\(P\\) 不依赖于\\(\\theta\\)。注意，此式表示从状态 \\(s\\) 出发一步之后的能到达的所有 \\(s^{\\prime}\\) ，将次式反复unroll \\(V^{\\pi}\\) 成 \\(Q^{\\pi}\\) 之后得到 \\(\\operatorname{Pr}(s \\rightarrow x, k, \\pi)\\) 表示 k 步后 状态 s 能到达的所有状态 x 根据定义，\\(\\rho = V^{\\pi}(s_0)\\) 将 \\(V^{\\pi}(s_0)\\) 替换成unroll 成 \\(Q^{\\pi}\\) 的表达式 \\(\\operatorname{Pr}(s \\rightarrow x, k, \\pi)\\) 即 \\(d^{\\pi}\\)","link":"/zh/2020/paper-rl-pg-sutton-1999/"},{"title":"解读TRPO论文，一种深度强化学习和传统优化方法结合的方法","text":"导读：本论文由Berkeley 的几位大神于2015年发表于 JMLR（Journal of Machine Learning Research）。深度强化学习算法例如DQN或者PG（Policy Gradient）都无法避免训练不稳定的问题：在训练过程中效果容易退化并且很难恢复。针对这个通病，TRPO采用了传统优化算法中的trust region方法，以保证每一步迭代能够获得效果提升，直至收敛到局部最优点。 本篇论文涉及到的知识点比较多，不仅建立在强化学习领域经典论文的结论：Kakade &amp; Langford 于2002 年发表的 Approximately Optimal Approximate Reinforcement Learning 关于优化目标的近似目标和重要性采样，也涉及到传统优化方法 trust region 的建模和其具体的矩阵近似数值算法。读懂本论文，对于深度强化学习及其优化方法可以有比较深入的理解。本论文附录的证明部分由于更为深奥和冗长，在本文中不做具体讲解，但是也建议大家能够仔细研读。 阅读本论文需要注意的是，这里解读的版本是arxiv的版本，这个版本带有附录，不同于 JMLR的版本的是，arxiv版本中用reward函数而后者用cost函数，优化方向相反。 arxiv 下载链接为 https://arxiv.org/pdf/1502.05477.pdf 0. 论文框架 本论文解决的目标是希望每次迭代参数能保证提升效果，具体想法是利用优化领域的 trust region方法（中文可以翻译成置信域方法或信赖域方法），通过参数在trust region范围中去找到一定能提升的下一次迭代。 本论文框架如下 首先，引入Kakade &amp; Langford 论文 Approximately Optimal Approximate Reinforcement Learning 中关于近似优化目标的结论。（论文第二部分） 基于 Kakade 论文中使用mixture policy保证每一步效果提升的方法，扩展到一般随机策略，引入策略分布的total variation divergence作为约束。（论文第三部分） 将total variation divergence约束替换成平均 KL divergence 约束，便于使用蒙特卡洛方法通过采样来生成每一步的具体优化问题。（论文第四，五部分） 给出解决优化问题的具体算法，将优化目标用first order来近似，约束项用second order 来近似，由于second order涉及到构造Hessian matrix，计算量巨大，论文给出了 conjugate gradient + Fisher information matrix的近似快速实现方案。（论文第六部分） 从理论角度指出，Kakade 在2002年提出的方法natrual policy gradient 和经典的policy gradient 都是TRPO的特别形式。（论文第七部分） 评价TRPO在两种强化学习模式下的最终效果，一种是MuJoCo模拟器中能得到真实状态的模式，一种是Atari游戏环境，即观察到的屏幕像素可以信息完全地表达潜在真实状态的模式。（论文第八部分） 本文下面的小结序号和论文小结序号相同，便于对照查阅。 1. 介绍 TRPO 第一次证明了最小化某种 surrogate 目标函数且采用non-trivial的步长，一定可以保证策略提升。进一步将此 surrogate 目标函数转换成trust region约束下的优化问题。TRPO是一种on-policy 的算法，因为每一步迭代，需要在新的策略下通过采样数据来构建具体优化问题。 2. 已有理论基础 第二部分主要回顾了 Kakade &amp; Langford 于2002 年的论文 Approximately Optimal Approximate Reinforcement Learning 中的一系列结论。 先来定义几个重要概念的数学定义 \\(\\eta(\\pi)\\) 是策略 \\(\\pi\\) 的目标，即discounted reward 和的期望。 然后是策略的Q值和V值 最后是策略的advantage函数 接着，开始引入 Kakade &amp; Langford 论文结论，即下式（公式1）。 公式1表明，下一次迭代策略的目标可以分解成现有策略的目标 \\(\\eta(\\pi)\\) 和现有advantage 函数在新策略trajectory分布下的期望。 公式1可以很容易从trajectory分布转换成新策略在状态的访问频率，即公式2 状态的访问频率或稳定状态分布定义成 注意到公式2中状态的期望依然依赖于新策略 \\(\\rho_{\\widetilde\\pi}\\) 的稳定状态分布，不方便实现。原因如下，期望形式有利于采样来解决问题，但是由于采样数据源于 on-policy \\(\\pi\\) 而非 \\({\\widetilde\\pi}\\) ，因此无法直接采样未知的策略 \\({\\widetilde\\pi}\\)。 幸好，Kakade 论文中证明了，可以用 \\(\\rho_{\\pi}\\) 的代替 \\(\\rho_{\\widetilde\\pi}\\) 并且证明了这种代替下的近似目标函数 \\(L_{\\pi}\\) 是原来函数的一阶近似 \\[ L_{\\pi}(\\widetilde\\pi) \\approx \\eta(\\widetilde\\pi) \\] 即满足 \\(L_{\\pi}\\) 具体定义表达式为 \\(L_{\\pi}(\\widetilde\\pi)\\) 是一阶近似意味着在小范围区域中一定是可以得到提升的，但是范围是多大，是否能保证 \\(\\eta\\) 的提升？Kakade的论文中不仅给出了通过mix新老策略的提升方式，还给出了这个方式对原目标 \\(\\eta\\) 较 \\(L_{\\pi}(\\widetilde\\pi)\\) 的提升下届。 策略更新规则如下 公式6为具体提升下届为 3. 扩展到随机策略 论文的这一部分将Kakade的mix policy update 扩展到一般的随机策略，同时依然保证每次迭代能得到目标提升。 首先，每次策略迭代必须不能和现有策略变化太大，因此，引入分布间常见的TV divergence，即 total variation divergence。 有了两个分布距离的定义，就可以定义两个策略的距离。离散状态下，一个策略是状态到动作分布的 map 或者 dict，因此，可以定义两个策略的距离为所有状态中最大的动作分布的 \\(D_{TV}\\)，即 至此，可以引出定理一：在一般随机策略下，Kakade 的surrogate函数较原目标的提升下届依然成立，即公式8在新的\\(\\alpha\\)定义下可以从公示6推导而来。 进一步将 TV divergence 转换成 KL divergence，转换成KL divergence 的目的是为了后续使用传统且成熟的 trust region 蒙特卡洛方法和 conjugate gradient 的优化近似解法。 由于上面两种距离的大小关系，可以推导出用KL divergence表示的 \\(\\eta\\) 较 \\(L_{\\pi}(\\widetilde\\pi)\\) 的提升下届 根据公式9，就可以形成初步的概念上的算法一，通过每一步形成无约束优化问题，同时保证每次迭代的 \\(\\pi_i\\) 对应的 \\(\\eta\\) 是递增的。 4. Trust Region Policy Optimization 看到这里已经不容易了，尽管算法一给出了一个解决方案，但是本论文的主角TRPO 还未登场。TRPO算法的作用依然是近似！ 算法一对于下面的目标函数做优化，即每次找到下一个 \\(\\theta_i\\) 最大化下式，\\(\\eta\\) 每一步一定能得到提升。 问题是在实践中，惩罚系数 \\(C\\) 会导致步长非常小，一种稳定的使用较大步长的方法是将惩罚项变成约束项，即： 将 \\(D^{max}_{KL}\\) 放入约束项中符合trust region 这种传统优化解法。 关于 \\(D^{max}_{KL}\\) 约束，再补充两点 其定义是两个策略中所有状态中最大的动作分布的 \\(D_{TV}\\) ，因此它约束了所有状态下新老策略动作分布的KL散度，也就意味着有和状态数目相同数量的约束项，海量的约束项导致算法很难应用到实际中。 约束项的 trust region 不是参数 \\(\\theta\\) 的空间，而是其KL散度的空间。 基于第一点，再次使用近似法，在约束项中用KL期望来代替各个状态下的KL散度，权重为on-policy 策略的分布 \\(\\rho(\\theta_{old})\\) 最终，得到TRPO在实际中的优化目标（12式）： 5. 用采样方法来Trust Region约束优化 论文第五部分，将TRPO优化目标12式改写成期望形式，引入两种蒙特卡洛方法 single path 和 vine 来采样。 具体来说，\\(L_{\\theta_{old}}\\) 由两项组成 \\[ L_{\\theta_{old}} = \\eta(\\theta_{old}) + \\sum_s \\rho_{\\theta_{old}}(s)\\sum_a {\\pi_{\\theta}}(a |s) A_{\\theta_{old}}(s,a) \\] 第一项是常量，只需优化第二项，即优化问题等价为13式 随后，为了可以适用非 on-policy \\(\\pi_{\\theta_{old}}\\) 的动作分布来任意采样，引入采样的动作分布 \\(q(a|s)\\)，将13式中的 \\(\\sum_a\\) 部分通过重要性采样改成以下形式： 再将13式中的 \\(\\sum_s \\rho(s)\\) 改成期望形式 \\(\\mathbb{E}_{s \\sim \\rho}\\) ，并将 \\(A\\) 改成 \\(Q\\) 值，得14式。 至此，我们得到trust region优化的期望形式：优化目标中期望的状态空间是基于 on-policy \\(\\pi_{\\theta_{old}}\\)，动作空间是基于任意采样分布 \\(q(a|s)\\)，优化约束中的期望是基于 on-policy \\(\\pi_{\\theta_{old}}\\)。 5.1 Single path采样 根据14式，single path 是最基本的的蒙特卡洛采样方法，和REINFORCE算法一样， 通过on-policy \\(\\pi_{\\theta_{old}}\\)生成采样的 trajectory数据： \\(s_0, a_0, s_1, a_1, ..., a_{T-1}, s_{T}\\)，然后代入14式。注意，此时 \\(q(a|s) = \\pi_{\\theta_{old}}(a|s)\\)，即用现有策略的动作分布直接代替采样分布。 5.2 Vine 采样 虽然single path方法简单明了，但是有着online monte carlo方法固有的缺陷，即variance较大。Vine方法通过在一个状态多次采样来改善此缺陷。Vine的翻译是藤，寓意从一个状态多次出发来采样，如下图，\\(s_n\\) 状态下采样多个rollouts，很像植物的藤长出多分叉。当然，vine方法要求环境能restart 到某一状态，比如游戏环境通过save load返回先前的状态。 具体来说，vine 方法首先通过生成多个on-policy 的trajectories来确定一个状态集合 \\(s_1, s_2, ..., s_N\\)。对于状态集合的每一个状态 \\(s_n\\) 采样K个动作，服从 $ a_{n, k} q(s_{n}) $ 。接着，对于每一个 \\((s_n, a_{n,k})\\) 再去生成一次 rollout 来估计 \\(\\hat{Q}_{\\theta_{i}}\\left(s_{n}, a_{n, k}\\right)\\) 。试验证明，在连续动作空间问题中，\\(q\\left(\\cdot \\mid s_{n}\\right)\\) 直接使用 on-policy 可以取得不错效果，在离散空间问题中，使用uniform分布效果更好。 6. 转换成具体优化问题 再回顾一下现在的进度，12式定义了优化目标，约束项是KL divergence空间的trust region 形式。14式改写成了等价的期望形式，通过两种蒙特卡洛方法生成 state-action 数据集，可以代入14式得到每一步的具体数值的优化问题。论文这一部分简单叙述了如何高效但近似的解此类问题，详细的一些步骤在附录中阐述。我们把相关解读都放在下一节。 7. 和已有理论的联系 7.1 简化成 Natural Policy Gradient 再回到12式，即约束项是KL divergence空间的trust region 形式 对于这种形式的优化问题，一般的做法是通过对优化目标做一阶函数近似，即 \\[ L_{\\theta_{old}}(\\theta) \\approx L_{\\theta_{old}}\\left(\\theta_{old}\\right)+g^{T}\\left(\\theta-\\theta_{old}\\right) \\] \\[ \\left.g \\doteq \\nabla_{\\theta} L_{\\theta_{old}}(\\theta)\\right|_{\\theta_{old}} \\] 并对约束函数做二阶函数近似，因为约束函数在 \\(\\theta_{old}\\) 点取到极值，因此一阶导为0。 \\[ \\bar{D}_{K L}\\left(\\theta \\| \\theta_{old}\\right) \\approx \\frac{1}{2}\\left(\\theta-\\theta_{old}\\right)^{T} H\\left(\\theta-\\theta_{old}\\right) \\] \\[ \\left.H \\doteq \\nabla_{\\theta}^{2} \\bar{D}_{K L}\\left(\\theta \\| \\theta_{old}\\right)\\right|_{\\theta_{old}} \\] 12式的优化目标可以转换成17式 对应参数迭代更新公式如下 这个方法便是Kakade在2002年发表的 natrual policy gradient 论文。 7.2 简化成 Policy Gradient 注意，\\(L_{\\theta_{old}}\\)的一阶近似的梯度 \\[ \\left.\\nabla_{\\theta} L_{\\theta_{\\text {old }}}(\\theta)\\right|_{\\theta=\\theta_{\\text {old }}} \\cdot\\left(\\theta-\\theta_{\\text {old }}\\right) \\] 即PG定理 \\[ \\frac{\\partial \\rho}{\\partial \\theta}=\\sum_{s} d^{\\pi}(s) \\sum_{a} \\frac{\\partial \\pi(s, a)}{\\partial \\theta} Q^{\\pi}(s, a) \\] 因此，PG定理等价于\\(L_{\\theta_{old}}\\)的一阶近似的梯度在\\(\\theta\\) 空间 \\(l_2\\) 约束下的优化问题，即18式 7.3 近似数值解法 这里简单描述关于17式及其参数更新规则中的大矩阵数值计算近似方式。 $ {D}_{}^{} $ 二阶近似中的 \\(A\\) 是 Hessian 方形矩阵，维度为 \\(\\theta\\) 个数的平方。 直接构建 \\(A\\) 矩阵或者其逆矩阵 \\(A^{-1}\\)都是计算量巨大的， 注\\(A^{-1}\\)出现在natural policy update \\(\\theta\\) 更新公式中，\\(A^{-1} \\nabla_{\\theta} L(\\theta)\\) 。 一种方法是通过构建Fisher Information Matrix，引入期望形式便于采样 \\[ \\mathbf{A}=E_{\\pi_{\\theta}}\\left[\\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a} \\mid \\mathbf{s}) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\mathbf{a} \\mid \\mathbf{s})^{T}\\right] \\] 另一种方式是使用conjugate gradient 方法，通过矩阵乘以向量快速计算法迭代逼近 \\(A^{-1} \\nabla_{\\theta} L(\\theta)\\)。 8. 试验结果 在两种强化学习模式下，比较TRPO和其他模型的效果。模式一是在MuJoCo模拟器中，这种环境下能得到真实状态的情况。 另一种模式是完全信息下的Atari游戏环境，这种环境下观察到的屏幕像素可以信息完全地表达潜在真实状态。","link":"/zh/2020/paper-rl-trpo-2017/"},{"title":"深度强化学习之：DQN训练超级玛丽闯关","text":"上一期 MyEncyclopedia公众号文章 从Q-Learning 演化到 DQN，我们从原理上讲解了DQN算法，这一期，让我们通过代码来实现任天堂游戏机中经典的超级玛丽的自动通关吧。本文所有代码在 https://github.com/MyEncyclopedia/reinforcement-learning-2nd/tree/master/super_mario。 DQN 算法回顾 上期详细讲解了DQN中的两个重要的技术：Target Network 和 Experience Replay，正是有了它们才使得 Deep Q Network在实战中容易收敛，以下是Deepmind 发表在Nature 的 Human-level control through deep reinforcement learning 的完整算法流程。 超级玛丽 NES OpenAI 环境 安装基于OpenAI gym的超级玛丽环境执行下面的 pip 命令即可。 1pip install gym-super-mario-bros 我们先来看一下游戏环境的输入和输出。下面代码采用随机的action来和游戏交互。有了 组合游戏系列3: 井字棋、五子棋的OpenAI Gym GUI环境 对于OpenAI Gym 接口的介绍，现在对于其基本的交互步骤已经不陌生了。 12345678910111213141516171819202122import gym_super_mario_brosfrom random import random, randrangefrom gym_super_mario_bros.actions import RIGHT_ONLYfrom nes_py.wrappers import JoypadSpacefrom gym import wrappersenv = gym_super_mario_bros.make('SuperMarioBros-v0')env = JoypadSpace(env, RIGHT_ONLY)# Play randomlydone = Falseenv.reset()step = 0while not done: action = randrange(len(RIGHT_ONLY)) state, reward, done, info = env.step(action) print(done, step, info) env.render() step += 1env.close() 游戏render效果如下 。。。 注意我们在游戏环境初始化的时候用了参数 RIGHT_ONLY，它定义成五种动作的list，表示仅使用右键的一些组合，适用于快速训练来完成Mario第一关。 1234567RIGHT_ONLY = [ ['NOOP'], ['right'], ['right', 'A'], ['right', 'B'], ['right', 'A', 'B'],] 观察一些 info 输出内容，coins表示金币获得数量，flag_get 表示是否取得最后的旗子，time 剩余时间，以及 Mario 大小状态和所在的 x，y位置。 123456789101112{ \"coins\":0, \"flag_get\":False, \"life\":2, \"score\":0, \"stage\":1, \"status\":\"small\", \"time\":381, \"world\":1, \"x_pos\":594, \"y_pos\":89} 游戏图像处理 Deep Reinforcement Learning 一般是 end-to-end learning，意味着游戏的 screen image 作为observation直接视为真实状态，喂给神经网络训练。于此相反的另一种做法是，通过游戏环境拿到内部状态，例如所有相关物品的位置和属性作为模型输入。这两种方式的区别有两点。第一点，用观察到的屏幕像素代替真正的状态 s，在partially observable 的环境时可能因为 non-stationarity 导致无法很好的工作，而拿内部状态利用了额外的作弊信息，在partially observable环境中也可以工作。第二点，第一种方式屏幕像素维度比较高，输入数据量大，需要神经网络的大量训练拟合，第二种方式，内部真实状态往往维度低得多，训练起来很快，但缺点是因为除了内部状态往往还需要游戏相关规则作为输入，因此generalization能力不如前者强。 这里，我们当然采样屏幕像素的 end-to-end 方式了，自然首要任务是将游戏帧图像有效处理。超级玛丽游戏环境的屏幕输出是 (240, 256, 3) shape的 numpy array，通过下面一系列的转换，尽可能的在不影响训练效果的情况下减小采样到的数据量。 MaxAndSkipFrameWrapper：每4个frame连在一起，采取同样的动作，降低frame数量。 FrameDownsampleWrapper：将原始的 (240, 256, 3) down sample 到 (84, 84, 1) ImageToPyTorchWrapper：转换成适合 pytorch 的 (1, 84, 84) shape FrameBufferWrapper：保存最后4次屏幕采样 NormalizeFloats：Normalize 成 [0., 1.0] 的浮点值 123456789def wrap_environment(env_name: str, action_space: list) -&gt; Wrapper: env = make(env_name) env = JoypadSpace(env, action_space) env = MaxAndSkipFrameWrapper(env) env = FrameDownsampleWrapper(env) env = ImageToPyTorchWrapper(env) env = FrameBufferWrapper(env, 4) env = NormalizeFloats(env) return env CNN 模型 模型比较简单，三个卷积层后做 softmax输出，输出维度数为离散动作数。act() 采用了epsilon-greedy 模式，即在epsilon小概率时采取随机动作来 explore，大于epsilon时采取估计的最可能动作来 exploit。 123456789101112131415161718192021222324252627282930313233class DQNModel(nn.Module): def __init__(self, input_shape, num_actions): super(DQNModel, self).__init__() self._input_shape = input_shape self._num_actions = num_actions self.features = nn.Sequential( nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU() ) self.fc = nn.Sequential( nn.Linear(self.feature_size, 512), nn.ReLU(), nn.Linear(512, num_actions) ) def forward(self, x): x = self.features(x).view(x.size()[0], -1) return self.fc(x) def act(self, state, epsilon, device): if random() &gt; epsilon: state = torch.FloatTensor(np.float32(state)).unsqueeze(0).to(device) q_value = self.forward(state) action = q_value.max(1)[1].item() else: action = randrange(self._num_actions) return action Experience Replay 缓存 实现采用了 Pytorch CartPole DQN 的官方代码，本质是一个最大为 capacity 的 list 保存采样的 (s, a, r, s', is_done) 五元组。 1234567891011121314151617181920Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))class ReplayMemory: def __init__(self, capacity): self.capacity = capacity self.memory = [] self.position = 0 def push(self, *args): if len(self.memory) &lt; self.capacity: self.memory.append(None) self.memory[self.position] = Transition(*args) self.position = (self.position + 1) % self.capacity def sample(self, batch_size): return random.sample(self.memory, batch_size) def __len__(self): return len(self.memory) DQNAgent 我们将 DQN 的逻辑封装在 DQNAgent 类中。DQNAgent 成员变量包括两个 DQNModel，一个ReplayMemory。 train() 方法中会每隔一定时间将 Target Network 的参数同步成现行Network的参数。在td_loss_backprop()方法中采样 ReplayMemory 中的五元组，通过minimize TD error方式来改进现行 Network 参数 \\(\\theta\\)。Loss函数为： \\[ L\\left(\\theta_{i}\\right)=\\mathbb{E}_{\\left(s, a, r, s^{\\prime}\\right) \\sim \\mathrm{U}(D)}\\left[\\left(r+\\gamma \\max _{a^{\\prime}} Q_{target}\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right)^{2}\\right] \\] 123456789101112131415161718192021222324252627282930313233343536373839class DQNAgent(): def act(self, state, episode_idx): self.update_epsilon(episode_idx) action = self.model.act(state, self.epsilon, self.device) return action def process(self, episode_idx, state, action, reward, next_state, done): self.replay_mem.push(state, action, reward, next_state, done) self.train(episode_idx) def train(self, episode_idx): if len(self.replay_mem) &gt; self.initial_learning: if episode_idx % self.target_update_frequency == 0: self.target_model.load_state_dict(self.model.state_dict()) self.optimizer.zero_grad() self.td_loss_backprop() self.optimizer.step() def td_loss_backprop(self): transitions = self.replay_mem.sample(self.batch_size) batch = Transition(*zip(*transitions)) state = Variable(FloatTensor(np.float32(batch.state))).to(self.device) action = Variable(LongTensor(batch.action)).to(self.device) reward = Variable(FloatTensor(batch.reward)).to(self.device) next_state = Variable(FloatTensor(np.float32(batch.next_state))).to(self.device) done = Variable(FloatTensor(batch.done)).to(self.device) q_values = self.model(state) next_q_values = self.target_net(next_state) q_value = q_values.gather(1, action.unsqueeze(-1)).squeeze(-1) next_q_value = next_q_values.max(1)[0] expected_q_value = reward + self.gamma * next_q_value * (1 - done) loss = (q_value - expected_q_value.detach()).pow(2) loss = loss.mean() loss.backward() 外层 Training 代码 最后是外层调用代码，基本和以前文章一样。 12345678910111213141516def train(env, args, agent): for episode_idx in range(args.num_episodes): episode_reward = 0.0 state = env.reset() while True: action = agent.act(state, episode_idx) if args.render: env.render() next_state, reward, done, stats = env.step(action) agent.process(episode_idx, state, action, reward, next_state, done) state = next_state episode_reward += reward if done: print(f'{episode_idx}: {episode_reward}') break","link":"/zh/2020/rl-dqn-mario/"},{"title":"深度强化学习之：Policy Gradient Theorem 一些理解","text":"Policy gradient 定理作为现代深度强化学习的基石，同时也是actor-critic的基础，重要性不言而喻。但是它的推导和理解不是那么浅显，不同的资料中又有着众多形式，不禁令人困惑。本篇文章MyEncyclopedia试图总结众多资料背后的一些相通的地方，并写下自己的一些学习理解心得。 引入 Policy Gradient Policy gradient 引入的目的是若我们将策略 \\(\\pi_{\\theta}\\) 的参数 \\(\\theta\\) 直接和一个标量 \\(J\\) 直接联系在一起的话，就能够利用目前最流行的深度学习自动求导的方法，迭代地去找到 \\(\\theta^*\\) 来最大化 \\(J\\)： \\[ \\theta^{\\star}=\\arg \\max _{\\theta} J(\\theta) \\] \\[ {\\theta}_{t+1} \\doteq {\\theta}_{t}+\\alpha \\nabla J(\\theta) \\] 此时，训练神经网络成功地收敛到 \\(\\theta^{*}\\) 时可以直接给出任意一个状态 s 的动作分布。 那么问题来了，首先一个如何定义 \\(J(\\theta)\\)，其次，如何求出或者估计 $ J()$。 第一个问题比较直白，用value function或者广义的expected return都可以。 这里列举一些常见的定义。对于episodic 并且初始都是 \\(s_0\\)状态的情况，直接定义成v值，即Sutton教程中的episodic情况下的定义 \\[ J(\\boldsymbol{\\theta}) \\doteq v_{\\pi_{\\boldsymbol{\\theta}}}\\left(s_{0}\\right) \\quad \\quad \\text{(1.1)} \\] 进一步，上式等价于 \\(V(s)\\) 在状态平稳分布下的均值。 \\[ \\begin{aligned} J(\\theta) &amp;= \\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) V^{\\pi}(s) \\\\ &amp;=\\sum_{s \\in \\mathcal{S}} d^{\\pi}(s) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta}(a \\mid s) Q^{\\pi}(s, a) \\end{aligned} \\quad \\quad \\text{(1.2)} \\] 其中，状态平稳分布 \\(d^{\\pi}(s)\\) 定义为 \\[ d^{\\pi}(s)=\\lim _{t \\rightarrow \\infty} P\\left(s_{t}=s \\mid s_{0}, \\pi_{\\theta}\\right) \\] 另一种定义从trajectory角度出发，公式如下： \\[ J(\\boldsymbol{\\theta}) \\doteq E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[\\sum_{t} r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\quad \\quad \\text{(1.3)} \\] 即$ $ 是一次trajectory，服从以 \\(\\theta\\) 作为参数的随机变量 \\[ \\tau \\sim p_{\\theta}\\left(\\mathbf{s}_{1}, \\mathbf{a}_{1}, \\ldots, \\mathbf{s}_{T}, \\mathbf{a}_{T}\\right) \\] \\(J(\\theta)\\) 对于所有的可能的 \\(\\tau\\) 求 expected return。这种视角下对于finite 和 infinite horizon来说也有变形。 Infinite horizon 情况下，通过 \\((s, a)\\) 的marginal distribution来计算 \\[ J(\\boldsymbol{\\theta}) \\doteq E_{(\\mathbf{s}, \\mathbf{a}) \\sim p_{\\theta}(\\mathbf{s}, \\mathbf{a})}[r(\\mathbf{s}, \\mathbf{a})] \\quad \\quad \\text{(1.4)} \\] Finite horizon 情况下，通过每一时刻下 \\((s_t, a_t)\\) 的marginal distribution来计算 \\[ J(\\boldsymbol{\\theta}) \\doteq \\sum_{t=1}^{T} E_{\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right) \\sim p_{\\theta}\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)} \\quad \\quad \\text{(1.5)} \\] 关于第二个问题，如何求出或者估计 $ J()$ 就是 policy gradient theorem 的主题了。仔细想想确实会有一些问题。一是 reward 随机变量 \\(R(s, a)\\) 是离散情况下 $ J()$ 还是否存在，再是 \\(J(\\theta)\\) 不仅取决于agent 主观的 \\(\\pi_{\\theta}\\)，还取决于环境客观的dynamics model \\[ p\\left(s^{\\prime}, r \\mid s, a\\right) = \\operatorname{Pr}\\left\\{S_{t}=s^{\\prime}, R_{t}=r \\mid S_{t-1}=s, A_{t-1}=a\\right\\} \\] 当环境dynamics未知时，如何再去求 $ J()$ 呢。还有就是如果涉及到状态的分布也是取决于环境dynamics的，计算 $ J()$ 也面临同样的问题。 幸好，policy gradient定理完美的解答了上述问题。我们先来看看它的表述内容。 Policy Gradient Theorem 策略梯度定理证明了，无论定义何种 \\(J(\\theta)\\) ，策略梯度等比于下式，其中 \\(\\mu(s)\\) 为 \\(\\pi_{\\theta}\\) 下的状态分布。等比系数在episodic情况下为episode的平均长度，在infinite horizon情况下为1。 \\[ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_{a} q_{\\pi}(s, a) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) \\quad \\quad \\text{(2.1)} \\] 考虑到系数可以包含在步长 \\(\\alpha\\) 中， \\(\\mu(s)\\) 是on policy \\(\\pi_{\\theta}\\) 的权重，\\(\\nabla J(\\theta)\\) 也可以写成期望形式的等式，注意，下式中 \\(S_t\\) 从具体 \\(s\\) 变成了随机变量，随机概率部分移到了 \\(\\mathbb{E}_{\\pi}\\)中了。 \\[ \\nabla J(\\boldsymbol{\\theta}) =\\mathbb{E}_{\\pi}\\left[\\sum_{a} q_{\\pi}\\left(S_{t}, a\\right) \\nabla \\pi\\left(a \\mid S_{t}, \\boldsymbol{\\theta}\\right)\\right] \\quad \\quad \\text{(2.2)} \\] Policy Gradient 定理的伟大之处在于等式右边并没有 \\(d^{\\pi}(s)\\)，或者环境transition model \\(p\\left(s^{\\prime}, r \\mid s, a\\right)\\)！同时，等式右边变换成了最利于统计采样的期望形式，因为期望可以通过样本的平均来估算。 但是，这里必须注意的是action space的期望并不是基于 $(a S_{t}, ) $ 的权重的，因此，继续改变形式，引入 action space的 on policy 权重 $(a S_{t}, ) $ ，得到 2.3式。 \\[ \\nabla J(\\boldsymbol{\\theta})=\\mathbb{E}_{\\pi}\\left[\\sum_{a} \\pi\\left(a \\mid S_{t}, \\boldsymbol{\\theta}\\right) q_{\\pi}\\left(S_{t}, a\\right) \\frac{\\nabla \\pi\\left(a \\mid S_{t}, \\boldsymbol{\\theta}\\right)}{\\pi\\left(a \\mid S_{t}, \\boldsymbol{\\theta}\\right)}\\right] \\quad \\quad \\text{(2.3)} \\] 将 \\(a\\) 替换成 $A_{t} $，得到2.4式 \\[ \\nabla J(\\boldsymbol{\\theta})==\\mathbb{E}_{\\pi}\\left[q_{\\pi}\\left(S_{t}, A_{t}\\right) \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)}\\right] \\quad \\quad \\text{(2.4)} \\] 将 \\(q_{\\pi}\\)替换成 \\(G_t\\)，由于 \\[ \\mathbb{E}_{\\pi}[G_{t} \\mid S_{t}, A_{t}]= q_{\\pi}\\left(S_{t}, A_{t}\\right) \\] 得到2.5式 \\[ \\nabla J(\\boldsymbol{\\theta})==\\mathbb{E}_{\\pi}\\left[G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)}\\right] \\quad \\quad \\text{(2.5)} \\] 至此，action 和 state space的权重都源自 \\(\\pi_{\\theta}\\)，期望内的随机变量可以通过 \\(\\pi_{\\theta}\\) 在每一时间 t 采样来无偏估计，这便是大名鼎鼎的 REINFORCE 算法，即Monte Carlo Policy Gradient。 \\[ \\nabla J(\\boldsymbol{\\theta}) \\approx G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}\\right)} \\quad \\quad \\text{(2.6)} \\] 此时，\\(\\theta\\) 迭代更新公式为 \\[ \\boldsymbol{\\theta}_{t+1} \\doteq \\boldsymbol{\\theta}_{t}+\\alpha G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)} \\quad \\quad \\text{(2.7)} \\] 下面是REINFORCE算法完整流程 Policy Gradient Theorem - Trajectory Form Trajectory 形式的策略梯度定理也很常见，这里也总结一下，回顾 1.3 式 \\(J(\\theta)\\)的定义 \\[ J(\\boldsymbol{\\theta}) \\doteq E_{\\tau \\sim p_{\\theta}(\\tau)}\\left[\\sum_{t} r\\left(\\mathbf{s}_{t}, \\mathbf{a}_{t}\\right)\\right] \\quad \\quad \\text{(1.3)} \\] 最后可以证明出 \\[ \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) R(\\tau)\\right] \\quad \\quad \\text{(3.1)} \\] 3.1式中每一时刻 t 中依赖全时刻的 \\(R(\\tau)\\) ，进一步优化可以证明，时刻 t 只依赖于后续reward sum，即 reward-to-go， $ _{t}$ \\[ \\hat{R}_{t} \\doteq \\sum_{t^{\\prime}=t}^{T} R\\left(s_{t^{\\prime}}, a_{t^{\\prime}}, s_{t^{\\prime}+1}\\right) \\] 最终的策略梯度定理的形式为： \\[ \\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}\\left[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{t} \\mid s_{t}\\right) \\hat{R}_{t} \\right] \\quad \\quad \\text{(3.2)} \\] 由于 log-derivative trick的存在，3.2式和2.5式（Sutton 教程中的policy gradient）等价。 \\[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a)=\\frac{\\nabla_{\\theta} \\pi_{\\theta}}{\\pi_{\\theta}} \\quad \\quad \\text{(3.3)} \\] 和监督学习的联系 Policy Gradient中的 \\(\\nabla_{\\theta} \\log \\pi\\) 广泛存在在机器学习范畴中，被称为 score function gradient estimator。RL 在supervised learning settings 中有 imitation learning，即通过专家的较优stochastic policy \\(\\pi_{\\theta}(a|s)\\) 收集数据集 \\[ \\{(s_1, a^{*}_1), (s_2, a^{*}_2), ...\\} \\] 算法有监督的学习去找到max log likelyhook 的 \\(\\theta^{*}\\) \\[ \\theta^{*}=\\operatorname{argmax}_{\\theta} \\sum_{n} \\log \\pi_{\\theta}\\left(a_{n}^{*} \\mid s_{n}\\right) \\quad \\quad \\text{(4.1)} \\] 此时，参数迭代公式为 \\[ \\theta_{n+1} \\leftarrow \\theta_{n}+\\alpha_{n} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{n}^{*} \\mid s_{n}\\right) \\quad \\quad \\text{(4.2)} \\] 对照Policy Graident RL，on-policy \\(\\pi_{\\theta}(a|s)\\) 产生数据集 \\[ \\{(s_1, a_1, r_1), (s_2, a_2, r_2), ...\\} \\] 目标是最大化on-policy \\(\\pi_{\\theta}\\) 分布下的expected return \\[ \\theta^{*}=\\operatorname{argmax}_{\\theta} \\sum_{n} R(\\tau_{n}) \\] 对照2.7式 \\(\\theta\\) 的更新公式，2.7式可以写成如下4.3式 \\[ \\theta_{n+1} \\leftarrow \\theta_{n}+\\alpha_{n} G_{n} \\nabla_{\\theta} \\log \\pi_{\\theta}\\left(a_{n} \\mid s_{n}\\right) \\quad \\quad \\text{(4.3)} \\] 对比 4.3 和 4.2，发现此时4.3中只多了一个权重系数 \\(G_n\\)。 关于 $G_{n} {} {}(a_{n} s_{n}) $ 或者 \\(G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}\\) 有一些深入的理解。 首先policy gradient RL 不像supervised imitation learning直接有label 作为signal，PG RL必须通过采样不同的action获得reward或者return作为signal，即1.4式中的 \\[ E_{(\\mathbf{s}, \\mathbf{a}) \\sim p_{\\theta}(\\mathbf{s}, \\mathbf{a})}[r(\\mathbf{s}, \\mathbf{a})] \\quad \\quad \\text{(5.1)} \\] 广义的score function gradient estimator 对于形式为5.2的函数期望求gradient。对比上式，PG RL ， \\(f(x)\\)视为reward 随机变量，期望是under on-policy \\(\\pi_{\\theta}\\)。 \\[ E_{x \\sim p(x \\mid \\theta)}[f(x)] \\quad \\quad \\text{(5.2)} \\] 以下是score function gradient estimator的推导，这里不做赘述，主要利用了3.3式的 log-derivative trick。 \\[ \\begin{aligned} \\nabla_{\\theta} E_{x}[f(x)] &amp;=\\nabla_{\\theta} \\sum_{x} p(x) f(x) \\\\ &amp;=\\sum_{x} \\nabla_{\\theta} p(x) f(x) \\\\ &amp;=\\sum_{x} p(x) \\frac{\\nabla_{\\theta} p(x)}{p(x)} f(x) \\\\ &amp;=\\sum_{x} p(x) \\nabla_{\\theta} \\log p(x) f(x) \\\\ &amp;=E_{x}\\left[f(x) \\nabla_{\\theta} \\log p(x)\\right] \\end{aligned} \\quad \\quad \\text{(5.3)} \\] Policy Gradient 工作的机制大致如下 首先，根据现有的 on-policy \\(\\pi_{\\theta}\\) 采样出一些动作 action 产生trajectories，这些trajectories最终得到反馈 \\(R(\\tau)\\) 用采样到的数据通过R加权来代替imitation learning的labeled loss \\[ R(s,a) \\nabla \\pi_{\\theta_{t}}(a \\mid s) \\approx \\nabla \\pi_{\\theta_{t}}(a^{*} \\mid s) \\] 最后，由于采样到的action分布服从于\\(a \\sim \\pi_{\\theta}(a)\\) ，除掉 \\(\\pi_{\\theta}\\) ： \\(G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}\\) 此时，采样的均值可以去无偏估计2.2式中的Expectation。 \\[ \\sum_N G_{t} \\frac{\\nabla \\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)}{\\pi\\left(A_{t} \\mid S_{t}, \\boldsymbol{\\theta}_{t}\\right)} \\] \\[ =\\mathbb{E}_{\\pi}\\left[\\sum_{a} q_{\\pi}\\left(S_{t}, a\\right) \\nabla \\pi\\left(a \\mid S_{t}, \\boldsymbol{\\theta}\\right)\\right] \\]","link":"/zh/2020/rl-policy-gradient/"},{"title":"通过代码学Sutton强化学习：从Q-Learning 演化到 DQN","text":"上一期 MyEncyclopedia公众号文章 SARSA、Q-Learning和Expected SARSA时序差分算法训练CartPole中，我们通过CartPole的OpenAI Gym环境实现了Q-learning算法，这一期，我们将会分析Q-learning算法面临的maximization bias 问题和提出double learning算法来改进。接着，我们将tabular Q-learning算法扩展到用带参函数来近似 Q(s, a)，这就是Deepmind 在2015年Nature上发表的Deep Q Network （DQN）思想：用神经网络结合Q-learning算法实现超越人类玩家打Atari游戏的水平。 Q-Learning 回顾 \\[ \\begin{align*} &amp;\\textbf{Q-learning (off-policy TD Control) for estimating } \\pi \\approx \\pi_{*} \\\\ &amp; \\text{Algorithm parameters: step size }\\alpha \\in ({0,1}]\\text{, small }\\epsilon &gt; 0 \\\\ &amp; \\text{Initialize }Q(s,a), \\text{for all } s \\in \\mathcal{S}^{+}, a \\in \\mathcal{A}(s) \\text{, arbitrarily except that } Q(terminal, \\cdot) = 0 \\\\ &amp; \\text{Loop for each episode:}\\\\ &amp; \\quad \\text{Initialize }S\\\\ &amp; \\quad \\text{Loop for each step of episode:} \\\\ &amp; \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\ &amp; \\quad \\quad \\text{Take action }A, \\text { observe } R, S^{\\prime} \\\\ &amp; \\quad \\quad Q(S,A) \\leftarrow Q(S,A) + \\alpha[R+\\gamma \\max_{a}Q(S^{\\prime}, a) - Q(S,A)] \\\\ &amp; \\quad \\quad S \\leftarrow S^{\\prime}\\\\ &amp; \\quad \\text{until }S\\text{ is terminal} \\\\ \\end{align*} \\] 在SARSA、Q-Learning和Expected SARSA时序差分算法训练CartPole&nbsp;中，我们实现了同样基于 \\(\\epsilon\\)-greedy 策略的Q-learning算法和SARSA算法，两者代码上的区别确实不大，但本质上Q-learning是属于 off-policy 范畴而 SARSA却属于 on-policy 范畴。一种理解方式是，Q-learning相比于SARSA少了第二次从 \\(\\epsilon\\)-greedy 策略采样出下一个action，即S, A, R', S', A' 五元组中最后一个A'，而直接通过max操作去逼近 \\(q^{*}\\)。如此，Q-learning并没有像SARSA完成一次完整的GPI（Generalized Policy Iteration），缺乏on-policy的策略迭代的特点，故而 Q-learning 属于off-policy方法。我们也可以从另一个角度来分析两者的区别。注意到这两个算法不是一定非要使用 \\(\\epsilon\\)-greedy 策略的。对于Q-learning来说，完全可以使用随机策略，理论上已经证明，只要保证每个action以后依然有几率会被探索下去，Q-learning 最终会收敛到最优策略。Q-learning使用 \\(\\epsilon\\)-greedy 是为了能快速收敛。对于SARSA算法来说，则无法使用随机策略，因为随机策略无法形成策略提升。而 \\(\\epsilon\\)-greedy 策略却可以形成策略迭代，完成策略提升，当然，\\(\\epsilon\\)-greedy 策略在 SARSA 算法中也可以保证快速收敛。因此，尽管两者都使用 \\(\\epsilon\\)-greedy 策略再借由环境产生reward和state，它们的作用并非完全一样。至此，我们可以体会到on-policy和off-policy本质的区别。 收敛条件 Tabular Q-Learning 收敛到最佳Q函数的条件如下[2]: \\[ \\Sigma^{\\infty}_{n=0} \\alpha_{n} = {\\infty} \\quad \\text{ AND } \\quad \\Sigma^{\\infty}_{n=0} \\alpha^2_{n} \\lt {\\infty} \\] 一种方式是将 \\(\\alpha\\)设置成 (s, a)访问次数的倒数：\\(\\alpha_{n}(s,a) = 1/ n(s,a )\\) 则整体更新公式为 \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha_n(s, a)[R+\\gamma \\max_{a^{\\prime}}Q(s^{\\prime}, a^{\\prime}) - Q(s, a)] \\] Q-Learning 最大化偏差问题 Q-Learning 会产生最大化偏差问题（Maximization Bias，在Sutton 教材6.7节），它的原因是用估计值中取最大值去估计真实值中最大是有偏的。这个可以做如下试验来模拟，若有5个 [-3, 3] 的离散均匀分布 \\(d_i\\)，\\(\\max(\\mathbb{E}[d_i]) = 0\\)，但是若我们用单批采样 \\(x_i \\sim d_i\\)来估算 \\(\\mathbb{E}[d_i]\\)在取max的话，\\(\\mathbb{E}[{\\max(x_i)]}\\) 是有bias的。但是如果我们将这个过程分解成选择最大action和评估其值两步，每一步用独立的采样集合就可以做到无偏，这个改进方法称为double learning。具体过程为第一步在\\(Q_1\\)集合中找到最大的action，第二步在\\(Q_2\\)中返回此action值，即： \\[ \\begin{align*} A^{\\star} = \\operatorname{argmax}_{a}Q_1(a) \\\\ Q_2(A^{\\star}) = Q_2(\\operatorname{argmax}_{a}Q_1(a)) \\end{align*} \\] 则无限模拟后结果是无偏的：\\(\\mathbb{E}[Q_2(A^{\\star})] = q(A^{\\star})\\) 下面是简单模拟试验两种方法的均值比较 Maximization Bias 试验完整代码如下 1234567891011121314151617181920212223242526272829303132import randomfrom math import floorimport numpy as npimport pandas as pdimport seaborn as snsdef uniform(a: int, b: int) -&gt; int: u = random.random() return a + floor((b - a + 1) * u)if __name__ == \"__main__\": total_max_bias = 0 avgs_max_bias = [] total_double_sampling = 0 avgs_double_sampling = [] for e in range(1, 100): samples = np.array([uniform(-3, 3) for _ in range(5)]) max_sample = max(samples) total_max_bias += max_sample avgs_max_bias.append(total_max_bias / e) samples2 = np.array([uniform(-3, 3) for _ in range(5)]) total_double_sampling += samples2[np.argmax(samples)] avgs_double_sampling.append(total_double_sampling / e) df = pd.DataFrame({'Max of Samples': avgs_max_bias, 'Double Samples': avgs_double_sampling}) import matplotlib.pyplot as plt sns.lineplot(data=df) plt.show() 回到Q-learning 中使用的 \\(\\epsilon\\)-greedy策略，Q-learning可以保证随着\\(\\epsilon\\) 的减小，最大化偏差会 asymptotically 趋近于真实值，但是double learning 可以更快地趋近于真实值。 Maximization Bias vs Double learning 下面是Sutton 强化学习第二版6.7节中完整的Double Q-learning算法。 \\[ \\begin{align*} &amp;\\textbf{Double Q-learning, for estimating } Q_1 \\approx Q_2 \\approx q_{*} \\\\ &amp; \\text{Algorithm parameters: step size }\\alpha \\in ({0,1}]\\text{, small }\\epsilon &gt; 0 \\\\ &amp; \\text{Initialize }Q_1(s,a), \\text{ and } Q_2(s,a) \\text{, for all } s \\in \\mathcal{S}^{+}, a \\in \\mathcal{A}(s) \\text{, such that } Q(terminal, \\cdot) = 0 \\\\ &amp; \\text{Loop for each episode:}\\\\ &amp; \\quad \\text{Initialize }S\\\\ &amp; \\quad \\text{Loop for each step of episode:} \\\\ &amp; \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy } \\epsilon\\text{-greedy in } Q_1 + Q_2 \\\\ &amp; \\quad \\quad \\text{Take action }A, \\text { observe } R, S^{\\prime} \\\\ &amp; \\quad \\quad \\text{With 0.5 probability:} \\\\ &amp; \\quad \\quad \\quad Q_1(S,A) \\leftarrow Q_1(S,A) + \\alpha \\left ( R+\\gamma Q_2(S^{\\prime}, \\operatorname{argmax}_{a}Q_1(S^{\\prime}, a)) - Q_1(S,A) \\right )\\\\ &amp; \\quad \\quad \\text{else:} \\\\ &amp; \\quad \\quad \\quad Q_1(S,A) \\leftarrow Q_1(S,A) + \\alpha \\left ( R+\\gamma Q_2(S^{\\prime}, \\operatorname{argmax}_{a}Q_1(S^{\\prime}, a)) - Q_1(S,A) \\right )\\\\ &amp; \\quad \\quad S \\leftarrow S^{\\prime}\\\\ &amp; \\quad \\text{until }S\\text{ is terminal} \\\\ \\end{align*} \\] 更详细内容，可以参考 Hado V. Hasselt 的 Double Q-learning paper [3]。 Gradient Q-Learning Tabular Q-learning由于受制于维度爆炸，无法扩展到高维状态空间，一般近似解决方案是用 approximating function来逼近Q函数。即我们将状态抽象出一组特征 \\(s = \\vec x= [x_1, x_2, ..., x_n]^T\\)，Q 用一个 x 的函数来近似表达 \\(Q(s, a) \\approx g(\\vec x; \\theta)\\)，如此，就联系起了深度神经网络。有了函数表达，深度学习还必须的元素是损失函数，这个很自然的可以用 TD error。至此，问题转换成深度学习的几个要素均已具备，Q-learning算法改造成了深度学习中的有监督问题。 估计值：\\(Q\\left(s, a ; \\theta\\right)\\) 目标值：\\(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta\\right)\\) 损失函数： \\[ L\\left(\\theta\\right)=\\mathbb{E}_{\\left(s, a, r, s^{\\prime}\\right) \\sim \\mathrm{U}(D)}\\left[\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta\\right)-Q\\left(s, a ; \\theta\\right)\\right)^{2}\\right] \\] 收敛性分析 首先明确一点，至此 gradient q-learning 和 tabular Q-learning 一样，都是没有记忆的，即对于一个新的环境产生的 sample 去做 stochastic online update。 若Q函数是状态特征的线性函数，即 \\(Q(s, a; \\theta) = \\Sigma_i w_i x_i\\) ，那么线性Gradient Q-learning的收敛条件和Tabular Q-learning 一样，也为 \\[ \\Sigma^{\\infty}_{n=0} \\alpha_{n} = {\\infty} \\quad \\text{ AND } \\quad \\Sigma^{\\infty}_{n=0} \\alpha^2_{n} \\lt {\\infty} \\] 若Q函数是非线性函数，即使符合上述条件，也无法保证收敛，本质上源于改变 \\(\\theta\\) 使得 Q 值在 (s, a) 点上减小误差会影响 (s, a) 周边点的误差。 DQN减少不收敛的两个技巧 \\(\\theta_{i-1} \\rightarrow \\theta_{i}\\) 改变导致max中的估计值和目标值中的Q同时变化，面临着 chasing its own tail 的问题。解决的方法是使用不同的参数来parameterize两个Q，并且目标值的Q网络参数固定一段时间产生一批固定策略下的环境采样。这个技巧称为 Target Network。引入这个 trick 后深度学习的要素变成 估计值：\\(Q\\left(s, a ; \\theta_{i}\\right)\\) 目标值：\\(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_i^{-}\\right)\\) 损失函数，DQN在Nature上的loss函数： \\[ L\\left(\\theta_{i}\\right)=\\mathbb{E}_{\\left(s, a, r, s^{\\prime}\\right) \\sim \\mathrm{U}(D)}\\left[\\left(r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta_{i}^{-}\\right)-Q\\left(s, a ; \\theta_{i}\\right)\\right)^{2}\\right] \\] 尽管目标值的 \\(Q(;\\theta^{-})\\)固定了，但是\\(\\theta_{i-1} \\rightarrow \\theta_{i}\\) 还会使得估计值的 \\(Q(s, a;\\theta_i)\\) 在变化的同时影响其他的 \\(Q(s_k, a_j;\\theta_i)\\)，让之前训练过的 (s, a)的点的损失值发生变化，解决的办法是将 online stochastic 改成 batch gradient，也就是将最近的一系列采样值保存下来，这个方法称为 experience replay。 有了这两个优化，Deep Q Network投入实战效果就容易收敛了，以下是Deepmind 发表在Nature 的 Human-level control through deep reinforcement learning [1] 的完整算法流程。 \\[ \\begin{align*} &amp;\\textbf{Deep Q-learning with experience replay}\\\\ &amp; \\text{Initialize replay memory } D\\text{ to capacity } N \\\\ &amp; \\text{Initialize action-value function } Q \\text{ with random weights } \\theta \\\\ &amp; \\text{Initialize target action-value function } \\hat{Q} \\text{ with weights } \\theta^{-} = \\theta \\\\ &amp; \\textbf{For} \\text{ episode = 1, } M \\textbf{ do} \\\\ &amp; \\text{Initialize sequences } s_1 = \\{x_1\\} \\text{ and preprocessed sequence } \\phi_1 = \\phi(s_1)\\\\ &amp; \\quad \\textbf{For } t=\\text{ 1, T }\\textbf{ do} \\\\ &amp; \\quad \\quad \\text{With probability }\\epsilon \\text{ select a random action } a_t \\\\ &amp; \\quad \\quad \\text{otherwise select } a_t = \\operatorname{argmax}_{a}Q(\\phi(s_t), a; \\theta)\\\\ &amp; \\quad \\quad \\text{Execute action } a_t \\text{ in emulator and observe reward } r_t \\text{ and image }x_{t+1}\\\\ &amp; \\quad \\quad \\text{Set } s_{t+1} = s_t, a_t, x_{t+1} \\text{ and preprocess } \\phi_{t+1} = \\phi(s_{t+1})\\\\ &amp; \\quad \\quad \\text{Store transition } (\\phi_t, a_t, r_t, \\phi_{t+1}) \\text{ in } D\\\\ &amp; \\quad \\quad \\text{Sample random minibatch of transitions } (\\phi_j, a_j, r_j, \\phi_{j+1}) \\text{ from } D\\\\ &amp; \\quad \\quad \\text{Set } y_j= \\begin{cases} r_j \\quad \\quad\\quad\\quad\\text{if episode terminates at step j+1}\\\\ r_j + \\gamma \\max_{a^{\\prime}}\\hat Q(\\phi_{j+1}, a^{\\prime}; \\theta^{-}) \\quad \\text { otherwise}\\\\ \\end{cases} \\\\ &amp; \\quad \\quad \\text{Perform a gradient descent step on } (y_j - Q(\\phi_j, a_j; \\theta))^2 \\text{ with respect to the network parameters } \\theta\\\\ &amp; \\quad \\quad \\text{Every C steps reset } \\hat Q = Q\\\\ &amp; \\quad \\textbf{End For} \\\\ &amp; \\textbf{End For} \\end{align*} \\] DQN with Double Q-Learning DQN 算法和 Double Q-Learning 能不能结合起来呢？Hado van Hasselt 在 Deep Reinforcement Learning with Double Q-learning [4] 中提出参考 Double Q-learning 将 DQN 的目标值改成如下函数，可以进一步提升最初DQN的效果。 目标值：\\(r+\\gamma Q(s^{\\prime}, \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}; \\theta_t\\right); \\theta_t^{-})\\) 参考资料 Human-level control through deep reinforcement learning Volodymyr Mnih, Koray Kavukcuoglu, David Silver (2015) CS885 Reinforcement Learning Lecture 4b: May 11, 2018 Double Q-learning Hado V. Hasselt (2010) Deep Reinforcement Learning with Double Q-learning Hado van Hasselt, Arthur Guez, David Silver (2015)","link":"/zh/2020/rl-qlearning-to-dqn/"},{"title":"通过代码学Sutton强化学习3：21点游戏的策略蒙特卡洛值预测","text":"从这期开始我们进入Sutton强化学习第二版，第五章蒙特卡洛方法。蒙特卡洛方法是一种在工程各领域都存在的基本方法，在强化领域中，其特点是无需知道环境的dynamics，只需不断模拟记录并分析数据即可逼近理论真实值。蒙特卡洛方法本篇将会用21点游戏作为示例来具体讲解其原理和代码实现。 21点游戏问题 21点游戏是一个经典的赌博游戏。大致规则是玩家和庄家各发两张牌，一张明牌，一张暗牌。玩家和庄家可以决定加牌或停止加牌，新加的牌均为暗牌，最后比较两个玩家的牌面和，更接近21点的获胜。游戏的变化因素是牌Ace，既可以作为11也可以作为1来计算，算作11的时候称作usable。 Sutton教材中的21点游戏规则简化了几个方面用于控制问题状态数： 已发的牌的无状态性：和一副牌的21点游戏不同的是，游戏环境简化为牌是可以无穷尽被补充的，一副牌的某一张被派发后，同样的牌会被补充进来，或者可以认为每次发放的牌都是从一副新牌中抽出的。统计学中的术语称为重复采样（sample with replacement）。这种规则下极端情况下，玩家可以拥有 5个A或者5个2。另外，这会导致玩家无法通过开局看到的3张牌的信息推断后续发牌的概率，如此就大规模减小了游戏状态数。 庄家和玩家独立游戏，无需按轮次要牌。开局给定4张牌后，玩家先行动，加牌直至超21点或者停止要牌，如果超21点，玩家输，否则，等待庄家行动，庄家加牌直至超21点或者停止要牌，如果超21点，庄家输，否则比较两者的总点数。这种方式可以认为当玩家和庄家看到初始的三张牌后独立做一系列决策，最后比较结果，避免了交互模式下因为能观察到每一轮后对方牌数变化产生额外的信息而导致的游戏状态数爆炸。 有以上两个规则的简化，21点游戏问题的总状态数有下面三个维度 自己手中的点数和（12到21） 庄家明牌的点数（A到10) 庄家明牌是否有 A（True, False）。 状态总计总数为三个维度的乘积 10 * 10 * 2 = 200。 关于游戏状态有几个比较subtle的假设或者要素。首先，玩家初始时能看到三张牌，这三张牌确定了状态的三个维度的值，当然也就确定了Agent的初始状态，随后按照独立游戏的规则进行，玩家根据初始状态依照某种策略决策要牌还是结束要牌，新拿的牌更新了游戏状态，玩家转移到新状态下继续做决策。举个例子，假设初始时玩家明牌为8，暗牌为6，庄家明牌为7，则游戏状态为Tuple (14, 7, False)。若玩家的策略为教材中的固定规则策略：没到20或者21继续要牌。下一步玩家拿到牌3，则此时新状态为 (17, 7, False)，按照策略继续要牌。 第二个方面是游戏的状态完全等价于玩家观察到的信息。比如尽管初始时有4张牌，真正的状态是这四张牌的值，但是出于简化目的，不考虑partially observable 的情况，即不将暗牌纳入游戏状态中。另外，庄家做决策的时候也无法得知玩家的手中的总牌数。 第三个方面是关于玩家点数。考虑玩家初始时的两张牌为2，3，总点数是5，那么为何不将5加入到游戏状态中呢？原则上是可以将初始总和为2到11都加入到游戏状态，但是意义不大，原因在于我们已经假设了已发牌的无状态性，拿到的这两张牌并不会改变后续补充的牌的出现概率。当玩家初始总和为2到11时一定会追加牌，因为无论第三张牌是什么，都不会超过21点，只会增加获胜概率。若后续第三张牌为8，总和变成13，就进入了有效的游戏状态，因为此时如果继续要牌，获得10，则游戏输掉。因此，我们关心的游戏状态并不完全等价于所有可能的游戏状态。 21点游戏 OpenAI Gym环境 OpenAI Gym 已经实现了Sutton版本的21点游戏环境，并按上述规则来进行。在安装完OpenAI Gym包之后 import BlackjackEnv即可使用。 1from gym.envs.toy_text import BlackjackEnv 根据这个游戏环境，我们先来定义一些类型，可以令代码更具可读性和抽象化。State 上文说过是由三个分量组成的Tuple。Action 为bool类型 表示是否继续要牌。Reward 为+1或者-1，玩家叫牌过程中为0。StateValue 为书中的 \\(V_{\\pi}\\)，实现上是一个Dict。DeterministicPolicy 为一个函数，输入是某一状态，输出是唯一的决策动作。 {linenos12345State = Tuple[int, int, bool]Action = boolReward = floatStateValue = Dict[State, float]DeterministicPolicy = Callable[[State], Action] 以下代码是 BlackjackEnv 核心代码，step 方法的输入为玩家的决策动作（叫牌还是结束），并输出State, Reward, is_done。简单解释一下代码逻辑，当玩家继续加牌时，需要判断是否超21点，如果没有超过的话，返回下一状态，同时reward 为0，等待下一step方法。若玩家停止叫牌，则按照庄家策略：小于17时叫牌。游戏终局时产生+1表示玩家获胜，-1表示庄家获胜。 {linenos1234567891011121314151617181920212223class BlackjackEnv(gym.Env): def step(self, action): assert self.action_space.contains(action) if action: # hit: add a card to players hand and return self.player.append(draw_card(self.np_random)) if is_bust(self.player): done = True reward = -1. else: done = False reward = 0. else: # stick: play out the dealers hand, and score done = True while sum_hand(self.dealer) &lt; 17: self.dealer.append(draw_card(self.np_random)) reward = cmp(score(self.player), score(self.dealer)) if self.natural and is_natural(self.player) and reward == 1.: reward = 1.5 return self._get_obs(), reward, done, {} def _get_obs(self): return (sum_hand(self.player), self.dealer[0], usable_ace(self.player)) 下面示例如何调用step方法生成一个episode的数据集。数据集的类型为 List[Tuple[State, Action, Reward]]。 {linenos12345678910def gen_episode_data(policy: DeterministicPolicy, env: BlackjackEnv) -&gt; List[Tuple[State, Action, Reward]]: episode_history = [] state = env.reset() done = False while not done: action = policy(state) next_state, reward, done, _ = env.step(action) episode_history.append((state, action, reward)) state = next_state return episode_history 策略的蒙特卡洛值预测 Monte Carlo Prediction解决如下问题：当给定Agent 策略\\(\\pi\\)时，反复试验来预估策略的 \\(V_{\\pi}\\) 值。具体来说，产生一系列的episode数据之后，对于出现了的所有状态分别计算其Return，再通过 average 某一状态 s 的Return来估计 \\(V_{\\pi}(s)\\)，理论上，依据大数定理（Law of large numbers），在可以无限模拟的情况下，Monte Carlo prediction 一定会收敛到真实的 \\(V_{\\pi}\\)。算法实现上有两个略微不同的版本，一个版本称为 First-visit，另一个版本称为 Every-visit，区别在于如何计算出现的状态 s 的 Return值。 对于 First-visit 来说，当状态 s 第一次出现时计算一次 Returns，若继续出现状态 s 不再重复计算。对于Every-visit来说，每次出现 s 计算一次 Returns(s)。举个例子，某episode 数据如下： \\[ S_1, R_1, S_2, R_2, S_1, R_3, S_3, R_4 \\] First-visit 对于状态S1的Returns计算为 \\[ Returns(S_1) = R_1 + R_2 + R_3 + R_4 \\] Every-visit 对于状态S1的Returns计算了两次，因为S1出现了两次。 \\[ \\begin{align*} Returns(S_1) = \\frac{Return_1(S_1) + Return_2(S_1)}2 \\\\ = \\frac{(R_1 + R_2 + R_3 + R_4) + (R_3 + R_4)} 2 \\end{align*} \\] 下面用Monte Carlo来模拟解得书中示例玩家固定策略的V值，策略具体为：加牌直到手中点数&gt;=20，代码为 {linenos123456def fixed_policy(observation): \"\"\" sticks if the player score is &gt;= 20 and hits otherwise. \"\"\" score, dealer_score, usable_ace = observation return 0 if score &gt;= 20 else 1 First-visit MC Predicition 伪代码如下，注意考虑到实现上的高效性，在遍历episode序列数据时是从后向前扫的，这样可以边扫边更新G。 \\[ \\begin{align*} &amp;\\textbf{First-visit MC prediction, for estimating } V \\approx v_{\\pi} \\\\ &amp; \\text{Input: a policy } \\pi \\text{ to be evaluated} \\\\ &amp; \\text{Initialize} \\\\ &amp; \\quad V(s) \\in \\mathbb R \\text{, arbitrarily, for all }s \\in \\mathcal{S} \\\\ &amp; \\quad Returns(s) \\leftarrow \\text{ an empty list, arbitrarily, for all }s \\in \\mathcal{S} \\\\ &amp; \\\\ &amp; \\text{Loop forever (for episode):}\\\\ &amp; \\quad \\text{Generate an episode following } \\pi: S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\\\ &amp; \\quad G \\leftarrow 0\\\\ &amp; \\quad \\text{Loop for each step of episode, } t = T-1, T-2, ..., 0:\\\\ &amp; \\quad \\quad \\quad G \\leftarrow \\gamma G + R_{t+1}\\\\ &amp; \\quad \\quad \\quad \\text{Unless } S_t \\text{ appears in } S_0, S_1, ..., S_{t-1}\\\\ &amp; \\quad \\quad \\quad \\quad \\text{Append } G \\text { to }Returns(S_t) \\\\ &amp; \\quad \\quad \\quad \\quad V(S_t) \\leftarrow \\operatorname{average}(Returns(S_t))\\\\ \\end{align*} \\] 对应的 python 实现 {linenos12345678910111213141516171819def mc_prediction_first_visit(policy: DeterministicPolicy, env: BlackjackEnv, num_episodes, discount_factor=1.0) -&gt; StateValue: returns_sum = defaultdict(float) returns_count = defaultdict(float) for episode_i in range(1, num_episodes + 1): episode_history = gen_episode_data(policy, env) G = 0 for t in range(len(episode_history) - 1, -1, -1): s, a, r = episode_history[t] G = discount_factor * G + r if not any(s_a_r[0] == s for s_a_r in episode_history[0: t]): returns_sum[s] += G returns_count[s] += 1.0 V = defaultdict(float) V.update({s: returns_sum[s] / returns_count[s] for s in returns_sum.keys()}) return V Every-visit MC Prediciton Every-visit 代码实现相对更简单一些，t 从后往前遍历时更新对应s的状态变量。如下所示 {linenos123456789101112131415161718def mc_prediction_every_visit(policy: DeterministicPolicy, env: BlackjackEnv, num_episodes, discount_factor=1.0) -&gt; StateValue: returns_sum = defaultdict(float) returns_count = defaultdict(float) for episode_i in range(1, num_episodes + 1): episode_history = gen_episode_data(policy, env) G = 0 for t in range(len(episode_history) - 1, -1, -1): s, a, r = episode_history[t] G = discount_factor * G + r returns_sum[s] += G returns_count[s] += 1.0 V = defaultdict(float) V.update({s: returns_sum[s] / returns_count[s] for s in returns_sum.keys()}) return V 策略 V值 3D 可视化 运行first-visit 算法，模拟10000次episode，fixed_policy的V值的3D图为下面两张图，分别是不含usable Ace和包含usable Ace。总的说来，一旦玩家能到达20点或21点获胜概率极大，到达13-17获胜概率较小，在11-13时有一定获胜概率，比较符合经验直觉。 first-visit MC 10000次没有usable A的V值 first-visit MC 10000次含有usable A的V值 同样运行every-visit 算法，模拟10000次的V值图。对比两种方法结果比较接近。 every-visit MC 10000次没有usable A的V值 every-visit MC 10000次含有usable A的V值","link":"/zh/2020/rl-sutton-blackjack-1/"},{"title":"通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制","text":"这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期21点游戏的策略蒙特卡洛值预测学习了如何用Monte Carlo来预估给定策略 \\(\\pi\\) 的 \\(V_{\\pi}\\) 值之后，这一期我们用Monte Carlo方法来解得21点游戏最佳策略 \\(\\pi_{*}\\)。 蒙特卡洛策略提升 回顾一下，在Grid World 策略迭代和值迭代中由于存在Policy Improvement Theorem，我们可以利用环境dynamics信息计算出策略v值，再选取最greedy action的方式改进策略，形成策略提示最终能够不断逼近最佳策略。 \\[ \\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2} \\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{*} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{*} \\] Monte Carlo Control方法搜寻最佳策略 \\(\\pi{*}\\)，是否也能沿用同样的思路呢？答案是可行的。不过，不同于第四章中我们已知环境MDP就知道状态的前后依赖关系，进而从v值中能推断出策略 \\(\\pi\\)，在Monte Carlo方法中，环境MDP是未知的，因而我们只能从action-value中下手，通过海量Monte Carlo试验来近似 \\(q_{\\pi}\\)。有了策略 Q 值，再和MDP策略迭代方法一样，选取最greedy action的策略，这种策略提示方式理论上被证明了最终能够不断逼近最佳策略。 \\[ \\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} q_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1} \\stackrel{\\mathrm{E}}{\\longrightarrow} q_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2} \\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{*} \\stackrel{\\mathrm{E}}{\\longrightarrow} q_{*} \\] 但是此方法有一个前提要满足，由于数据是依据策略 \\(\\pi_{i}\\) 生成的，理论上需要保证在无限次的模拟过程中，每个状态都必须被无限次访问到，才能保证最终每个状态的Q估值收敛到真实的 \\(q_{*}\\)。满足这个前提的一个简单实现是强制随机环境初始状态，保证每个状态都能有一定概率被生成。这个思路就是 Monte Carlo Control with Exploring Starts算法，伪代码如下： \\[ \\begin{align*} &amp;\\textbf{Monte Carlo ES (Exploring Starts), for estimating } \\pi \\approx \\pi_{*} \\\\ &amp; \\text{Initialize:} \\\\ &amp; \\quad \\pi(s) \\in \\mathcal A(s) \\text{ arbitrarily for all }s \\in \\mathcal{S} \\\\ &amp; \\quad Q(s, a) \\in \\mathbb R \\text{, arbitrarily, for all }s \\in \\mathcal{S}, a \\in \\mathcal A(s) \\\\ &amp; \\quad Returns(s, a) \\leftarrow \\text{ an empty list, for all }s \\in \\mathcal{S}, a \\in \\mathcal A(s)\\\\ &amp; \\\\ &amp; \\text{Loop forever (for episode):}\\\\ &amp; \\quad \\text{Choose } S_0\\in \\mathcal{S}, A_0 \\in \\mathcal A(S_0) \\text{ randomly such that all pairs have probability &gt; 0} \\\\ &amp; \\quad \\text{Generate an episode from } S_0, A_0 \\text{, following } \\pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\\\ &amp; \\quad G \\leftarrow 0\\\\ &amp; \\quad \\text{Loop for each step of episode, } t = T-1, T-2, ..., 0:\\\\ &amp; \\quad \\quad \\quad G \\leftarrow \\gamma G + R_{t+1}\\\\ &amp; \\quad \\quad \\quad \\text{Unless the pair } S_t, A_t \\text{ appears in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\\\ &amp; \\quad \\quad \\quad \\quad \\text{Append } G \\text { to }Returns(S_t, A_t) \\\\ &amp; \\quad \\quad \\quad \\quad Q(S_t, A_t) \\leftarrow \\operatorname{average}(Returns(S_t, A_t))\\\\ &amp; \\quad \\quad \\quad \\quad \\pi(S_t) \\leftarrow \\operatorname{argmax}_a Q(S_t, a)\\\\ \\end{align*} \\] 下面我们实现21点游戏的Monte Carlo ES 算法。21点游戏只有200个有效的状态，可以满足算法要求的生成episode前先随机选择某一状态的前提条件。 相对于上一篇，我们增加 ActionValue和Policy的类型定义，ActionValue表示 \\(q(s, a)\\) ，是一个State到动作分布的Dict，Policy 类型也一样。Actions为一维ndarray，维数是离散动作数量。 {linenos123456State = Tuple[int, int, bool]Action = boolReward = floatActions = np.ndarrayActionValue = Dict[State, Actions]Policy = Dict[State, Actions] 下面代码示例如何给定 Policy后，依据指定状态state的动作分布采样，决定下一动作。 123policy: PolicyA: ActionValue = policy[state]action = np.random.choice([0, 1], p=A/sum(A)) 整个算法的 python 代码实现如下： {linenos1234567891011121314151617181920212223242526def mc_control_exploring_starts(env: BlackjackEnv, num_episodes, discount_factor=1.0) \\ -&gt; Tuple[ActionValue, Policy]: states = list(product(range(10, 22), range(1, 11), (True, False))) policy = {s: np.ones(env.action_space.n) * 1.0 / env.action_space.n for s in states} Q = defaultdict(lambda: np.zeros(env.action_space.n)) returns_sum = defaultdict(float) returns_count = defaultdict(float) for episode_i in range(1, num_episodes + 1): s0 = random.choice(states) reset_env_with_s0(env, s0) episode_history = gen_custom_s0_stochastic_episode(policy, env, s0) G = 0 for t in range(len(episode_history) - 1, -1, -1): s, a, r = episode_history[t] G = discount_factor * G + r if not any(s_a_r[0] == s and s_a_r[1] == a for s_a_r in episode_history[0: t]): returns_sum[s, a] += G returns_count[s, a] += 1.0 Q[s][a] = returns_sum[s, a] / returns_count[s, a] best_a = np.argmax(Q[s]) policy[s][best_a] = 1.0 policy[s][1-best_a] = 0.0 return Q, policy 在MC Exploring Starts 算法中，我们需要指定环境初始状态，一种做法是env.reset()时接受初始状态，但是考虑到不去修改第三方实现的 BlackjackEnv类，采用一个取巧的办法，在调用reset()后直接改写env 的私有变量，这个逻辑封装在 reset_env_with_s0 方法中。 {linenos123456789101112131415161718def reset_env_with_s0(env: BlackjackEnv, s0: State) -&gt; BlackjackEnv: env.reset() player_sum = s0[0] oppo_sum = s0[1] has_usable = s0[2] env.dealer[0] = oppo_sum if has_usable: env.player[0] = 1 env.player[1] = player_sum - 11 else: if player_sum &gt; 11: env.player[0] = 10 env.player[1] = player_sum - 10 else: env.player[0] = 2 env.player[1] = player_sum - 2 return env 算法结果的可视化和理论对比 下图是有Usable Ace情况下的理论最优策略。 理论最佳策略（有Ace） Monte Carlo方法策略提示的收敛是比较慢的，下图是运行10,000,000次episode后有Usable Ace时的策略 \\(\\pi_{*}^{\\prime}\\)。对比理论最优策略，MC ES在不少的状态下还未收敛到理论最优解。 MC ES 10M的最佳策略（有Ace） 同样的，下两张图是无Usable Ace情况下的理论最优策略和试验结果的对比。 理论最佳策略（无Ace） MC ES 10M的最佳策略（无Ace） 下面的两张图画出了运行代码10,000,000次episode后 \\(\\pi{*}\\)的V值图。 MC ES 10M的最佳V值（有Ace） MC ES 10M的最佳V值（无Ace） Exploring Starts 蒙特卡洛控制改进 为了避免Monte Carlo ES Control在初始时必须访问到任意状态的限制，教材中介绍了一种改进算法，On-policy first-visit MC control for \\(\\epsilon \\text{-soft policies}\\) ，它同样基于Monte Carlo 预估Q值，但用 \\(\\epsilon \\text{-soft}\\) 策略来代替最有可能的action策略作为下一次迭代策略，\\(\\epsilon \\text{-soft}\\) 本质上来说就是对于任意动作都保留 \\(\\epsilon\\) 小概率的访问可能，权衡了exploration和exploitation，由于每个动作都可能被无限次访问到，Explorting Starts中的强制随机初始状态就可以去除了。Monte Carlo ES Control 和 On-policy first-visit MC control for \\(\\epsilon \\text{-soft policies}\\) 都属于on-policy算法，其区别于off-policy的本质在于预估 \\(q_{\\pi}(s,a)\\)时是否从同策略\\(\\pi\\)生成的数据来计算。一个比较subtle的例子是著名的Q-Learning，因为根据这个定义，Q-Learning属于off-policy。 \\[ \\begin{align*} &amp;\\textbf{On-policy first-visit MC control (for }\\epsilon \\textbf{-soft policies), estimating } \\pi \\approx \\pi_{*} \\\\ &amp; \\text{Algorithm parameter: small } \\epsilon &gt; 0 \\\\ &amp; \\text{Initialize:} \\\\ &amp; \\quad \\pi \\leftarrow \\text{ an arbitrary } \\epsilon \\text{-soft policy} \\\\ &amp; \\quad Q(s, a) \\in \\mathbb R \\text{, arbitrarily, for all }s \\in \\mathcal{S}, a \\in \\mathcal A(s) \\\\ &amp; \\quad Returns(s, a) \\leftarrow \\text{ an empty list, for all }s \\in \\mathcal{S}, a \\in \\mathcal A(s)\\\\ &amp; \\\\ &amp; \\text{Repeat forever (for episode):}\\\\ &amp; \\quad \\text{Generate an episode following } \\pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\\\ &amp; \\quad G \\leftarrow 0\\\\ &amp; \\quad \\text{Loop for each step of episode, } t = T-1, T-2, ..., 0:\\\\ &amp; \\quad \\quad \\quad G \\leftarrow \\gamma G + R_{t+1}\\\\ &amp; \\quad \\quad \\quad \\text{Unless the pair } S_t, A_t \\text{ appears in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\\\ &amp; \\quad \\quad \\quad \\quad \\text{Append } G \\text { to }Returns(S_t, A_t) \\\\ &amp; \\quad \\quad \\quad \\quad Q(S_t, A_t) \\leftarrow \\operatorname{average}(Returns(S_t, A_t))\\\\ &amp; \\quad \\quad \\quad \\quad A^{*} \\leftarrow \\operatorname{argmax}_a Q(S_t, a)\\\\ &amp; \\quad \\quad \\quad \\quad \\text{For all } a \\in \\mathcal A(S_t):\\\\ &amp; \\quad \\quad \\quad \\quad \\quad \\pi(a|S_t) \\leftarrow \\begin{cases} 1 - \\epsilon + \\epsilon / |\\mathcal A(S_t)| &amp; \\text{ if } a = A^{*}\\\\ \\epsilon / |\\mathcal A(S_t)| &amp; \\text{ if } a \\neq A^{*}\\\\ \\end{cases} \\\\ \\end{align*} \\] 伪代码对应的 Python 实现如下。 {linenos12345678910111213141516171819202122232425262728def mc_control_epsilon_greedy(env: BlackjackEnv, num_episodes, discount_factor=1.0, epsilon=0.1) \\ -&gt; Tuple[ActionValue, Policy]: returns_sum = defaultdict(float) returns_count = defaultdict(float) states = list(product(range(10, 22), range(1, 11), (True, False))) policy = {s: np.ones(env.action_space.n) * 1.0 / env.action_space.n for s in states} Q = defaultdict(lambda: np.zeros(env.action_space.n)) def update_epsilon_greedy_policy(policy: Policy, Q: ActionValue, s: State): policy[s] = np.ones(env.action_space.n, dtype=float) * epsilon / env.action_space.n best_action = np.argmax(Q[s]) policy[s][best_action] += (1.0 - epsilon) for episode_i in range(1, num_episodes + 1): episode_history = gen_stochastic_episode(policy, env) G = 0 for t in range(len(episode_history) - 1, -1, -1): s, a, r = episode_history[t] G = discount_factor * G + r if not any(s_a_r[0] == s and s_a_r[1] == a for s_a_r in episode_history[0: t]): returns_sum[s, a] += G returns_count[s, a] += 1.0 Q[s][a] = returns_sum[s, a] / returns_count[s, a] update_epsilon_greedy_policy(policy, Q, s) return Q, policy","link":"/zh/2020/rl-sutton-blackjack-2/"},{"title":"通过代码学Sutton强化学习：SARSA、Q-Learning和Expected SARSA时序差分算法训练CartPole","text":"这一期我们进入第六章：时序差分学习（Temporal-Difference Learning）。TD Learning本质上是加了bootstrapping的蒙特卡洛（MC），也是model-free的方法，但实践中往往比蒙特卡洛收敛更快。我们选取OpenAI Gym中经典的CartPole环境来讲解TD。更多相关内容，欢迎关注 本公众号 MyEncyclopedia。 CartPole OpenAI 环境 如图所示，小车上放了一根杆，杆会根据物理系统定理因重力而倒下，我们可以控制小车往左或者往右，目的是尽可能地让杆保持树立状态。 CartPole OpenAI Gym CartPole 观察到的状态是四维的float值，分别是车位置，车速度，杆角度和杆角速度。下表为四个维度的值范围。给到小车的动作，即action space，只有两种：0，表示往左推；1，表示往右推。 Min Max Cart Position -4.8 4.8 Cart Velocity -Inf Inf Pole Angle -0.418 rad (-24 deg) 0.418 rad (24 deg) Pole Angular Velocity -Inf Inf 离散化连续状态 从上所知，CartPole step() 函数返回了4维ndarray，类型为float32的连续状态空间。对于传统的tabular方法来说第一步必须离散化状态，目的是可以作为Q table的主键来查找。下面定义的State类型是离散化后的具体类型，另外 Action 类型已经是0和1，不需要做离散化处理。 {linenos12State = Tuple[int, int, int, int]Action = int 离散化处理时需要考虑的一个问题是如何设置每个维度的分桶策略。分桶策略会决定性地影响训练的效果。原则上必须将和action以及reward强相关的维度做细粒度分桶，弱相关或者无关的维度做粗粒度分桶。举个例子，小车位置本身并不能影响Agent采取的下一动作，当给定其他三维状态的前提下，因此我们对小车位置这一维度仅设置一个桶（bucket size=1）。而杆的角度和角速度是决定下一动作的关键因素，因此我们分别设置成6个和12个。 以下是离散化相关代码，四个维度的 buckets=(1, 2, 6, 12)。self.q是action value的查找表，具体类型是shape 为 (1, 2, 6, 12, 2) 的ndarray。 {linenos123456789101112131415161718192021class CartPoleAbstractAgent(metaclass=abc.ABCMeta): def __init__(self, buckets=(1, 2, 6, 12), discount=0.98, lr_min=0.1, epsilon_min=0.1): self.env = gym.make('CartPole-v0') env = self.env # [position, velocity, angle, angular velocity] self.dims_config = [(env.observation_space.low[0], env.observation_space.high[0], 1), (-0.5, 0.5, 1), (env.observation_space.low[2], env.observation_space.high[2], 6), (-math.radians(50) / 1., math.radians(50) / 1., 12)] self.q = np.zeros(buckets + (self.env.action_space.n,)) self.pi = np.zeros_like(self.q) self.pi[:] = 1.0 / env.action_space.n def to_bin_idx(self, val: float, lower: float, upper: float, bucket_num: int) -&gt; int: percent = (val + abs(lower)) / (upper - lower) return min(bucket_num - 1, max(0, int(round((bucket_num - 1) * percent)))) def discretize(self, obs: np.ndarray) -&gt; State: discrete_states = tuple([self.to_bin_idx(obs[d], *self.dims_config[d]) for d in range(len(obs))]) return discrete_states train() 方法串联起来 agent 和 env 交互的流程，包括从 env 得到连续状态转换成离散状态，更新 Agent 的 Q table 甚至 Agent的执行policy，choose_action会根据执行 policy 选取action。 {linenos12345678910111213141516def train(self, num_episodes=2000): for e in range(num_episodes): print(e) s: State = self.discretize(self.env.reset()) self.adjust_learning_rate(e) self.adjust_epsilon(e) done = False while not done: action: Action = self.choose_action(s) obs, reward, done, _ = self.env.step(action) s_next: State = self.discretize(obs) a_next = self.choose_action(s_next) self.update_q(s, action, reward, s_next, a_next) s = s_next choose_action 的默认实现为基于现有 Q table 的 \\(\\epsilon\\)-greedy 策略。 {linenos12345def choose_action(self, state) -&gt; Action: if np.random.random() &lt; self.epsilon: return self.env.action_space.sample() else: return np.argmax(self.q[state]) 抽象出公共的基类代码 CartPoleAbstractAgent 之后，SARSA、Q-Learning和Expected SARSA只需要复写 update_q 抽象方法即可。 {linenos1234class CartPoleAbstractAgent(metaclass=abc.ABCMeta): @abc.abstractmethod def update_q(self, s: State, a: Action, r, s_next: State, a_next: Action): pass TD Learning的精髓 在上一期，本公众号 MyEncyclopedia 的21点游戏的蒙特卡洛On-Policy控制介绍了Monte Carlo方法，知道MC需要在环境中模拟直至最终结局。若记\\(G_t\\)为t步以后的最终return，则 MC online update 版本更新为： \\[ V(S_t) \\leftarrow V(S_t) + \\alpha[G_{t} - V(S_t)] \\] 可以认为 \\(V(S_t)\\) 向着目标为 \\(G_t\\) 更新了一小步。 而TD方法可以只模拟下一步，得到 \\(R_{t+1}\\)，而余下步骤的return，\\(G_t - R_{t+1}\\) 用已有的 \\(V(S_{t+1})\\) 来估计，或者统计上称作bootstrapping。这样 TD 的更新目标值变成 \\(R_{t+1} + \\gamma V(S_{t+1})\\)，整体online update 公式则为： \\[ V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1})- V(S_t)] \\] 概念上，如果只使用下一步 \\(R_{t+1}\\) 值然后bootstrap称为 TD(0)，用于区分使用多步后的reward的TD方法。另外，变化的数值 \\(R_{t+1} + \\gamma V(S_{t+1})- V(S_t)\\) 称为TD error。 另外一个和Monte Carlo的区别在于一般TD方法保存更精细的Q值，\\(Q(S_t, A_t)\\)，并用Q值来boostrap，而MC一般用V值也可用Q值。 SARSA: On-policy TD 控制 SARSA的命名源于一次迭代产生了五元组 \\(S_t，A_t，R_{t+1}，S_{t+1}，A_{t+1}\\)。SARSA利用五个值做 action-value的 online update： \\[ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)] \\] 对应的Q table更新实现为： {linenos1234class SarsaAgent(CartPoleAbstractAgent): def update_q(self, s: State, a: Action, r, s_next: State, a_next: Action): self.q[s][a] += self.lr * (r + self.discount * (self.q[s_next][a_next]) - self.q[s][a]) SARSA 在执行policy 后的Q值更新是对于针对于同一个policy的，完成了一次策略迭代（policy iteration），这个特点区分于后面的Q-learning算法，这也是SARSA 被称为 On-policy 的原因。下面是完整算法伪代码。 \\[ \\begin{align*} &amp;\\textbf{Sarsa (on-policy TD Control) for estimating } Q \\approx q_{*} \\\\ &amp; \\text{Algorithm parameters: step size }\\alpha \\in ({0,1}]\\text{, small }\\epsilon &gt; 0 \\\\ &amp; \\text{Initialize }Q(s,a), \\text{for all } s \\in \\mathcal{S}^{+}, a \\in \\mathcal{A}(s) \\text{, arbitrarily except that } Q(terminal, \\cdot) = 0 \\\\ &amp; \\text{Loop for each episode:}\\\\ &amp; \\quad \\text{Initialize }S\\\\ &amp; \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\ &amp; \\quad \\text{Loop for each step of episode:} \\\\ &amp; \\quad \\quad \\text{Take action }A, \\text { observe } R, S^{\\prime} \\\\ &amp; \\quad \\quad \\text{Choose }A^{\\prime} \\text { from } S^{\\prime} \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\ &amp; \\quad \\quad Q(S,A) \\leftarrow Q(S,A) + \\alpha[R+\\gamma Q(S^{\\prime}, A^{\\prime}) - Q(S,A)] \\\\ &amp; \\quad \\quad S \\leftarrow S^{\\prime}; A \\leftarrow A^{\\prime} \\\\ &amp; \\quad \\text{until }S\\text{ is terminal} \\\\ \\end{align*} \\] SARSA 训练分析 SARSA收敛较慢，1000次episode后还无法持久稳定，后面的Q-learning 和 Expected Sarsa 都可以在1000次episode学习长时间保持不倒的状态。 Q-Learning: Off-policy TD 控制 Q-Learning 是深度学习时代前强化学习领域中的著名算法，它的 online update 公式为： \\[ Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma \\max_{a}Q(S_{t+1}, a) - Q(S_t,A_t)] \\] 对应的 update_q() 方法具体实现 {linenos1234class QLearningAgent(CartPoleAbstractAgent): def update_q(self, s: State, a: Action, r, s_next: State, a_next: Action): self.q[s][a] += self.lr * (r + self.discount * np.max(self.q[s_next]) - self.q[s][a]) 本质上用现有的Q table中最好的action来bootrap 对应的最佳Q值，推导如下： \\[ \\begin{aligned} q_{*}(s, a) &amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma \\max _{a^{\\prime}} q_{*}\\left(S_{t+1}, a^{\\prime}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\mathbb{E}[R \\mid S_{t}=s, A_{t}=a] + \\gamma\\sum_{s^{\\prime}} p\\left(s^{\\prime}\\mid s, a\\right)\\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right) \\\\ &amp;\\approx r + \\gamma \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right) \\end{aligned} \\] Q-Learning 被称为 off-policy 的原因是它并没有完成一次policy iteration，而是直接用已有的 Q 来不断近似 \\(Q_{*}\\)。 对比下面的Q-Learning 伪代码和之前的 SARSA 版本可以发现，Q-Learning少了一次模拟后的 \\(A_{t+1}\\)，这也是Q-Learning 中执行policy和预估Q值（即off-policy）分离的一个特征。 \\[ \\begin{align*} &amp;\\textbf{Q-learning (off-policy TD Control) for estimating } \\pi \\approx \\pi_{*} \\\\ &amp; \\text{Algorithm parameters: step size }\\alpha \\in ({0,1}]\\text{, small }\\epsilon &gt; 0 \\\\ &amp; \\text{Initialize }Q(s,a), \\text{for all } s \\in \\mathcal{S}^{+}, a \\in \\mathcal{A}(s) \\text{, arbitrarily except that } Q(terminal, \\cdot) = 0 \\\\ &amp; \\text{Loop for each episode:}\\\\ &amp; \\quad \\text{Initialize }S\\\\ &amp; \\quad \\text{Loop for each step of episode:} \\\\ &amp; \\quad \\quad \\text{Choose } A \\text{ from } S \\text{ using policy derived from } Q \\text{ (e.g., } \\epsilon\\text{-greedy)} \\\\ &amp; \\quad \\quad \\text{Take action }A, \\text { observe } R, S^{\\prime} \\\\ &amp; \\quad \\quad Q(S,A) \\leftarrow Q(S,A) + \\alpha[R+\\gamma \\max_{a}Q(S^{\\prime}, a) - Q(S,A)] \\\\ &amp; \\quad \\quad S \\leftarrow S^{\\prime}\\\\ &amp; \\quad \\text{until }S\\text{ is terminal} \\\\ \\end{align*} \\] Q-Learning 训练分析 Q-Learning 1000次episode就可以持久稳定住。 SARSA 改进版 Expected SARSA Expected SARSA 改进了 SARSA 的地方在于考虑到了在某一状态下的现有策略动作分布，以此来减少variance，加快收敛，具体更新规则为： \\[ \\begin{aligned} Q(S_t,A_t) &amp;\\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma \\mathbb{E}_{\\pi}[Q(S_{t+1}, A_{t+1} \\mid S_{t+1})] - Q(S_t,A_t)] \\\\ &amp;\\leftarrow Q(S_t,A_t) + \\alpha[R_{t+1}+\\gamma \\sum_{a} \\pi\\left(a\\mid S_{t+1}\\right) Q(S_{t+1}, a) - Q(S_t,A_t)] \\\\ \\end{aligned} \\] 注意在实现中，update_q() 不仅更新了Q table，还显示更新了执行policy \\(\\pi\\)。 {linenos123456789class ExpectedSarsaAgent(CartPoleAbstractAgent): def update_q(self, s: State, a: Action, r, s_next: State, a_next: Action): self.q[s][a] = self.q[s][a] + self.lr * (r + self.discount * np.dot(self.pi[s_next], self.q[s_next]) - self.q[s][a]) # update pi[s] best_a = np.random.choice(np.where(self.q[s] == max(self.q[s]))[0]) n_actions = self.env.action_space.n self.pi[s][:] = self.epsilon / n_actions self.pi[s][best_a] = 1 - (n_actions - 1) * (self.epsilon / n_actions) 同样的，Expected SARSA 1000次迭代也能比较好的学到最佳policy。","link":"/zh/2020/rl-sutton-cartpole-sarsa-qlearning/"},{"title":"通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法","text":"经典教材Reinforcement Learning: An Introduction 第二版由强化领域权威Richard S. Sutton 和 Andrew G. Barto 完成编写，内容深入浅出，非常适合初学者。在本篇中，引入Grid World示例，结合强化学习核心概念，并用python代码实现OpenAI Gym的模拟环境，进一步实现策略评价算法。 Grid World 问题 第四章例子4.1提出了一个简单的离散空间状态问题：Grid World，其大致意思是在4x4的网格世界中有14个格子是非终点状态，在这些非终点状态的格子中可以往上下左右四个方向走，直至走到两个终点状态格子，则游戏结束。每走一步，Agent收获reward -1，表示Agent希望在Grid World中尽早出去。另外，Agent在Grid World边缘时，无法继续往外只能呆在原地，reward也是-1。 Finite MDP 模型 先来回顾一下强化学习的建模基础：有限马尔可夫决策过程（Finite Markov Decision Process, Finite MDP）。如下图，强化学习模型将世界抽象成两个实体，强化学习解决目标的主体Agent和其他外部环境。它们之间的交互过程遵从有限马尔可夫决策过程：若Agent在t时间步骤时处于状态 \\(S_t\\)，采取动作 \\(A_t\\)，然后环境根据自身机制，产生Reward \\(R_{t+1}\\) 并将Agent状态变为 \\(S_{t+1}\\)。 环境自身机制又称为dynamics，工程上可以看成一个输入(S, A)，输出(S, R)的方法。由于MDP包含随机过程，某个输入并不能确定唯一输出，而会根据概率分布输出不同的(S, R)。Finite MDP简化了时间对于模型的影响，因为(S, R)只和(S, A)有关，不和时间t有关。另外，有限指的是S，A，R的状态数量是有限的。 数学上dynamics可以如下表示 \\[ p\\left(s^{\\prime}, r \\mid s, a\\right) \\doteq \\operatorname{Pr}\\left\\{S_{t}=s^{\\prime}, R_{t}=r \\mid S_{t-1}=s, A_{t-1}=a\\right\\} \\] 即是四元组作为输入的概率函数 \\(p: S \\times R \\times S \\times A \\rightarrow [0, 1]\\)。 满足 \\[ \\sum_{s^{\\prime} \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p\\left(s^{\\prime}, r \\mid s, a\\right)=1, \\text { for all } s \\in \\mathcal{S}, a \\in \\mathcal{A}(s) \\] 以Grid World为例，当Agent处于编号1的网格时，可以往四个方向走，往任意方向走都只产生一种 S, R，因为这个简单的游戏是确定性的，不存在某一动作导致stochastic状态。例如，在1号网格往左就到了终点网格（编号0），得到Reward -1这个规则可以如下表示 \\[ p\\left(s^{\\prime}=0, r=-1 \\mid s=1, a=\\text{L}\\right) = 1 \\] 因此，状态s=1的所有dynamics概率映射为 \\[ \\begin{aligned} p\\left(s^{\\prime}=0, r=-1 \\mid s=1, a=\\text{L}\\right) &amp;=&amp; 1 \\\\ p\\left(s^{\\prime}=2, r=-1 \\mid s=1, a=\\text{R}\\right) &amp;=&amp; 1 \\\\ p\\left(s^{\\prime}=1, r=-1 \\mid s=1, a=\\text{U}\\right) &amp;=&amp; 1 \\\\ p\\left(s^{\\prime}=5, r=-1 \\mid s=1, a=\\text{D}\\right) &amp;=&amp; 1 \\end{aligned} \\] 强化学习的目的 在给定了问题以及定义了强化学习的模型之后，强化学习的目的当然是通过学习让Agent能够学到最佳策略\\(\\pi_{*}\\)，也就是在某个状态下的行动分布，记成 \\(\\pi(a|s)\\)。对应在数值上的优化目标是Agent在一系列过程中采取某种策略的reward总和的期望（Expected Return）。下面公式定义了t步往后的reward总和，其中 \\(\\gamma\\) 为discount factor，用于权衡短期和长期reward对于当前Agent的效用影响。等式最后一步的意义是t步后的reward总和等价于t步所获的立即reward \\(R_{t+1}\\)，加上t+1步后的reward总和 \\(\\gamma G_{t+1}\\)。 \\[ \\begin{aligned} G_{t} &amp; \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\gamma^{3} R_{t+4}+\\cdots \\\\ &amp;=R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\gamma^{2} R_{t+4}+\\cdots\\right) \\\\ &amp;=R_{t+1}+\\gamma G_{t+1} \\end{aligned} \\] 有了reward总和的定义，评价Agent策略 \\(\\pi\\) 就可以定义成Agent在状态 s 时采用此策略的Expected Return。 \\[ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\] 下面公式推导了 \\(v_{\\pi}(s)\\) 数值上和相关状态 \\(s{\\prime}\\) 的关系： \\[ \\begin{aligned} v_{\\pi}(s) &amp;\\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] \\\\ &amp;=\\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} \\mid S_{t}=s\\right]\\\\ &amp;=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s\\right] \\\\ &amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}} \\sum_{r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\mathbb{E}_{\\pi}\\left[G_{t+1} \\mid S_{t+1}=s^{\\prime}\\right]\\right] \\\\ &amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\quad \\text { for all } s \\in \\mathcal{S} \\end{aligned} \\] 注意到如果将 \\(v_{\\pi}(s)\\) 看成未知数，上式即形成 \\(\\mid \\mathcal{S} \\mid\\) 个未知变量的方程组，可以在数值上解得各个 \\(v_{\\pi}(s)\\)。 书中用Backup Diagram来表示递推关系，下图是\\(v_{\\pi}(s)\\)的backup diagram。 尽管v值可以来衡量策略，但由于\\(v_{\\pi}(s)\\) 是Agent在策略\\(\\pi(a|s)\\)的Expected Return，将不同的action拆出来单独计算Expected Return，这样的做法有时更为直接，这就是著名的Q Learning中的q 值，记成\\(q_{\\pi}(s, a)\\) 。 \\[ q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right] \\] 下面是 $q_{}(s, a) $ 的递推 backup diagram。 Bellman 最佳原则 对于所有状态集合\\(\\mathcal{S}\\)，策略\\({\\pi}\\)的评价指标 \\(v_{\\pi}(s)\\) 是一个向量，本质上是无法相互比较的。但由于存在Bellman 最佳原则（Bellman's principle of optimality）：在有限状态情况下，一定存在一个或者多个最好的策略 \\({\\pi}_{*}\\)，它在所有状态下的v值都是最好的，即 \\(v_{\\pi_{*}}(s) \\ge v_{\\pi^{\\prime}}(s) \\text { for all } s \\in \\mathcal{S}\\)。 因此，最佳v值定义为最佳策略 \\({\\pi}_{*}\\) 对应的 v 值 \\[ v_{*}(s) \\doteq \\max_{\\pi} v_{\\pi}(s) \\] 同理，也存在最佳q值，记为 \\[ \\begin{aligned} q_{*}(s, a) &amp;\\doteq \\max_{\\pi} q_{\\pi}(s,a) \\end{aligned} \\] 将 \\(v_{*}(s)\\) 改写成递推形式，称为 Bellman Optimality Equation，推导如下 \\[ \\begin{aligned} v_{*}(s) &amp;=\\max _{a \\in \\mathcal{A}(s)} q_{\\pi_{*}}(s, a) \\\\ &amp;=\\max _{a} \\mathbb{E}_{\\pi_{*}}\\left[G_{t} \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max _{a} \\mathbb{E}_{\\pi_{*}}\\left[R_{t+1}+\\gamma G_{t+1} \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{*}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\] 直觉上可以理解为状态 s 对应的最佳v值是只采取此状态下的最佳动作后的Expected Return。 最佳q值递归形式的意义为最佳策略下状态s时采取行动 a 的Expected Return，等于所有可能后续状态 s' 下采取最优行动的Expected Return的均值。推导如下： \\[ \\begin{aligned} q_{*}(s, a) &amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma \\max _{a^{\\prime}} q_{*}\\left(S_{t+1}, a^{\\prime}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\end{aligned} \\] \\(v_{*}(s), q_{*}(s, a)\\) 的backup diagram 如下图 Grid World 最佳策略和V值 Grid World 的最佳策略如下：尽可能快的走出去 Grid World最佳策略 上面的2D图中不同颜色表示不同V值，终点格子的红色表示0，隔着一步的黄色为-1，隔两步的绿色为-2，最远的紫色为-3。下面是立体图示。 Grid World最佳策略V值 Grid World OpenAI Gym 环境 下面是OpenAI Gym框架下Grid World环境的代码实现。本质是在GridWorldEnv构造函数中构建MDP，类型定义如下 123456MDP = Dict[State, Dict[Action, List[Tuple[Prob, State, Reward, bool]]]]# P[state][action] = [# (prob1, next_state1, reward1, is_done),# (prob2, next_state2, reward2, is_done), ...] {linenos12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class Action(Enum): UP = 0 DOWN = 1 LEFT = 2 RIGHT = 3State = intReward = floatProb = floatPolicy = Dict[State, Dict[Action, Prob]]Value = List[float]StateSet = Set[int]NonTerminalStateSet = Set[int]MDP = Dict[State, Dict[Action, List[Tuple[Prob, State, Reward, bool]]]]# P[s][a] = [(prob, next_state, reward, is_done), ...]class GridWorldEnv(discrete.DiscreteEnv): \"\"\" Grid World environment described in Sutton and Barto Reinforcement Learning 2nd, chapter 4. \"\"\" def __init__(self, shape=[4,4]): self.shape = shape nS = np.prod(shape) nA = len(list(Action)) MAX_R = shape[0] MAX_C = shape[1] self.grid = np.arange(nS).reshape(shape) isd = np.ones(nS) / nS # P[s][a] = [(prob, next_state, reward, is_done), ...] P: MDP = {} action_delta = {Action.UP: (-1, 0), Action.DOWN: (1, 0), Action.LEFT: (0, -1), Action.RIGHT: (0, 1)} for s in range(0, MAX_R * MAX_C): P[s] = {a.value : [] for a in list(Action)} is_terminal = self.is_terminal(s) if is_terminal: for a in list(Action): P[s][a.value] = [(1.0, s, 0, True)] else: r = s // MAX_R c = s % MAX_R for a in list(Action): neighbor_r = min(MAX_R-1, max(0, r + action_delta[a][0])) neighbor_c = min(MAX_C-1, max(0, c + action_delta[a][1])) s_ = neighbor_r * MAX_R + neighbor_c P[s][a.value] = [(1.0, s_, -1, False)] super(GridWorldEnv, self).__init__(nS, nA, P, isd) 策略评估（Policy Evaluation） 策略评估需要解决在给定环境dynamics和Agent策略 \\(\\pi\\)下，计算策略的v值 \\(v_{\\pi}\\)。由于所有数量关系都已知，可以通过解方程组的方式求得，但通常会通过数值迭代的方式来计算，即通过一系列 \\(v_{0}, v_{1}, ..., v_{k}\\) 收敛至 \\(v_{\\pi}\\)。如下迭代方式已经得到证明，当 \\(k \\rightarrow \\infty\\) 一定收敛至 \\(v_{\\pi}\\)。 \\[ \\begin{aligned} v_{k+1}(s) &amp; \\doteq \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) \\mid S_{t}=s\\right] \\\\ &amp;=\\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\] 书中具体伪代码如下 \\[ \\begin{align*} &amp;\\textbf{Iterative Policy Evaluation, for estimating } V\\approx v_{\\pi} \\\\ &amp; \\text{Input } {\\pi}, \\text{the policy to be evaluated} \\\\ &amp; \\text{Algorithm parameter: a small threshold } \\theta &gt; 0 \\text{ determining accuracy of estimation} \\\\ &amp; \\text{Initialize } V(s), \\text{for all } s \\in \\mathcal{S}^{+} \\text{, arbitrarily except that } V (terminal) = 0\\\\ &amp; \\\\ &amp;1: \\text{Loop:}\\\\ &amp;2: \\quad \\quad \\Delta \\leftarrow 0\\\\ &amp;3: \\quad \\quad \\text{Loop for each } s \\in \\mathcal{S}:\\\\ &amp;4: \\quad \\quad \\quad \\quad v \\leftarrow V(s) \\\\ &amp;5: \\quad \\quad \\quad \\quad V(s) \\leftarrow \\sum_{a} \\pi(a \\mid s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right] \\\\ &amp;6: \\quad \\quad \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) \\\\ &amp;7: \\text{until } \\Delta &lt; \\theta \\end{align*} \\] 下面是python 代码实现，注意这里单run迭代时，新的v值直接覆盖数组里的旧v值，这种做法在书中被证明不仅有效，甚至更为高效。这种做法称为原地（in place）更新。 {linenos1234567891011121314def policy_evaluate(policy: Policy, env: GridWorldEnv, gamma=1.0, theta=0.0001): V = np.zeros(env.nS) while True: delta = 0 for s in range(env.nS): v = 0 for a, action_prob in enumerate(policy[s]): for prob, next_state, reward, done in env.P[s][a]: v += action_prob * prob * (reward + gamma * V[next_state]) delta = max(delta, np.abs(v - V[s])) V[s] = v if delta &lt; theta: break return np.array(V) 输入策略为随机选择方向，运行上面的policy_evaluate最终多轮收敛后的V值输出为 {linenos1234[[ 0. -13.99931242 -19.99901152 -21.99891199] [-13.99931242 -17.99915625 -19.99908389 -19.99909436] [-19.99901152 -19.99908389 -17.99922697 -13.99942284] [-21.99891199 -19.99909436 -13.99942284 0. ]] 在3D V值图中可以发现，由于是随机选择方向的策略， Agent在每个格子的V值绝对数值要比最佳V值大，意味着随机策略下Agent在Grid World会得到更多的负reward。 Grid World随机策略V值","link":"/zh/2020/rl-sutton-gridworld-1/"},{"title":"通过代码学Sutton强化学习2：Grid World 策略迭代和值迭代","text":"上一期 通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法，我们引入了 Grid World 问题，实现了对应的OpenAI Gym 环境，也分析了其最佳策略和对应的V值。这一期中，继续通过这个例子详细讲解策略提升（Policy Improvment）、策略迭代（Policy Iteration）、值迭代（Value Iteration）和异步迭代方法。 回顾 Grid World 问题 Grid World 问题 在Grid World 中，Agent初始可以出现在编号1-14的网格中，Agent 每往四周走一步得到 -1 reward，因此需要尽快走到两个出口。当然最佳策略是以最小步数往出口逃离，如下所示。 Grid World 最佳策略 最佳策略对应的状态V值和3D heatmap如下 1234[[ 0. -1. -2. -3.] [-1. -2. -3. -2.] [-2. -3. -2. -1.] [-3. -2. -1. 0.]] Grid World V值 3D heatmap 策略迭代 上一篇中，我们知道如何evaluate 给定policy \\(\\pi\\) 的 \\(v_{\\pi}\\)值，那么是否可能在此基础上改进生成更好的策略 \\(\\pi^{\\prime}\\)。如果可以，能否最终找到最佳策略\\({\\pi}_{*}\\)？答案是肯定的，因为存在策略提升定理（Policy Improvement Theorem）。 策略提升定理 在 4.2 节 Policy Improvement Theorem 可以证明，利用 \\(v_{\\pi}\\) 信息对于每个状态采取最 greedy 的 action （又称exploitation）能够保证生成的新 \\({\\pi}^{\\prime}\\) 是不差于旧的policy \\({\\pi}\\)，即 \\[ q_{\\pi}(s, {\\pi}^{\\prime}(s)) \\gt v_{\\pi}(s) \\] \\[ v_{\\pi^{\\prime}}(s) \\gt v_{\\pi}(s) \\] 因此，可以通过在当前policy求得v值，再选取最greedy action的方式形成如下迭代，就能够不断逼近最佳策略。 \\[ \\pi_{0} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{1} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{2} \\stackrel{\\mathrm{E}}{\\longrightarrow} \\cdots \\stackrel{\\mathrm{I}}{\\longrightarrow} \\pi_{*} \\stackrel{\\mathrm{E}}{\\longrightarrow} v_{*} \\] 策略迭代算法 以下为书中4.3的policy iteration伪代码。其中policy evaluation的算法在上一篇中已经实现。Policy improvement 的精髓在于一次遍历所有状态后，通过policy 的最大Q值找到该状态的最佳action，并更新成最新policy，循环直至没有 action 变更。 \\[ \\begin{align*} &amp;\\textbf{Policy Iteration (using iterative policy evaluation) for estimating } \\pi\\approx {\\pi}_{*} \\\\ &amp;1. \\quad \\text{Initialization} \\\\ &amp; \\quad \\quad V(s) \\in \\mathbb R\\text{ and } \\pi(s) \\in \\mathcal A(s) \\text{ arbitrarily for all }s \\in \\mathcal{S} \\\\ &amp; \\\\ &amp;2. \\quad \\text{Policy Evaluation} \\\\ &amp; \\quad \\quad \\text{Loop:}\\\\ &amp; \\quad \\quad \\Delta \\leftarrow 0\\\\ &amp; \\quad \\quad \\text{Loop for each } s \\in \\mathcal{S}:\\\\ &amp; \\quad \\quad \\quad \\quad v \\leftarrow V(s) \\\\ &amp; \\quad \\quad \\quad \\quad V(s) \\leftarrow \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right] \\\\ &amp; \\quad \\quad \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) \\\\ &amp; \\quad \\quad \\text{until } \\Delta &lt; \\theta \\text{ (a small positive number determining the accuracy of estimation)}\\\\ &amp; \\\\ &amp;3. \\quad \\text{Policy Improvement} \\\\ &amp; \\quad \\quad policy\\text{-}stable\\leftarrow true \\\\ &amp; \\quad \\quad \\text{Loop for each } s \\in \\mathcal{S}:\\\\ &amp; \\quad \\quad \\quad \\quad old\\text{-}action\\leftarrow \\pi(s) \\\\ &amp; \\quad \\quad \\quad \\quad \\pi(s) \\leftarrow \\operatorname{argmax}_{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right] \\\\ &amp; \\quad \\quad \\quad \\quad \\text{If } old\\text{-}action \\neq \\pi\\text{,then }policy\\text{-}stable\\leftarrow false \\\\ &amp; \\quad \\quad \\text{If } policy\\text{-}stable \\text{, then stop and return }V \\approx v_{*} \\text{ and } \\pi\\approx {\\pi}_{*}\\text{; else go to 2} \\end{align*} \\] 注意到状态Q值 \\(q_{\\pi}(s, a)\\) 会被多处调用，将其封装为单独的函数。 \\[ \\begin{aligned} q_{\\pi}(s, a) &amp;=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\] Q值函数实现如下： {linenos123456def action_value(env: GridWorldEnv, state: State, V: StateValue, gamma=1.0) -&gt; ActionValue: q = np.zeros(env.nA) for a in range(env.nA): for prob, next_state, reward, done in env.P[state][a]: q[a] += prob * (reward + gamma * V[next_state]) return q 有了 action_value 和上期的 policy_evaluate，policy iteration 实现完全对应上面的伪代码。 {linenos1234567891011121314151617181920212223def policy_improvement(env: GridWorldEnv, policy: Policy, V: StateValue, gamma=1.0) -&gt; bool: policy_stable = True for s in range(env.nS): old_action = np.argmax(policy[s]) Q_s = action_value(env, s, V) best_action = np.argmax(Q_s) policy[s] = np.eye(env.nA)[best_action] if old_action != best_action: policy_stable = False return policy_stabledef policy_iteration(env: GridWorldEnv, policy: Policy, gamma=1.0) -&gt; Tuple[Policy, StateValue]: iter = 0 while True: V = policy_evaluate(policy, env, gamma) policy_stable = policy_improvement(env, policy, V) iter += 1 if policy_stable: return policy, V Grid World 例子通过两轮迭代就可以收敛，以下是初始时随机策略的V值和第一次迭代后的V值。 初始随机策略 V 值 第一次迭代后的 V 值 值迭代 值迭代（ Value Iteration）的本质是，将policy iteration中的policy evaluation过程从不断循环到收敛直至小于theta，改成只执行一遍，并直接用最佳Q值更新到状态V值，如此可以不用显示地算出\\({\\pi}\\) 而直接在V值上迭代。具体迭代公式如下： \\[ \\begin{aligned} v_{k+1}(s) &amp; \\doteq \\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) \\mid S_{t}=s, A_{t}=a\\right] \\\\ &amp;=\\max_{a} q_{\\pi_k}(s, a) \\\\ &amp;=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right] \\end{aligned} \\] 完整的伪代码为： \\[ \\begin{align*} &amp;\\textbf{Value Iteration, for estimating } \\pi\\approx \\pi_{*} \\\\ &amp; \\text{Algorithm parameter: a small threshold } \\theta &gt; 0 \\text{ determining accuracy of estimation} \\\\ &amp; \\text{Initialize } V(s), \\text{for all } s \\in \\mathcal{S}^{+} \\text{, arbitrarily except that } V (terminal) = 0\\\\ &amp; \\\\ &amp;1: \\text{Loop:}\\\\ &amp;2: \\quad \\quad \\Delta \\leftarrow 0\\\\ &amp;3: \\quad \\quad \\text{Loop for each } s \\in \\mathcal{S}:\\\\ &amp;4: \\quad \\quad \\quad \\quad v \\leftarrow V(s) \\\\ &amp;5: \\quad \\quad \\quad \\quad V(s) \\leftarrow \\operatorname{max}_{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right] \\\\ &amp;6: \\quad \\quad \\quad \\quad \\Delta \\leftarrow \\max(\\Delta, |v-V(s)|) \\\\ &amp;7: \\text{until } \\Delta &lt; \\theta \\\\ &amp; \\\\ &amp; \\text{Output a deterministic policy, }\\pi\\approx \\pi_{*} \\text{, such that} \\\\ &amp; \\quad \\quad \\pi(s) \\leftarrow \\operatorname{argmax}_{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r \\mid s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right] \\end{align*} \\] 代码实现也比较直接，可以复用上面已经实现的 action_value 函数。 {linenos12345678910111213141516171819def value_iteration(env:GridWorldEnv, gamma=1.0, theta=0.0001) -&gt; Tuple[Policy, StateValue]: V = np.zeros(env.nS) while True: delta = 0 for s in range(env.nS): action_values = action_value(env, s, V, gamma=gamma) best_action_value = np.max(action_values) delta = max(delta, np.abs(best_action_value - V[s])) V[s] = best_action_value if delta &lt; theta: break policy = np.zeros([env.nS, env.nA]) for s in range(env.nS): action_values = action_value(env, s, V, gamma=gamma) best_action = np.argmax(action_values) policy[s, best_action] = 1.0 return policy, V 异步迭代 在第4.5节中提到了DP迭代方式的改进版：异步方式迭代（Asychronous Iteration）。这里的异步是指每一轮无需全部扫一遍所有状态，而是根据上一轮变化的状态决定下一轮需要最多计算的状态数，类似于Dijkstra最短路径算法中用 heap 来维护更新节点集合，减少运算量。下面我们通过异步值迭代来演示异步迭代的工作方式。 下图表示状态的变化方向，若上一轮 \\(V(s)\\) 发生更新，那么下一轮就要考虑状态 s 可能会影响到上游状态的集合（ p1，p2），避免下一轮必须遍历所有状态的V值计算。 Async 反向传播 要做到部分更新就必须知道每个状态可能影响到的上游状态集合，上图对应的映射关系可以表示为 \\[ \\begin{align*} s'_1 &amp;\\rightarrow \\{s\\} \\\\ s'_2 &amp;\\rightarrow \\{s\\} \\\\ s &amp;\\rightarrow \\{p_1, p_2\\} \\end{align*} \\] 建立映射关系的代码如下，build_reverse_mapping 返回类型为 Dict[State, Set[State]]。 {linenos12345678910111213def build_reverse_mapping(env:GridWorldEnv) -&gt; Dict[State, Set[State]]: MAX_R, MAX_C = env.shape[0], env.shape[1] mapping = {s: set() for s in range(0, MAX_R * MAX_C)} action_delta = {Action.UP: (-1, 0), Action.DOWN: (1, 0), Action.LEFT: (0, -1), Action.RIGHT: (0, 1)} for s in range(0, MAX_R * MAX_C): r = s // MAX_R c = s % MAX_R for a in list(Action): neighbor_r = min(MAX_R - 1, max(0, r + action_delta[a][0])) neighbor_c = min(MAX_C - 1, max(0, c + action_delta[a][1])) s_ = neighbor_r * MAX_R + neighbor_c mapping[s_].add(s) return mapping 有了描述状态依赖的映射 dict 后，代码也比较简洁，changed_state_set 变量保存了这轮必须计算的状态集合。新的一轮迭代时，将下一轮需要计算的状态保存到 changed_state_set_ 中，本轮结束后，changed_state_set 更新成changed_state_set_，开始下一轮循环直至没有状态需要更新。 {linenos1234567891011121314151617181920212223242526def value_iteration_async(env:GridWorldEnv, gamma=1.0, theta=0.0001) -&gt; Tuple[Policy, StateValue]: mapping = build_reverse_mapping(env) V = np.zeros(env.nS) changed_state_set = set(s for s in range(env.nS)) iter = 0 while len(changed_state_set) &gt; 0: changed_state_set_ = set() for s in changed_state_set: action_values = action_value(env, s, V, gamma=gamma) best_action_value = np.max(action_values) v_diff = np.abs(best_action_value - V[s]) if v_diff &gt; theta: changed_state_set_.update(mapping[s]) V[s] = best_action_value changed_state_set = changed_state_set_ iter += 1 policy = np.zeros([env.nS, env.nA]) for s in range(env.nS): action_values = action_value(env, s, V, gamma=gamma) best_action = np.argmax(action_values) policy[s, best_action] = 1.0 return policy, V 比较值迭代和异步值迭代方法后发现，值迭代用了4次循环，每次涉及所有状态，总计算状态数为 4 x 16 = 64。异步值迭代也用了4次循环，但是总计更新了54个状态。由于Grid World 的状态数很少，异步值迭代优势并不明显，但是对于状态数众多并且迭代最终集中在少部分状态的环境下，节省的计算量还是很可观的。","link":"/zh/2020/rl-sutton-gridworld-2/"},{"title":"分享课程 Berkeley Deep Reinforcement Learning Bootcamp 2017","text":"Berkeley 2017年联合了DeepMind 以及 OpenAI 举办了一个大咖云集的深度强化学习训练营，是难得的前沿深度强化学习佳品，本公众号 MyEncyclopedia 用代码实现了权威教材 Sutton &amp; Barto 第二版强化学习的基础部分之后，会大致沿着这个训练营的思路，从原理到代码逐步揭示强化深度学习面纱，并结合各种有意思的游戏环境来演示。 如果没有耐心的同学可以直接跳到文末的百度云盘下载链接，内容涵盖所有视频和slide。 此次训练营主讲的强化学习领域专家包括 Pieter Abbeel，前Berkeley 机器人学习实验室主任，伯克利人工智能研究(BAIR)实验室联合主任 Andrej Karpathy，前 OpenAI研究科学家、现特斯拉AI总监 Vlad Mnih，Deepmind 研究科学家 John Schulman，Deepmind 研究科学家，OpenAI共同创建人 Sergey Levine，Berkeley 计算机副教授 课程列表 Core Lecture 1 Intro to MDPs and Exact Solution Methods -- Pieter Abbeel Core Lecture 2 Sample-based Approximations and Fitted Learning -- Rocky Duan Core Lecture 3 DQN + Variants -- Vlad Mnih Core Lecture 4a Policy Gradients and Actor Critic -- Pieter Abbeel Core Lecture 4b Pong from Pixels -- Andrej Karpathy Core Lecture 5 Natural Policy Gradients, TRPO, and PPO -- John Schulman Core Lecture 6 Nuts and Bolts of Deep RL Experimentation -- John Schulman Core Lecture 7 SVG, DDPG, and Stochastic Computation Graphs -- John Schulman Core Lecture 8 Derivative-free Methods -- Peter Chen Core Lecture 9 Model-based RL -- Chelsea Finn Core Lecture 10a Utilities -- Pieter Abbeel Core Lecture 10b Inverse RL -- Chelsea Finn Frontiers Lecture I: Recent Advances, Frontiers and Future of Deep RL -- Vlad Mnih Frontiers Lecture II: Recent Advances, Frontiers and Future of Deep RL -- Sergey Levine TAs Research Overviews 前两讲总结了强化学习基础理论方面，包括用动态规划求精确解，采样与环境交互的传统基本方法。第三四讲覆盖了主流的深度强化学习的几种模式：DQN，PG和AC。第五到七讲深入了深度强化学习的各种前沿方法。值得一提的是第六讲，很好的从实践中总结了各种调试诊断方法。余下的若干讲涉及到了非主流的剩余强化学习领域。 下载方法 关注 MyEncyclopedia 公众号，输入 rl-bootcamp-ucb-2017 即可获得百度云盘链接","link":"/zh/2020/share-rl-bootcamp-berkeley-2017/"},{"title":"TSP问题从DP算法到深度学习1： 递归DP方法 AC AIZU TSP问题","text":"旅行商问题（TSP）是计算机算法中经典的NP hard 问题。 在本系列文章中，我们将首先使用动态规划 AC aizu中的TSP问题，然后再利用深度学习求大规模下的近似解。深度学习应用解决问题时先以PyTorch实现监督学习算法 Pointer Network，进而结合强化学习来无监督学习，提高数据使用效率。 本系列完整列表如下： 第一篇: 递归DP方法 AC AIZU TSP问题 第二篇: 二维空间TSP数据集及其DP解法 第三篇: 深度学习 Pointer Networks 的 Pytorch实现 第四篇: 搜寻最有可能路径：Viterbi算法和其他 第五篇: 深度强化学习无监督算法的 Pytorch实现 TSP 问题回顾 TSP可以用图模型来表达，无论有向图或无向图，无论全连通图或者部分连通的图都可以作为TSP问题。 Wikipedia TSP 中举了一个无向全连通的TSP例子。如下图所示，四个顶点A，B，C，D构成无向全连通图。TSP问题要求在所有遍历所有点后返回初始点的回路中找到最短的回路。例如，\\(A \\rightarrow B \\rightarrow C \\rightarrow D \\rightarrow A\\) 和 \\(A \\rightarrow C \\rightarrow B \\rightarrow D \\rightarrow A\\) 都是有效的回路，但是TSP需要返回这些回路中的最短回路（注意，最短回路可能会有多条）。 Wikipedia 4个顶点组成的图 无论是哪种类型的图，我们都能用邻接矩阵表示出一个图。上面的Wikipedia中的图可以用下面的矩阵来描述。 \\[ \\begin{matrix} &amp; \\begin{matrix}A&amp;B&amp;C&amp;D\\end{matrix} \\\\\\\\ \\begin{matrix}A\\\\\\\\B\\\\\\\\C\\\\\\\\D\\end{matrix} &amp; \\begin{bmatrix}-&amp;20&amp;42&amp;35\\\\\\\\20&amp;-&amp;30&amp;34\\\\\\\\42&amp;30&amp;-&amp;12\\\\\\\\35&amp;34&amp;12&amp;-\\end{bmatrix}\\\\\\\\ \\end{matrix} \\] 当然，大多数情况下，TSP问题会被限定在欧氏空间，即二维地图中的全连通无向图。因为，如果将顶点表示一个地理位置，一般来说它可以和其他所有顶点连通，回来的距离相同，由此构成无向图。 AIZU TSP 问题 AIZU在线题库 有一道有向不完全连通图的TSP问题。给定V个顶点和E条边，输出最小回路值。例如，题目里的例子如下所示，由4个顶点和6条单向边构成。 这个示例的答案是16，对应的回路是 \\(0\\rightarrow1\\rightarrow3\\rightarrow2\\rightarrow0\\)，由下图的红色边构成。注意，这个题目可能不存在合法解，原因是无回路存在，此时返回-1，可以合理地理解成无穷大。 暴力解法 一种暴力方法是枚举所有可能的从某一顶点的回路，取其中的最小值即可。下面的 Python 示例如何枚举4个顶点构成的图中从顶点0出发的所有回路。 {linenos12345from itertools import permutationsv = [1,2,3]p = permutations(v)for t in list(p): print([0] + list(t) + [0]) 所有从顶点0出发的回路如下： {linenos123456[0, 1, 2, 3, 0][0, 1, 3, 2, 0][0, 2, 1, 3, 0][0, 2, 3, 1, 0][0, 3, 1, 2, 0][0, 3, 2, 1, 0] 很显然，这种方式的时间复杂度是 O(\\(n!\\))，无法通过AIZU。 动态规划求解 我们可以使用位状态压缩的动态规划来AC这道题。 首先，需要将回路过程中的状态编码成二进制的表示。例如，在四顶点的例子中，如果顶点2和1都被访问过，并且此时停留在顶点1。将已经访问的顶点对应的位置1，那么编码成0110，此外，还需要保存当前顶点的位置，因此我们将代表状态的数组扩展成二维，第一维是位状态，第二维是顶点所在位置，即 \\(dp[bitstate][v]\\)。这个例子的状态表示就是 \\(dp[\"0110\"][1]\\)。 状态转移方程如下： \\[ dp[bitstate][v] = \\min ( dp[bitstate \\cup \\{u\\}][u] + dist(v,u) \\mid u \\notin bitstate ) \\] 这种方法对应的时间复杂度是 O(\\(n^2*2^n\\) )，因为总共有 \\(2^n * n\\) 个状态，而每个状态又需要一次遍历。虽然都是指数级复杂度，但是它们的巨大区别由下面可以看出区别。 \\(n!\\) \\(n^2*2^n\\) n=8 40320 16384 n=10 3628800 102400 n=12 479001600 589824 n=14 87178291200 3211264 暂停思考一下为什么状态压缩DP能工作。注意到之前暴力解法中其实是有很多重复计算，下面红圈表示重复的计算节点。 在本篇中，我们将会用Python 3和Java 8 实现自顶向下的DP 缓存版本。这种方式比较符合直觉，因为我们不需要预先考虑计算节点的依赖关系。在Java中我们使用了一个小技巧，dp数组初始化成Integer.MAX_VALUE，如此只需要一条语句就能完成更新dp值。 1res = Math.min(res, s + g.edges[v][u]); 当然，为了AC 这道题，我们需要区分出真正无法到达的情况并返回-1。 在Python实现中，也可以使用同样的技巧，但是这次示例一般的实现方法：将dp数组初始化成-1并通过 if-else 来区分不同情况。 1234567INT_INF = -1if s != INT_INF and edges[v][u] != INT_INF: if ret == INT_INF: ret = s + edges[v][u] else: ret = min(ret, s + edges[v][u]) 下面附完整的Python 3和Java 8的AC代码，同步在 github。 AIZU Java 8 递归DP版本 {linenos12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// passed http://judge.u-aizu.ac.jp/onlinejudge/description.jsp?id=DPL_2_Aimport java.util.Arrays;import java.util.Scanner;public class Main { public static class Graph { public final int V_NUM; public final int[][] edges; public Graph(int V_NUM) { this.V_NUM = V_NUM; this.edges = new int[V_NUM][V_NUM]; for (int i = 0; i &lt; V_NUM; i++) { Arrays.fill(this.edges[i], Integer.MAX_VALUE); } } public void setDist(int src, int dest, int dist) { this.edges[src][dest] = dist; } } public static class TSP { public final Graph g; long[][] dp; public TSP(Graph g) { this.g = g; } public long solve() { int N = g.V_NUM; dp = new long[1 &lt;&lt; N][N]; for (int i = 0; i &lt; dp.length; i++) { Arrays.fill(dp[i], -1); } long ret = recurse(0, 0); return ret == Integer.MAX_VALUE ? -1 : ret; } private long recurse(int state, int v) { int ALL = (1 &lt;&lt; g.V_NUM) - 1; if (dp[state][v] &gt;= 0) { return dp[state][v]; } if (state == ALL &amp;&amp; v == 0) { dp[state][v] = 0; return 0; } long res = Integer.MAX_VALUE; for (int u = 0; u &lt; g.V_NUM; u++) { if ((state &amp; (1 &lt;&lt; u)) == 0) { long s = recurse(state | 1 &lt;&lt; u, u); res = Math.min(res, s + g.edges[v][u]); } } dp[state][v] = res; return res; } } public static void main(String[] args) { Scanner in = new Scanner(System.in); int V = in.nextInt(); int E = in.nextInt(); Graph g = new Graph(V); while (E &gt; 0) { int src = in.nextInt(); int dest = in.nextInt(); int dist = in.nextInt(); g.setDist(src, dest, dist); E--; } System.out.println(new TSP(g).solve()); }} AIZU Python 3 递归DP版本 {linenos1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071from typing import ListINT_INF = -1class Graph: v_num: int edges: List[List[int]] def __init__(self, v_num: int): self.v_num = v_num self.edges = [[INT_INF for c in range(v_num)] for r in range(v_num)] def setDist(self, src: int, dest: int, dist: int): self.edges[src][dest] = distclass TSPSolver: g: Graph dp: List[List[int]] def __init__(self, g: Graph): self.g = g self.dp = [[None for c in range(g.v_num)] for r in range(1 &lt;&lt; g.v_num)] def solve(self) -&gt; int: return self._recurse(0, 0) def _recurse(self, v: int, state: int) -&gt; int: \"\"\" :param v: :param state: :return: -1 means INF \"\"\" dp = self.dp edges = self.g.edges if dp[state][v] is not None: return dp[state][v] if (state == (1 &lt;&lt; self.g.v_num) - 1) and (v == 0): dp[state][v] = 0 return dp[state][v] ret: int = INT_INF for u in range(self.g.v_num): if (state &amp; (1 &lt;&lt; u)) == 0: s: int = self._recurse(u, state | 1 &lt;&lt; u) if s != INT_INF and edges[v][u] != INT_INF: if ret == INT_INF: ret = s + edges[v][u] else: ret = min(ret, s + edges[v][u]) dp[state][v] = ret return retdef main(): V, E = map(int, input().split()) g: Graph = Graph(V) for _ in range(E): src, dest, dist = map(int, input().split()) g.setDist(src, dest, dist) tsp: TSPSolver = TSPSolver(g) print(tsp.solve())if __name__ == \"__main__\": main()","link":"/zh/2020/tsp-1-dp-alg/"},{"title":"TSP问题从DP算法到深度学习2：欧氏空间数据集的DP解","text":"本篇是TSP问题从DP算法到深度学习系列第二篇。 第一篇: 递归DP方法 AC AIZU TSP问题 第二篇: 二维空间TSP数据集及其DP解法 第三篇: 深度学习 Pointer Networks 的 Pytorch实现 第四篇: 搜寻最有可能路径：Viterbi算法和其他 第五篇: 深度强化学习无监督算法的 Pytorch实现 AIZU TSP 自底向上迭代DP解 上一篇中，我们用Python 3和Java 8完成了自顶向下递归版本的DP解。我们继续改进代码，将它转换成标准DP方式：自底向上的迭代DP版本。下图是3个点TSP问题的递归调用图。 将这个图反过来检查状态的依赖关系，可以很容易发现规律：首先计算状态位含有一个1的点，接着是两个1的节点，最后是状态位三个1的点。简而言之，在计算状态位为n+1个1的节点时需要用到n个1的节点的计算结果，如果能依照这样的 topological 顺序来的话，就可以去除递归，写成迭代（循环）版本的DP。 迭代算法的Java 伪代码如下 12345678910111213141516for (int bitset_num = N; bitset_num &gt;=0; bitset_num++) { while(hasNextCombination(bitset_num)) { int state = nextCombination(bitset_num); // compute dp[state][v], v-th bit is set in state for (int v = 0; v &lt; n; v++) { for (int u = 0; u &lt; n; u++) { // for each u not reached by this state if (!include(state, u)) { dp[state][v] = min(dp[state][v], dp[new_state_include_u][u] + dist[v][u]); } } } }} 举例来说，dp[00010][1] 是从顶点0出发，刚经过顶点1的最小距离 \\(0 \\rightarrow 1 \\rightarrow ? \\rightarrow ? \\rightarrow ? \\rightarrow 0\\)。 为了找到最小距离值，就必须遍历所有可能的下一个可能的顶点u （第一个问号位置）。 \\[ (0 \\rightarrow 1) + \\begin{align*} \\min \\left\\lbrace \\begin{array}{r@{}l} 2 \\rightarrow ? \\rightarrow ? \\rightarrow 0 + dist(1,2) \\qquad\\text{ new_state=[00110][2] } \\qquad\\\\\\\\ 3 \\rightarrow ? \\rightarrow ? \\rightarrow 0 + dist(1,3) \\qquad\\text{ new_state=[01010][3] } \\qquad\\\\\\\\ 4 \\rightarrow ? \\rightarrow ? \\rightarrow 0 + dist(1,4) \\qquad\\text{ new_state=[10010][4] } \\qquad \\end{array} \\right. \\end{align*} \\] 迭代DP AC代码 以下是AC 的Java 算法核心代码。完整代码在 github/MyEncyclopedia 的tsp/alg_aizu/Main_loop.java。 {linenos1234567891011121314151617181920public long solve() { int N = g.V_NUM; long[][] dp = new long[1 &lt;&lt; N][N]; // init dp[][] with MAX for (int i = 0; i &lt; dp.length; i++) { Arrays.fill(dp[i], Integer.MAX_VALUE); } dp[(1 &lt;&lt; N) - 1][0] = 0; for (int state = (1 &lt;&lt; N) - 2; state &gt;= 0; state--) { for (int v = 0; v &lt; N; v++) { for (int u = 0; u &lt; N; u++) { if (((state &gt;&gt; u) &amp; 1) == 0) { dp[state][v] = Math.min(dp[state][v], dp[state | 1 &lt;&lt; u][u] + g.edges[v][u]); } } } } return dp[0][0] == Integer.MAX_VALUE ? -1 : dp[0][0];} 很显然，时间算法复杂度对应了三重 for 循环，为 O(\\(2^n * n * n\\)) = O(\\(2^n*n^2\\) )。 类似的，Python 3 AC 代码如下。完整代码在 github/MyEncyclopedia 的tsp/alg_aizu/TSP_loop.py。 {linenos123456789101112131415161718192021222324252627class TSPSolver: g: Graph def __init__(self, g: Graph): self.g = g def solve(self) -&gt; int: \"\"\" :param v: :param state: :return: -1 means INF \"\"\" N = self.g.v_num dp = [[INT_INF for c in range(N)] for r in range(1 &lt;&lt; N)] dp[(1 &lt;&lt; N) - 1][0] = 0 for state in range((1 &lt;&lt; N) - 2, -1, -1): for v in range(N): for u in range(N): if ((state &gt;&gt; u) &amp; 1) == 0: if dp[state | 1 &lt;&lt; u][u] != INT_INF and self.g.edges[v][u] != INT_INF: if dp[state][v] == INT_INF: dp[state][v] = dp[state | 1 &lt;&lt; u][u] + self.g.edges[v][u] else: dp[state][v] = min(dp[state][v], dp[state | 1 &lt;&lt; u][u] + self.g.edges[v][u]) return dp[0][0] 一个欧式空间TSP数据集 至此，TSP的DP解法全部讲解完毕。接下去，我们引入一个二维欧式空间的TSP数据集 PTR_NET on Google Drive ，这个数据集是 Pointer Networks 的作者 Oriol Vinyals 用于模型的训练测试而引入的。 数据集的每一行格式如下： 1x1, y1, x2, y2, ... output 1 v1 v2 v3 ... 1 一行开始为n个点的x， y坐标，接着是 output，再接着是1，表示从顶点1出发，经v1，v2，...，返回1，注意顶点编号从1开始。 十个顶点数据集的一些数据示例如下： 12340.607122 0.664447 0.953593 0.021519 0.757626 0.921024 0.586376 0.433565 0.786837 0.052959 0.016088 0.581436 0.496714 0.633571 0.227777 0.971433 0.665490 0.074331 0.383556 0.104392 output 1 3 8 6 10 9 5 2 4 7 1 0.930534 0.747036 0.277412 0.938252 0.794592 0.794285 0.961946 0.261223 0.070796 0.384302 0.097035 0.796306 0.452332 0.412415 0.341413 0.566108 0.247172 0.890329 0.429978 0.232970 output 1 3 2 9 6 5 8 7 10 4 1 0.686712 0.087942 0.443054 0.277818 0.494769 0.985289 0.559706 0.861138 0.532884 0.351913 0.712561 0.199273 0.554681 0.657214 0.909986 0.277141 0.931064 0.639287 0.398927 0.406909 output 1 5 2 10 7 4 3 9 8 6 1 画出第一个例子的全部顶点和边。 {linenos123456789101112131415161718import matplotlib.pyplot as pltpoints='0.607122 0.664447 0.953593 0.021519 0.757626 0.921024 0.586376 0.433565 0.786837 0.052959 0.016088 0.581436 0.496714 0.633571 0.227777 0.971433 0.665490 0.074331 0.383556 0.104392'float_list = list(map(lambda x: float(x), points.split(' ')))x,y = [],[]for idx, p in enumerate(float_list): if idx % 2 == 0: x.append(p) else: y.append(p)for i in range(0, len(x)): for j in range(0, len(x)): if i == j: continue plt.plot((x[i],x[j]),(y[i],y[j]))plt.show() 全连接的图 这个例子的最短TSP旅程为 \\[ 1 \\rightarrow 3 \\rightarrow 8 \\rightarrow 6 \\rightarrow 10 \\rightarrow 9 \\rightarrow 5 \\rightarrow 2 \\rightarrow 4 \\rightarrow 7 \\rightarrow 1 \\] {linenos12345678tour_str = '1 3 8 6 10 9 5 2 4 7 1'tour = list(map(lambda x: int(x), tour_str.split(' ')))for i in range(0, len(tour)-1): p1 = tour[i] - 1 p2 = tour[i + 1] - 1 plt.plot((x[p1],x[p2]),(y[p1],y[p2]))plt.show() 最短路径 PTR_NET TSP 的Python代码 初始化Init Graph Edges 在之前的自顶向下的递归版本中，需要做一些改动。首先，是图的初始化，我们依然延续之前的邻接矩阵来表示，由于这次的图是无向图，对于任意两个顶点，需要初始化双向的边。 {linenos12345678g: Graph = Graph(N)for v in range(N): for u in range(N): diff_x = coordinates[v][0] - coordinates[u][0] diff_y = coordinates[v][1] - coordinates[u][1] dist: float = math.sqrt(diff_x * diff_x + diff_y * diff_y) g.setDist(u, v, dist) g.setDist(v, u, dist) 辅助变量记录父节点 另一大改动是需要在遍历过程中保存的顶点关联信息，以便在最终找到最短路径值时可以回溯对应的完整路径。在下面代码中，使用parent[bitstate][v] 来保存此状态下最小路径对应的顶点u。 {linenos1234567891011ret: float = FLOAT_INFu_min: int = -1 for u in range(self.g.v_num): if (state &amp; (1 &lt;&lt; u)) == 0: s: float = self._recurse(u, state | 1 &lt;&lt; u) if s + edges[v][u] &lt; ret: ret = s + edges[v][u] u_min = u dp[state][v] = ret self.parent[state][v] = u_min 当最终最短行程确定后，根据parent的信息可以按图索骥找到完整的行程顶点信息。 {linenos123456789def _form_tour(self): self.tour = [0] bit = 0 v = 0 for _ in range(self.g.v_num - 1): v = self.parent[bit][v] self.tour.append(v) bit = bit | (1 &lt;&lt; v) self.tour.append(0) 需要注意的是，有可能存在多个最短行程，它们的距离值是一致的。这种情况下，代码输出的最短路径可能和数据集output后行程路径不一致，但是的两者的总距离是一致的。下面的代码验证了这一点。 {linenos123456789101112131415161718tsp: TSPSolver = TSPSolver(g)tsp.solve()output_dist: float = 0.0output_tour = list(map(lambda x: int(x) - 1, output.split(' ')))for v in range(1, len(output_tour)): pre_v = output_tour[v-1] curr_v = output_tour[v] diff_x = coordinates[pre_v][0] - coordinates[curr_v][0] diff_y = coordinates[pre_v][1] - coordinates[curr_v][1] dist: float = math.sqrt(diff_x * diff_x + diff_y * diff_y) output_dist += dist passed = abs(tsp.dist - output_dist) &lt; 10e-5 if passed: print(f'passed dist={tsp.tour}') else: print(f'Min Tour Distance = {output_dist}, Computed Tour Distance = {tsp.dist}, Expected Tour = {output_tour}, Result = {tsp.tour}') 本文所有代码在 github/MyEncyclopedia tsp/alg_plane 中。","link":"/zh/2020/tsp-2-dp-tour/"},{"title":"TSP问题从DP算法到深度学习3：Pointer Network","text":"本篇是TSP问题从DP算法到深度学习系列第三篇，在这一篇中，我们会开始进入深度学习领域来求近似解法。本文会介绍并实现指针网络（Pointer Networks），一种seq-to-seq模型，它的设计目的就是为了解决TSP问题或者凸包（Convex Hull）问题。本文代码在 https://github.com/MyEncyclopedia/blog/tree/master/tsp/ptr_net_pytorch 中。 第一篇: 递归DP方法 AC AIZU TSP问题 第二篇: 二维空间TSP数据集及其DP解法 第三篇: 深度学习 Pointer Networks 的 Pytorch实现 第四篇: 搜寻最有可能路径：Viterbi算法和其他 第五篇: 深度强化学习无监督算法的 Pytorch实现 Pointer Networks 随着深度学习 seq-to-seq 模型作为概率近似模型在各领域的成功，TSP问题似乎也可以用同样的思路去解决。然而，传统的seq-to-seq 模型其输出的类别是预先固定的。例如，NLP RNN生成模型每一步会从 \\(|V|\\) 大的词汇表中产生一个单词。 然而，有很大一类问题，譬如TSP问题、凸包（Convex Hull）问题、Delaunay三角剖分问题，输出的类别不是事先固定的，而是随着输入而变化的。 Pointer Networks 的出现解决了这种限制：输出的类别可以通过指向某个输入，以此克服类别的问题，因此形象地取名为指针网络（Pointer Networks）。先来看看原论文中提到的三个问题。 凸包问题（Convex Hull） 如下图所示，需要在给定的10个点中找到若干个点，使得这些点包住了所有点。问题输入是不确定个数 n 个点的位置信息，输出是 k (k&lt;=n)个点的。 这个经典的算法问题已经被证明找出精确解等价于排序问题（wikipedia 链接），因此时间复杂度为 \\(O(n*log(n))\\)。 image info \\[ \\begin{align*} &amp;\\text{Input: } \\mathcal{P} &amp;=&amp; \\left\\{ P_{1}, \\ldots, P_{10} \\right\\} \\\\ &amp;\\text{Output: } C^{\\mathcal{P}} &amp;=&amp; \\{2,4,3,5,6,7,2\\} \\end{align*} \\] TSP 问题 TSP 和凸包问题很类似，输入为不确定个数的 n 个点信息，输出为这 n 个点的某序列。在。。。中，我们可以将确定解的时间复杂度从 \\(O(n!)\\) 降到 \\(O(n^2*2^n)\\)。 image info \\[ \\begin{align*} &amp;\\text{Input: } \\mathcal{P} &amp;= &amp;\\left\\{P_{1}, \\ldots, P_{6} \\right\\} \\\\ &amp;\\text{Output: } C^{\\mathcal{P}} &amp;=&amp; \\{1,3,2,4,5,6,1\\} \\end{align*} \\] Delaunay三角剖分 Delaunay三角剖分问题是将平面上的散点集划分成三角形，使得在可能形成的三角剖分中，所形成的三角形的最小角最大。这个问题的输出是若干个集合，每个集合代表一个三角形，由输入点的编号表示。 \\[ \\begin{align*} &amp;\\text{Input: } \\mathcal{P} &amp;=&amp; \\left\\{P_{1}, \\ldots, P_{5} \\right\\} \\\\ &amp;\\text{Output: } C^{\\mathcal{P}} &amp;=&amp; \\{(1,2,4),(1,4,5),(1,3,5),(1,2,3)\\} \\end{align*} \\] Seq-to-Seq 模型 现在假设n是固定的，传统基本的seq-to-seq模型（参数部分记为 \\(\\theta\\) ），训练数据若记为\\((\\mathcal{P}, C^{\\mathcal{P}})\\)，，将拟合以下条件概率： \\[ \\begin{equation} p\\left(\\mathcal{C}^{\\mathcal{P}} | \\mathcal{P} ; \\theta\\right)=\\prod_{i=1}^{m(\\mathcal{P})} p\\left(C_{i} | C_{1}, \\ldots, C_{i-1}, \\mathcal{P} ; \\theta\\right) \\end{equation} \\] 训练的方向是找到 \\(\\theta^{*}\\) 来最大化上述联合概率，即： \\[ \\begin{equation} \\theta^{*}=\\underset{\\theta}{\\arg \\max } \\sum_{\\mathcal{P}, \\mathcal{C}^{\\mathcal{P}}} \\log p\\left(\\mathcal{C}^{\\mathcal{P}} | \\mathcal{P} ; \\theta\\right) \\end{equation} \\] Content Based Input Attention 一种增强基本seq-to-seq模型的方法是加入attention机制。记encoder和decoder隐藏状态分别是 $ (e_{1}, , e_{n}) $ 和 $ (d_{1}, , d_{m()}) $。seq-to-seq第 i 次输出了 \\(d_i\\)，注意力机制额外计算第i步的注意力向量 \\(d_i^{\\prime}\\)，并将其和\\(d_i\\)连接后作为隐藏状态。\\(d_i^{\\prime}\\)的计算方式如下，输入 $ (e_{1}, , e_{n}) $ 和 i 对应的权重向量 $ (a_{1}^{i}, , a_{n}^{i}) $做点乘。 \\[ d_{i} = \\sum_{j=1}^{n} a_{j}^{i} e_{j} \\] $ (a_{1}^{i}, , a_{n}^{i}) $ 是向量 $ (u_{1}^{i}, , u_{n}^{i}) $ softmax后的值， \\(u_{j}^{i}\\) 表示 \\(d_{i}\\) 和 \\(e_{j}\\)的距离，Pointer Networks论文中的距离为如下的tanh公式。 \\[ \\begin{eqnarray} u_{j}^{i} &amp;=&amp; v^{T} \\tanh \\left(W_{1} e_{j}+W_{2} d_\\right) \\quad j \\in(1, \\ldots, n) \\\\ a_{j}^{i} &amp;=&amp; \\operatorname{softmax}\\left(u_{j}^{i}\\right) \\quad j \\in(1, \\ldots, n) \\end{eqnarray} \\] 更多Attention计算方式 在FloydHub Blog - Attention Mechanism 中，作者清楚地解释了两种经典的attention方法，第一种称为Additive Attention，由Dzmitry Bahdanau 提出，也就是Pointer Networks中通过tanh的计算方式，第二种称为 Multiplicative Attention，由Thang Luong*提出。 Luong Attention 有三种方法计算 \\(d_{i}\\) 和 \\(e_{j}\\) 的距离（或者可以认为向量间的对齐得分）。 \\[ \\operatorname{score} \\left( d_i, e_j \\right)= \\begin{cases} d_i^{\\top} e_j &amp; \\text { dot } \\\\ d_i^{\\top} W_a e_j &amp; \\text { general } \\\\ v_a^{\\top} \\tanh \\left( W_a \\left[ d_i ; e_j \\right] \\right) &amp; \\text { concat } \\end{cases} \\] Pointer Networks image info Pointer Networks 基于Additive Attention，其创新之处在于用 \\(u^i_j\\) 作为第j个输入的评分，即第 i 次输出为1-n个输入中 \\(u^i_j\\) 得分最高的j作为输出，这样巧妙的解决了n不是预先固定的限制。 \\[ \\begin{eqnarray*} u_{j}^{i} &amp;=&amp; v^{T} \\tanh \\left(W_{1} e_{j}+W_{2} d_{i}\\right) \\quad j \\in(1, \\ldots, n) \\\\ p\\left(C_{i} | C_{1}, \\ldots, C_{i-1}, \\mathcal{P}\\right) &amp;=&amp; \\operatorname{softmax}\\left(u^{i}\\right) \\end{eqnarray*} \\] PyTorch 代码实现 在本系列第二篇 episode 2，中，我们说明过TSP数据集的格式，每一行字段意义如下 1x0, y0, x1, y1, ... output 1 v1 v2 v3 ... 1 转换成PyTorch Dataset 每一个case会转换成nd.ndarray，共有五个分量，分别是 (input, input_len, output_in, output_out, output_len) 并且分装成pytorch的 Dataset类。 {linenos123456789101112from torch.utils.data import Datasetclass TSPDataset(Dataset): \"each data item of form (input, input_len, output_in, output_out, output_len)\" data: List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]] def __len__(self): return len(self.data) def __getitem__(self, index): input, input_len, output_in, output_out, output_len = self.data[index] return input, input_len, output_in, output_out, output_len PyTorch pad_packed_sequence 优化技巧 PyTorch 实现 seq-to-seq 模型一般会使用 pack_padded_sequence 以及 pad_packed_sequence 来减少计算量，本质上可以认为根据pad大小分批进行矩阵运算，减少被pad的矩阵元素导致的无效运算，详细的解释可以参考 https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#decoder-1。 image info 对应代码如下： {linenos12345678910111213141516class RNNEncoder(nn.Module): rnn: Union[nn.LSTM, nn.GRU, nn.RNN] def __init__(self, rnn_type: str, bidirectional: bool, num_layers: int, input_size: int, hidden_size: int, dropout: float): super(RNNEncoder, self).__init__() if bidirectional: assert hidden_size % 2 == 0 hidden_size = hidden_size // 2 self.rnn = rnn_init(rnn_type, input_size=input_size, hidden_size=hidden_size, bidirectional=bidirectional,num_layers=num_layers, dropout=dropout) def forward(self, src: Tensor, src_lengths: Tensor, hidden: Tensor = None) -&gt; Tuple[Tensor, Tensor]: lengths = src_lengths.view(-1).tolist() packed_src = pack_padded_sequence(src, lengths) memory_bank, hidden_final = self.rnn(packed_src, hidden) memory_bank = pad_packed_sequence(memory_bank)[0] return memory_bank, hidden_final 注意力机制相关代码 {linenos12345678910111213141516171819202122232425262728293031323334class Attention(nn.Module): linear_out: nn.Linear def __init__(self, dim: int): super(Attention, self).__init__() self.linear_out = nn.Linear(dim * 2, dim, bias=False) def score(self, src: Tensor, target: Tensor) -&gt; Tensor: batch_size, src_len, dim = src.size() _, target_len, _ = target.size() target_ = target src_ = src.transpose(1, 2) return torch.bmm(target_, src_) def forward(self, src: Tensor, target: Tensor, src_lengths: Tensor) -&gt; Tuple[Tensor, Tensor]: assert target.dim() == 3 batch_size, src_len, dim = src.size() _, target_len, _ = target.size() align_score = self.score(src, target) mask = sequence_mask(src_lengths) # (batch_size, max_len) -&gt; (batch_size, 1, max_len) mask = mask.unsqueeze(1) align_score.data.masked_fill_(~mask, -float('inf')) align_score = F.softmax(align_score, -1) c = torch.bmm(align_score, src) concat_c = torch.cat([c, target], -1) attn_h = self.linear_out(concat_c) return attn_h, align_score 参考资料","link":"/zh/2020/tsp-3-pointer-net/"},{"title":"TSP问题从DP算法到深度学习4：概率最大状态序列算法","text":"本篇是TSP问题从DP算法到深度学习系列第四篇，这一篇我们会详细举例并比较在 seq-to-seq 或者Markov Chain中的一些常见的搜索概率最大的状态序列的算法。这些方法在深度学习的seq-to-seq 中被用作decoding。在第五篇中，我们使用强化学习时也会使用了本篇中讲到的方法。 第一篇: 递归DP方法 AC AIZU TSP问题 第二篇: 二维空间TSP数据集及其DP解法 第三篇: 深度学习 Pointer Networks 的 Pytorch实现 第四篇: 搜寻最有可能路径：Viterbi算法和其他 第五篇: 深度强化学习无监督算法的 Pytorch实现 马尔科夫链问题 在 seq-to-seq 问题中，我们经常会遇到需要从现有模型中找概率最大的可能状态序列。这类问题在机器学习算法和控制领域广泛存在，抽象出来可以表达成马尔可夫链模型：给定初始状态的分布和系统的状态转移方程（称为系统动力，dynamics），找寻最有可能的状态序列。 举个例子，假设系统有 \\(n\\) 个状态，初始状态由 $s_0 = [0.35, 0.25, 0.4] $ 指定，表示初始时三种状态的分布为 0.35，0.25和0.4。 状态转移矩阵由 \\(T\\) 表达，其中 $ T[i][j]$ 表示从状态 \\(i\\) 到状态 \\(j\\) 的概率。注意下面的矩阵 \\(T\\) 每行的和为 1.0，对应了从任意状态出发，下一时刻的所有可能转移概率和为1。 \\[ T= \\begin{matrix} &amp; \\begin{matrix}0&amp;1&amp;2\\end{matrix} \\\\\\\\ \\begin{matrix}0\\\\\\\\1\\\\\\\\2\\end{matrix} &amp; \\begin{bmatrix}0.3&amp;0.6&amp;0.1\\\\\\\\0.4&amp;0.2&amp;0.4\\\\\\\\0.3&amp;0.3&amp;0.4\\end{bmatrix}\\\\\\\\ \\end{matrix} \\] 至此，系统的所有参数都定下来了。接下去的各个时刻的状态分布可以通过矩阵乘法来算得。比如，记\\(s_1\\) 为 \\(t_1\\) 时刻状态分布，计算方法为 \\(s_0\\) 乘以 \\(T\\)，动画如下： \\(s_1\\) 数值计算结果如下。 \\[ s_1 = \\begin{bmatrix}0.35&amp; 0.25&amp; 0.4\\end{bmatrix} \\begin{matrix} \\begin{bmatrix}0.3&amp;0.6&amp;0.1\\\\\\\\0.4&amp;0.2&amp;0.4\\\\\\\\0.3&amp;0.3&amp;0.4\\end{bmatrix}\\\\\\\\ \\end{matrix} = \\begin{bmatrix}0.325&amp; 0.35&amp; 0.255\\end{bmatrix} \\] 矩阵左乘行向量可以理解为矩阵每一行的线性组合，直觉上理解为下一时刻的状态分布是上一时刻初始状态分布乘以转移关系的线性组合。 \\[ \\begin{bmatrix}0.35&amp; 0.25&amp; 0.4\\end{bmatrix} \\begin{matrix} \\begin{bmatrix}0.3&amp;0.6&amp;0.1\\\\\\\\0.4&amp;0.2&amp;0.4\\\\\\\\0.3&amp;0.3&amp;0.4\\end{bmatrix}\\\\\\\\ \\end{matrix} = 0.35 \\times \\begin{bmatrix}0.35&amp; 0.6&amp; 0.1\\end{bmatrix} + 0.25 \\times \\begin{bmatrix}0.4&amp; 0.2&amp; 0.4\\end{bmatrix} + 0.4 \\times \\begin{bmatrix}0.3&amp; 0.3&amp; 0.4\\end{bmatrix} \\] 同样的，后面每一个时刻都可以由上一个状态分布向量乘以 \\(T\\)，当然这里我们假设每个时刻的转移矩阵是不变的。当然，问题也可以是每个时刻都有不同的转移矩阵来定义，例如深度学习 seq-to-seq 模型。当然，这个设定的变化不会影响搜索最可能状态序列的算法。出于简单考虑，本篇中我们假定所有时刻的状态转移矩阵都是 \\(T\\)。 下面我们通过多种算法来找出由上述参数定义的系统中前三个时刻的最有可能序列，即概率最大的 \\(s_0 \\rightarrow s_1 \\rightarrow s_2\\)。 令 \\(L\\) 是阶段数，\\(N\\) 是每个阶段的状态数，则我们的例子中 \\(L=N=3\\) 。并且，总共有 \\(N^L\\) 种不同的路径。 穷竭搜索 若给定一条路径，计算特定路径的概率是很直接的，例如，若给定路径为 \\(2(s_0) \\rightarrow 1(s_1) \\rightarrow 2(s_2)\\)，则这条路径的概率为 \\[ p(2 \\rightarrow 1 \\rightarrow 2) = s_0[2] \\times T[2][1] \\times T[1][2] = 0.4 \\times 0.3 \\times 0.4 = 0.048 \\] 因此，我们可以通过枚举所有 \\(N^L\\) 条路径并计算每条路径的概率来找到最有可能的状态序列。 下面是Python 3的穷竭搜索代码，输出为最有可能的概率及其路径。样例问题的输出为 0.084 和状态序列 \\(0 \\rightarrow 1 \\rightarrow 2\\)。 {linenos12345678910111213141516171819def search_brute_force(initial: List, transition: List, L: int) -&gt; Tuple[float, Tuple]: from itertools import combinations_with_replacement v = [0, 1, 2] path_all = combinations_with_replacement(v, L) max_prop = 0.0 max_route = None prob = 0.0 for path in list(path_all): for idx, v in enumerate(path): if idx == 0: prob = initial[v] # reset to initial state else: prev_v = path[idx-1] prob *= transition[prev_v][v] if prob &gt; max_prop: max_prop = max(max_prop, prob) max_route = path return max_prop, max_route 贪心搜索 穷竭搜索一定会找到最有可能的状态序列，但是算法复杂度是指数级的 \\(O(N^L)\\)。一种最简化的策略是，每一时刻都只选取下一时刻最可能的状态，显然这种策略没有考虑全局最优，只考虑下一步最优，因此称为贪心策略。当然，贪心策略虽然牺牲全局最优解但是换取了很快的时间复杂度。贪心搜索算法动画如下。 Python 3 实现中我们利用了 numpy 类库，主要是 np.argmax() 可以让代码简洁。代码本质上是两重循环，（一层循环是np.argmax中），对应时间算法复杂度是 \\(O(N\\times L)\\)。 {linenos123456789101112131415161718def search_greedy(initial: List, transition: List, L: int) -&gt; Tuple[float, Tuple]: import numpy as np max_route = [] max_prop = 0.0 states = np.array(initial) prev_max_v = None for l in range(0, L): max_v = np.argmax(states) max_route.append(max_v) if l == 0: max_prop = initial[max_v] else: max_prop = max_prop * transition[prev_max_v][max_v] states = max_prop * states prev_max_v = max_v return max_prop, max_route Beam 搜索 贪心策略只考虑了下一步的最大概率状态，若我们改进一下贪心策略，将下一步的最大 \\(k\\) 个状态保留下来就是beam 搜索了。具体来说， \\(k\\) beam search表示每个阶段保留 \\(k\\) 个最大概率路径，下一阶段扩展这 \\(k\\) 条路径至 \\(k \\times N\\) 条路径再选取最大的top k。以上例来说，选取\\(k=2\\)，则初始 \\(s_0\\)时选取最大概率的两种状态 0和 2，下一阶段 \\(s_1\\)，计算以0和2开始的共 \\(2 \\times 3\\) 条路径，并保留其中最大概率的两条，如此往复。显然，beam search也无法找到全局最优解，但是它能以线性时间复杂度探索更多的路径空间。 以下是Python 3 的代码实现，利用了 PriorityQueue 选取 \\(k\\) 路径。由于PriorityQueue 无法自定义比较关系，我们定义了 @total_ordering 标注的类来实现比较关心。时间算法复杂度是 \\(O(k\\times N \\times L)\\) 。 {linenos1234567891011121314151617181920212223242526272829303132333435363738def search_beam(initial: List, transition: List, L: int, K: int) -&gt; Tuple[float, Tuple]: N = len(initial) from queue import PriorityQueue current_q = PriorityQueue() next_q = PriorityQueue() from functools import total_ordering @total_ordering class PQItem(object): def __init__(self, prob, route): self.prob = prob self.route = route self.last_v = int(route[-1]) def __eq__(self, other): return self.prob == other.prob def __lt__(self, other): return self.prob &gt; other.prob for v in range(N): next_q.put(PQItem(initial[v], str(v))) for l in range(1, L): current_q = next_q next_q = PriorityQueue() k = K while not current_q.empty() and k &gt; 0: item = current_q.get() prob, route, prev_v = item.prob, item.route, item.last_v k -= 1 for v in range(N): nextItem = PQItem(prob * transition[prev_v][v], route + str(v)) next_q.put(nextItem) max_item = next_q.get() return max_item.prob, list(map(lambda x: int(x), max_item.route)) Viterbi 动态规划 和之前TSP 动态规划算法的思想一样，最有可能状态路径问题解法有可以将指数时间复杂度 \\(O(N^L)\\) 降到多项式时间复杂度 \\(O(L \\times N \\times N)\\) 的算法，就是大名鼎鼎的 Viterbi 算法（维特比算法）。核心思想是在每个阶段，用数组保存每个状态结尾路径的阶段最大概率（及其对应路径）。在不考虑优化空间的情况下，我们开一个二维数组 \\(dp[][]\\)，第一维表示阶段序号，第二维表示状态序号。例如，\\(dp[1][0]\\) 是 \\(s_1\\) 阶段时以状态0结尾的所有路径中的最大概率，即 \\[ dp[1][0] = \\max \\\\{s_0[0] \\rightarrow s_1[0], s_0[1] \\rightarrow s_1[0], s_0[2] \\rightarrow s_1[0]\\\\} \\] 实现代码中没有返回路径本身而只是其概率值，目的是通过简洁的三层循环来表达算法精髓。 {linenos1234567891011def search_dp(initial: List, transition: List, L: int) -&gt; float: N = len(initial) dp = [[0.0 for c in range(N)] for r in range(L)] dp[0] = initial[:] for l in range(1, L): for v in range(N): for prev_v in range(N): dp[l][v] = max(dp[l][v], dp[l - 1][prev_v] * transition[prev_v][v]) return max(dp[L-1]) 概率采用 以上所有的算法都是确定性的。在NLP 深度学习decoding 时候会带来一个问题：确定性容易导致生成重复的短语或者句子。比如，确定性算法很容易生成如下句子。 1This is the best of best of best of ... 一种简单的方法是采用概率采用的方式回避这个问题。也就是我们不寻找确定的局部最优或者全局最优的解，而是通过局部路径或者全局路径的概率信息进行采样生成序列。例如，对于穷竭搜索的 \\(N^L\\) 条路径计算得到对应概率，转变成 \\(N^L\\) 个点的 categorical 分布，采样生成某条路径。也可以如下改造贪心或者beam 这类阶段性生成算法一个时刻一个时刻的输出采样的状态序列。 {linenos12345678910111213141516def search_prob_greedy(initial: List, transition: List, L: int) -&gt; Tuple[float, Tuple]: import random N = len(initial) max_route = [] max_prop = 0.0 vertices = [i for i in range(N)] prob = initial[:] for l in range(0, L): v_lst = random.choices(vertices, prob) v = v_lst[0] max_route.append(v) max_prop = prob[v] prob = [prob[v] * transition[v][v_target] for v_target in range(N)] return max_prop, max_route","link":"/zh/2020/tsp-4-search/"},{"title":"Pytorch Geometric (pyg) 系列教程","text":"请在桌面浏览器中打开此链接，保证最好的浏览体验 关注公众号后回复 docker-geometric，获取 Docker Image 12345# 导入imagedocker load &lt; env-torch1.8-geometric.tar# 运行 jupyter notebookdocker run -p 8888:8888 -it env-torch1.8-geometric jupyter notebook --allow-root --ip '0.0.0.0' --notebook-dir=/proj 在线代码 01 Plot Cora 02 Geometric Graph Data Representation 03 GCN 公众号系列视频链接 B站视频系列链接","link":"/zh/2021/course-pytorch-geometric/"},{"title":"用逆变换采样方法构建随机变量生成器","text":"上期 从零构建统计随机变量生成器之离散基础篇，我们从零出发构建了基于伯努利的基础离散分布，这一期我们来详细介绍广泛使用的 Inverse Transform Method（逆变换采样方法）。 从零构建统计随机变量生成器之 离散基础篇 从零构建统计随机变量生成器之 用逆变换采样方法构建随机变量生成器 深入 LeetCode 470 拒绝采样，状态转移图求期望和一道经典统计求期望题目 从零构建统计随机变量生成器之 正态分布 Box-Muller方法 逆变换采样方法 Inverse Transform Method 是最基础常见的方法，可用于离散分布和连续分布。常见的分布一般都能通过此方法生成，只需要随机变量CDF的解析表达式。假设随机变量 \\(X\\)，其CDF为 \\(F^{-1}\\)，则 Inverse Transform Method 仅有两步 通过生成 [0, 1] 之间的均匀随机数 \\(u\\) 代入 \\(F^{-1}\\) 即产生满足\\(X\\)分布的实例 \\(x = F^{-1}(u)\\) 离散例子 我们先举一个离散分布来直观感受一下其工作机制。有如下PMF的离散类别分布，范围在 [1, 5]。 \\[ P(X = 1)=\\frac{1}{15} \\] \\[ P(X = 2)=\\frac{2}{15} \\] \\[ P(X = 3)=\\frac{1}{5} \\] \\[ P(X = 4)=\\frac{4}{15} \\] \\[ P(X = 5)=\\frac{1}{3} \\] 转换成CDF为 \\[ P(X \\leq 1)=\\frac{1}{15} \\] \\[ P(X \\leq 2)=\\frac{1}{15}+\\frac{2}{15}=\\frac{1}{5} \\] \\[ P(X \\leq 3)=\\frac{1}{15}+\\frac{2}{15}+\\frac{1}{5}=\\frac{6}{15} \\] \\[ P(X \\leq 4)=\\frac{1}{15}+\\frac{2}{15}+\\frac{1}{5}+\\frac{4}{15}=\\frac{2}{3} \\] \\[ P(X \\leq 5)=\\frac{1}{15}+\\frac{2}{15}+\\frac{1}{5}+\\frac{4}{15}+\\frac{1}{3}=1 \\] 画出对应的CDF图 那么Inverse Transformation Method 的第一步，随机生成 0-1 之间的数 u，可以直观的认为是在 y 轴上生成一个随机的点 u。注意到5段竖虚线对应了5个离散的取值，它们的长度和为1，并且每一段长度代表了每个值的权重。因此，通过在 y 轴上的均匀采样可以生成给定PMF的 x 的分布。 离散分布的逆变换采样方法用数学公式可以表述为：找到第一个 x，其CDF的范围包括了 u，即 \\[ F^{-1}(u)=\\min \\{x: F(x) \\geq u\\} \\] 扩展到连续分布 有了离散类别分布的直观感受，扩展到连续分布也就不难理解了。类似于微积分中将连续空间做离散切分，再通过极限的方法，连续光滑函数在 y 轴上可以切分成长度为 \\(\\Delta u\\) 的线段，那么生成的 x 值就是其近似值。随着 $ _{u } $，最终 $ x=F^{-1}(u) $ 即为满足要求的分布。 指数分布（连续） 以最为常见的指数分布为例，我们来看看具体的步骤。 我们知道指数分布的PDF如下 \\[ f(x)=\\left\\{\\begin{array}{ll}\\lambda e^{-\\lambda x}, &amp; x \\geq 0 \\\\ 0, &amp; x&lt;0\\end{array}\\right. \\] PDF 图为 计算CDF为 \\[ F(x)=\\int_{-\\infty}^{x} f(t) d t=\\left\\{\\begin{array}{ll}1-e^{-\\lambda x}, &amp; x \\geq 0 \\\\ 0, &amp; x&lt;0\\end{array}\\right. \\] CDF 图 可以求得逆函数为 \\[ x=F^{-1}(u)=-\\frac{1}{\\lambda} \\ln (1-u) \\] 由于 1-u 在 [0, 1] 范围上的随机数等价于 u，因此，x 的生成公式等价于 \\[ x=-\\frac{1}{\\lambda} \\ln (u) \\] 实现代码 对应代码很简单 123456import randomfrom math import log2 as lndef exp_gen(lambbda: float) -&gt; float: u = random.random() return -ln(u) / lambbda Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/coutinous_exp_inv.py 类别分布（离散） 我们再来看基于类别分布 Inverse Transformation Method的其他离散分布例子。在从零构建统计随机变量生成器之离散基础篇中，我们已经介绍了类别分布（Categorical Distribution）的逆变换采样算法，同时还介绍了通过模拟 Bernoulli 实验来生成二项，几何，超几何分布的方法。在这一篇中，我们通过逆变换采样算法再来生成这些分布。 先回顾一下类别分布的逆变换采样实现。 给定如下的类别分布， $p = [0.2, 0.3, 0.1, 0.4] $ 实现代码 类别分布的逆变换采样实现需要找到第一个大于 u 的元素的索引序号，在我们的实现中，将 $p = [0.2, 0.3, 0.1, 0.4] $ 转换成累计概率 $c = [0.2, 0.5, 0.6, 1] $ 后，由于 \\(\\vec c\\) 数组是非递减的，因此我们可以用二分法代替线性查找，将时间复杂度降到 \\(O(log(n))\\)。下面的实现中直接调用 python bisect 函数即可。 12345678910111213import bisectimport randomfrom typing import Listdef categorical(probs: List[float]) -&gt; int: assert abs(sum(probs) - 1.0) &lt; 0.001 cum = probs.copy() for i in range(1, len(cum)): cum[i] = cum[i-1] + probs[i] u = random.random() return bisect.bisect(cum, u) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_categorical.py 二项分布（离散） 二项分布（Binomial Distribution）有两个参数 n 和 p，表示伯努利实验做n次后成功的次数。图中为 n=6，p=0.5的二项分布。 概率质量函数（PMF） \\[ \\operatorname{P}_\\text{Binomial}(X=k)=\\left(\\begin{array}{c}n \\\\ k\\end{array}\\right)p^{k}(1- p)^{n-k} \\] 实现代码 根据上面的PMF定义，我们将 [0, 6] 上的PMF计算出来，然后调用类别分布的逆变换采样实现即可： 123456789from scipy.special import combfrom discrete_categorical import categoricalfrom math import powdef binomial(n: int, p: float) -&gt; int: pmf = [comb(n, k, exact=True) * pow(p, k) * pow(1-p, n-k) for k in range(0, n + 1)] return categorical(pmf) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_binomial_inv.py 超几何分布（离散） 同样的，超几何分布（HyperGeometric Distribution）也可以如法炮制。 概率质量函数（PMF） \\[ \\operatorname{P}_\\text{Hypergeo}(X=k)=\\frac{\\left(\\begin{array}{c}K \\\\ k\\end{array}\\right)\\left(\\begin{array}{c}N-k \\\\ n-k\\end{array}\\right)}{\\left(\\begin{array}{l}N \\\\ n\\end{array}\\right)} \\] 实现代码 12345678from scipy.special import combfrom discrete_categorical import categoricaldef hypergeometric(N: int, K_succ_num: int, n_trial_num: int) -&gt; int: pmf = [comb(K_succ_num, k, exact=True) * comb(N - K_succ_num, n_trial_num - k, exact=True) / comb(N, n_trial_num, exact=True) for k in range(max(0, n_trial_num - (N - K_succ_num)), min(K_succ_num, n_trial_num) + 1)] return categorical(pmf) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_hypergeometric_inv.py 几何分布（离散） 几何分布（Geometric Distribution）和上面的二项分布以及超几何分布不同的是，它的 support 是所有非负整数，因此，我们无法穷举计算所有 x 的概率。但是，我们可以通过将CDF 推出 Inverse CDF的解析表达式来直接实现。 概率质量函数（PMF） \\[ \\operatorname{P}_\\text{Geometric}(X=k)=(1-p)^{k-1} p \\] CDF \\[ F_X(x) = 1- (1-p)^x \\] Inverse CDF 反函数求得为 \\[ F^{-1}(u) = \\lfloor { log(1-u) \\over log(1-p) }\\rfloor \\] 实现代码 1234567import randomfrom math import floorfrom math import log2 as lndef geometric(p: float) -&gt; int: u = random.random() return floor(ln(u) / ln(1-p)) Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_geometric_inv.py 标准正态分布 一般，标准正态分布用Box-Muller 方法来生成，这个后续将做详细介绍。这里我们用 Schmeiser 提出的基于Inverse Transformation Method的近似方法来生成： \\[ X=F^{-1}(u) \\approx \\frac{u^{0.135}-(1-u)^{0.135}}{0.1975} \\] 实现代码 123456import randomdef normal(): import math u = random.random() return (math.pow(u, 0.135) - math.pow(1-u, 0.135)) / 0.1975 Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/coutinous_normal_apprx.py","link":"/zh/2021/distribution-inverse-transformation-method/"},{"title":"从零构建统计随机变量生成器之正态分布 Box-Muller方法","text":"在学习了一些基本的统计变量生成法之后，这次我们来看看如何生成正态分布。它就是大名鼎鼎的 Box-Muller 方法，Box-Muller 的理解过程可以体会到统计模拟的一些精妙思想。 从零构建统计随机变量生成器之 离散基础篇 从零构建统计随机变量生成器之 用逆变换采样方法构建随机变量生成器 深入 LeetCode 470 拒绝采样，状态转移图求期望和一道经典统计求期望题目 从零构建统计随机变量生成器之 正态分布 Box-Muller方法 尝试逆变换方法 我们先尝试通过标准的逆变换方法来生成正态分布。 正态分布的 PDF 表达式为 \\[ f_Z(z) = \\frac{1}{\\sqrt{2 \\pi}} \\exp\\left\\{-\\frac{z^2}{2}\\right\\} \\] 对应的函数图形是钟形曲线 根据 PDF，其 CDF 的积分形式为 \\[ \\Phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\int_{-\\infty}^{x} e^{-t^{2} / 2} d t \\] 和所有 PDF CDF 关系一样，\\(\\Phi(x)\\) 表示 \\(f_Z\\) 累积到 \\(x\\) 点的面积。 很不幸的是，\\(\\Phi(x)\\) 无法写出一般数学表达式，因而也无法直接用逆变换方法。 二维映射到一维 我们知道，高维正态分布有特殊的性质：它的每一维的分量都是正态分布；单个维度对于其他维度的条件概率分布也是正态分布。 用图来理解这两条性质就是，对于下图的二维正态分布 $ x = [x_1, x_2]^T $，单独的 \\(x_1\\) 和 \\(x_2\\) 都服从一维正态分布。 条件概率 \\(p(x_2|x_1 \\approx1)\\) 的PDF 对应图中的红线，显然也是一维正态分布。 写一段简单的代码验证二维正态分布的单个分量服从正态分布。 代码中，我们用np.random.normal生成了 10000 个服从二维正态分布的 x, y 点，然后我们丢弃 y，只保留 x，并画出 10000 个 x 的分布。 12345678def plot_normal_1d(): x, _ = np.random.normal(loc=0, scale=1, size=(2, 10000)) import seaborn as sns sns.distplot(x, hist=True, kde=True, bins=100, color='darkblue', hist_kws={'edgecolor': 'black'}, kde_kws={'linewidth': 4}) plt.title('PDF Normal 1D from 2D') plt.show() Box-Muller 原理 虽然无法直接用逆变换方法生成一维正态分布，但我们却能通过先生成二维的正态分布，利用上面一节的性质，生成一维正态分布。 而 Box-Muller 就是巧妙生成二维正态分布样本点的方法。 首先，我们来看看二维正态分布可以认为是两个维度是独立的，每个维度都是正态分布。此时，其 PDF 可以写成两个一维正态分布 PDF 的乘积。 这种写法表明，二维正态分布仅用一个 r 向量就可以充分表达。注意，r 是向量，不仅有大小还有角度，有两个分量。这两个分量本质上是独立的，这就是 Box-Muller 方法的巧妙之处。也就是，Box-Muller 通过角度和半径大小两个分量的独立性分别单独生成并转换成 (x, y) 对。 角度分量是在 \\(2\\pi\\) 范围均匀采样，这一点比较直觉好理解。 再来看看半径分量 r。我们令 \\[ s = {r^2 \\over 2} \\Longrightarrow r = \\sqrt{2s} \\] 则 s 服从指数分布 \\(\\lambda=1\\) 。 不信么？我们不妨来做个模拟实验，下图是模拟 10000次二维正态分布 (x, y) 点后转换成 s 的分布。 模拟和plot 代码如下 123456789101112131415161718192021def plot_r_squared(): def gen_normal_samples(n): x, y = np.random.normal(loc=0, scale=1, size=(2, n)) return x, y x, y = gen_normal_samples(10000) s = (x * x + y * y)/2 plot_dist_1d(s, title='PDF $s = {{x^2 + y^2}\\over{2}} \\sim exp(1)$') def plot_dist_1d(X, title='PDF '): import seaborn as sns plt.rcParams.update({ \"text.usetex\": True, \"font.family\": \"sans-serif\", \"font.sans-serif\": [\"Helvetica\"]}) sns.distplot(X, hist=True, kde=True, bins=100, color='darkblue', hist_kws={'edgecolor': 'black'}, kde_kws={'linewidth': 4}) plt.title(title) plt.show() 确信了 s 符合指数分布，根据指数分布的 PDF，可以推出二维正态 PDF中的 $ e{-r2/2}$ 也符合指数分布，即 \\[ s \\sim \\exp(1) \\Longrightarrow e^{-r^2/2} \\sim \\exp(1) \\] 至此，总结一下Box-Muller方法。我们视二维正态分布PDF为独立两部分的乘积，第一部分是在 \\(2\\pi\\) 范围中的均匀分布，代表了二维平面中的角度 \\(\\theta\\)，第二部分为 \\(\\lambda=1\\) 的指数分布，代表半径大小。 Box-Muller 方法通过两个服从 [0, 1] 均匀分布的样本 u1和u2，转换成独立的角度和半径样本，具体过程如下 生成 [0, 1] 的均匀分布 u1，利用逆变换采样方法转换成 exp(1) 样本，此为二维平面点半径 r 生成 [0, 1] 的均匀分布 u2，乘以 \\(2\\pi\\)，即为样本点的角度 \\(\\theta\\) 将 r 和 \\(\\theta\\) 转换成 x, y 坐标下的点。 理解了整个过程的意义，下面的代码就很直白。 12345678910def normal_box_muller(): import random from math import sqrt, log, pi, cos, sin u1 = random.random() u2 = random.random() r = sqrt(-2 * log(u1)) theta = 2 * pi * u2 z0 = r * cos(theta) z1 = r * sin(theta) return z0, z1 接下来，我们来看看 Box-Muller 法生成的二维标准正态分布动画吧 拒绝采样极坐标方法 Box-Muller 方法还有一种形式，称为极坐标形式，属于拒绝采样方法。 1. 生成独立的 u, v 和 s 分别生成 [0, 1] 均匀分布 u 和 v。令 \\(s = r^2 = u^2 + v^2\\)。如果 s = 0或 s ≥ 1，则丢弃 u 和 v ，并尝试另一对 (u , v)。因为 u 和 v 是均匀分布的，并且因为只允许单位圆内的点，所以 s 的值也将均匀分布在开区间 (0, 1) 中。注意，这里的 s 的意义虽然也为半径，但不同于基本方法中的 s。这里 s 取值范围为 (0, 1) ，目的是通过 s 生成指数分布，而基本方法中的 s 取值范围为 [0, +∞]，表示二维正态分布 PDF 采样点的半径。复用符号 s 的原因是为了对应维基百科中关于基本方法和极坐标方法的数学描述。 我们用代码来验证 s 服从 (0, 1) 范围上的均匀分布。 1234567891011121314def gen_polar_s(): import random while True: u = random.uniform(-1, 1) v = random.uniform(-1, 1) s = u * u + v * v if s &gt;= 1.0 or s == 0.0: continue return sdef plot_polar_s(): s = [gen_polar_s() for _ in range(10000) ] plot_dist_1d(s, title='PDF Polar $s = u^2 + v^2$') 2. 将 u, v, s 转换成 x, y 若将 $s = R^2 uniform(0, 1) $ 看成是基本方法中的 u1，就可以用同样的方式转换成指数分布，用以代表二维PDF的半径。 同时，根据下图，\\(\\cos \\theta\\) 和 \\(\\sin \\theta\\) 可以直接用 u, v, R 表示出来，并不需要通过三角函数显示计算出 \\(\\theta\\)。有了半径， \\(\\cos \\theta\\) 和 \\(\\sin \\theta\\) ，则可以直接计算出 x, y 坐标，（下面用 \\(z_0, z_1\\) 代替 \\(x, y\\)）。 \\[ z_{0}=\\sqrt{-2 \\ln U_{1}} \\cos \\left(2 \\pi U_{2}\\right)=\\sqrt{-2 \\ln s}\\left(\\frac{u}{\\sqrt{s}}\\right)=u \\cdot \\sqrt{\\frac{-2 \\ln s}{s}} \\] \\[ z_{1}=\\sqrt{-2 \\ln U_{1}} \\sin \\left(2 \\pi U_{2}\\right)=\\sqrt{-2 \\ln s}\\left(\\frac{v}{\\sqrt{s}}\\right)=v \\cdot \\sqrt{\\frac{-2 \\ln s}{s}} \\] 同样，Box-Muller 极坐标方法的代码和公式一致。 123456789101112def normal_box_muller_polar(): import random from math import sqrt, log while True: u = random.uniform(-1, 1) v = random.uniform(-1, 1) s = u * u + v * v if s &gt;= 1.0 or s == 0.0: continue z0 = u * sqrt(-2 * log(s) / s) z1 = v * sqrt(-2 * log(s) / s) return z0, z1 拒绝采样的效率 极坐标方法与基本方法的不同之处在于它是一种拒绝采样。因此，它会丢弃一些生成的随机数，但可能比基本方法更快，因为它计算更简单：避免使用昂贵的三角函数，并且在数值上更稳健。极坐标方法丢弃了生成总输入对的 1 − π /4 ≈ 21.46%，即需要 4/ π ≈ 1.2732 个输入随机数，输出一个随机采样。","link":"/zh/2021/distribution-normal/"},{"title":"从零构建统计随机变量生成器之泊松分布","text":"http://www.columbia.edu/~ks20/4404-Sigman/4404-Notes-ITM.pdf https://www.win.tue.nl/~marko/2WB05/lecture8.pdf 泊松分布 12345678910111213import randomfrom math import expdef poisson(lambdda: float) -&gt; int: total = 1.0 i = 0 threshold = exp(-1 * lambdda) while total &gt;= threshold: u = random.random() total *= u i += 1 return i - 1 https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_poisson_inv.py 12345678910from numpy.random import exponentialdef poisson(lambdda: float) -&gt; int: total = 0.0 i = 0 while total &lt;= lambdda: y = exponential(1) total += y i += 1 return i - 1 https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_poisson_from_exp.py 泊松分布 $ E_{1}, E_{2}, E_{3}, (1) $ \\[ \\mathbb{P}(K \\geqslant k)=\\mathbb{P}\\left(E_{1}+\\cdots+E_{k} \\leqslant \\lambda\\right) \\] 12345678910from numpy.random import exponentialdef poisson(lambdda: float) -&gt; int: total = 0.0 i = 0 while total &lt;= lambdda: y = exponential(1) total += y i += 1 return i - 1 Github 代码地址： https://github.com/MyEncyclopedia/stats_simulation/blob/main/distrib_sim/discrete_poisson_from_exp.py","link":"/zh/2021/distribution-poisson/"},{"title":"跨平台任天堂红白机强化学习预制环境","text":"今天和大家分享强化学习的经典训练环境，任天堂的红白机训练环境。 这次的环境，我将分装成 docker 镜像，这样在任何平台：Windows，Linux 甚至 Mac 上都可以运行。 这里将通过大家最常用的 Windows 系统来演示环境的使用。 X window 服务器 在 Windows上，首先，我们要装 X Window Server。可以用 Cygwin 或者是 XLaunch。 这里采用 XLaunch 是因为安装比较方便，安装包也很小。 如果 XLaunch 正常启动的话，就会在系统托管的地方显示出来。接着我们来下载环境的 docker images。 拉取镜像 用 docker pull 命令我们将预制的公开镜像拉下来 1docker pull myencyclopedia/gym-nes 拉下来以后，可以用 docker image命令来检查是否存在 1docker images 下一步，我们需要找到物理机或者 docker host 机器的 IP 地址。 在windows上，我们执行 ipconfig 命令，注意我们要的是 WSL 对应的 IP 地址。 12345678910111213$ ipconfig.exeWindows IP 配置以太网适配器 以太网: 媒体状态 . . . . . . . . . . . . : 媒体已断开连接 连接特定的 DNS 后缀 . . . . . . . :以太网适配器 vEthernet (WSL): 连接特定的 DNS 后缀 . . . . . . . : 本地链接 IPv6 地址. . . . . . . . : fe80::8841:6bd8:5064:9a3c%45 IPv4 地址 . . . . . . . . . . . . : 172.23.0.1 子网掩码 . . . . . . . . . . . . : 255.255.240.0 默认网关. . . . . . . . . . . . . : 得到了物理机的 docker 网段地址以后，我们将地址保存在物理机的 Display 环境变量中，注意最后需要加上 :0 1export DISPLAY=172.23.0.1:0 至此，我们可以一键跑超级玛丽了。 1docker run -e DISPLAY=$DISPLAY myencyclopedia/gym-nes bash -c 'python gym_nes_demo.py' 解释一点，-e DISPLAY=$DISPLAY 将 Display 环境变量从当前 shell 注入到 container 中。 一切顺利的话，有个 X window的窗口会跳出来，无人控制得超级玛丽运行了起来，它会随机执行一些动作。 结束程序记得要把 docker container 显示关掉，需要执行 docker stop。 各种游戏 其实，预制的 docker 环境给大家装了更多的游戏，大家也可以修改源码跑其他游戏 具体方法是，将上面命令稍微修改一下 次我们进入 interactive bash。 1docker run -it -e DISPLAY=$DISPLAY myencyclopedia/gym-nes bash 进了 container 之后，我们发现之前执行的 python 原代码是当前目录的 gym_nes_demo.py 先列出所有的游戏的 rom 文件。 12345678910111213141516(py3.7) root@aff72945133b:/proj/nes_py# find . -name '*.nes'./tests/games/excitebike.nes./tests/games/super-mario-bros-2.nes./tests/games/super-mario-bros-3.nes./tests/games/empty.nes./tests/games/super-mario-bros-lost-levels.nes./tests/games/super-mario-bros-1.nes./tests/games/the-legend-of-zelda.nes./tests/nes-roms/1942 (Japan, USA).nes./tests/nes-roms/contra.nes./tests/nes-roms/Battle City (J).nes./tests/nes-roms/red.nes./tests/nes-roms/Gradius 2 (J).nes./tests/nes-roms/super-mario.nes./tests/nes-roms/Contra Force (USA).nes./tests/nes-roms/Rush'n Attack (U).nes 文件夹里有很多游戏， 包括魂斗罗，坦克大战等等。 修改 gym_nes_demo.py，将超级玛丽替换成魂斗罗 nes。 1234567891011121314151617from nes_py import NESEnvimport tqdmenv = NESEnv('/proj/nes_py/tests/games/super-mario-bros-1.nes')done = Truetry: for _ in tqdm.tqdm(range(5000)): if done: state = env.reset() done = False else: state, reward, done, info = env.step(env.action_space.sample()) env.render()except KeyboardInterrupt: pass 1env = NESEnv('./tests/nes-roms/contra.nes') 保存后，执行 1python gym_nes_demo.py 魂斗罗的 random agent 也跑起来了。 下一期，我会把一些很经典的深度强化学习的算法应用到这个环境中，让大家可以很方便得训练调试深度强化学习算法来挑战各种红白机游戏。 最后，感谢大家关注 MyEncyclopedia 公众号，B站频道或者 Youtube 频道。谢谢大家再见。","link":"/zh/2021/docker-gym-nes/"},{"title":"为什么我将博客从 Hugo 切换到了 Hexo","text":"当下两种静态博客 Hugo 和 Hexo 都很出色，都是将 markdown 格式的文件编译成 html。 Hugo 是用 go 语言来写的，Hexo 是用 node.js 来实现的。但是在经过了一年多的 Hugo 使用之后，终于下定决心切换到 Hexo。这里记录了自己的一些使用心得，比较和一些小技巧。 发布到 GitHub 注意到 Hexo 是发现 Github 可以 host 静态博客，并且 Hexo 默认支持直接发布到 Github，就抱着好奇心尝试了下 Hexo。 下面的链接是站点发布到 Github后通过二级域名可以访问，打开速度比自己的站点快不少。 https://myencyclopedia.github.io/archives/ 两者共同优点 总的来说，hugo 对于不懂前端但是想部署博客的同学来说是很友好的。但是，对于熟悉大前端的同学，可以很方便在静态页面基础上加入动态元素以及快速开发插件，Hexo 是比较完美的解决方案。 下面是我在考虑静态博客时一些两者都满足的特性： 需求点 Hexo Hugo Markdown 源文件 ✔ ✔ 代码高亮 ✔ ✔ Latex支持 ✔ ✔ 多语言频道 ✔ ✔ 发布到 Github ✔ ✔ Markdown 中引用其他页面 ✔ ✔ 桌面版+手机版 ✔ ✔ 嵌入 Youtube 和 B站视频 ✔ ✔ 接入 Google Analytics ✔ ✔ 用户评论 ✔ ✔ Hexo 优势 当遇到默认框架无法支持的功能时，两者的差别就体现了出来。 Hugo 的插件系统主要是通过 shortcode 来完成。记得之前为了开发hugo最简单的 shortcode 花费了九牛二虎之力，既包含了部署 Go语言环境，也包含调试开发的小插件，并且由于开发工具的匮乏，只能通过log打印出来内部状态，非常耗时。另外，配置 mathjax 等也花了很多时间，出现莫名其妙的 quote 错误时只能不断试错，没有明显的错误提示。又比如，很多时候需要调整编译 pipeline 来完成类似图片加水印，图片文件压缩等功能，也没有现成方案。 Hexo 则相反，首先它有着更丰富的插件。其次，可以很方便的通过 node.js 生态来开发新插件，控制编译 pipeline。当然，这需要对 npm，javascript 等大前端的编程环境比较熟悉，所以 Hexo 很适合前端程序员。例如，Hexo 可以通过现有插件来实现手机版炫酷的无限下滚功能，或者类似于微信公众号的图片异步加载功能，这些都是 Hugo 无法简单完成的。可以说，使用大前端 js 的 Hexo 将静态markdown 动态编译和动态灵活的客户端技术完美结合为一体。 下面是一些我目前想到的 Hexo 相对于 Hugo 的优势特性 需求点 Hexo Hugo 新文章自动提交 Google Baidu 收录 ✔ ❌ 图片水印 ✔ ❌ 简单搜索支持 ✔ ❌ SVG 动画 ✔ ❌ 内嵌 PDF 阅读 ✔ ❌ 动态CDN ✔ ❌ 方便的插件开发 方便 比较麻烦 Typora 统一资源路径 在 Hexo 启用了文章资源目录后，可以将相关资源拷贝到专有同名目录下，并在文章中直接引用资源文件名。 具体步骤如下 1. 启用资源目录 **_config.yml** 中编辑 1post_asset_folder: true 2. 文件存放结构 1234567_posts/│└───article1.md└───article1/│ │ img1.png│ │ img2.png 3. markdown 中引用资源文件 article1.md 1![](img1.png) 4. Typora 启用同样资源引用模式 平时我用 Typora 编辑，Typora 也支持这种资源引用模式。 article1.md 1typora-root-url: article1 在 VS Code 中调试插件 VS Code 已经默认支持调试项目 npm 或者 yarn 安装的 node.js 模块，这对于开发或者改动第三方插件而言非常方便，具体方法为。 打开 package.json，将鼠标放在 scripts 的目标中，会弹出 Run Script | Debug Script。 选择 Debug Script 在 node_modules 中相关源文件中设置断点，就可以调试了！","link":"/zh/2021/general-hexo/"},{"title":"Leetcode 1029 两地调度优化解法（附OR-Tools和PuLP代码）","text":"Leetcode 1029. 两地调度 (medium) 公司计划面试 2N 人。第 i 人飞往 A 市的费用为 costs[i][0]，飞往 B 市的费用为 costs[i][1]。 返回将每个人都飞到某座城市的最低费用，要求每个城市都有 N 人抵达。&nbsp; 示例： 输入：[[10,20],[30,200],[400,50],[30,20]] 输出：110 解释： 第一个人去 A 市，费用为 10。 第二个人去 A 市，费用为 30。 第三个人去 B 市，费用为 50。 第四个人去 B 市，费用为 20。 最低总费用为 10 + 30 + 50 + 20 = 110，每个城市都有一半的人在面试。 提示： 1 &lt;= costs.length &lt;= 100 costs.length 为偶数 1 &lt;= costs[i][0], costs[i][1] &lt;= 1000 链接：https://leetcode-cn.com/problems/two-city-scheduling 暴力枚举法 最直接的方式是暴力枚举出所有分组的可能。因为 2N 个人平均分成两组，总数为 \\({2n \\choose n}\\)，是 n 的指数级数量。在文章24 点游戏算法题的 Python 函数式实现: 学用itertools，yield，yield from 巧刷题，我们展示如何调用 Python 的 itertools包，这里，我们也用同样的方式产生 [0, 2N] 的所有集合大小为N的可能（保存在left_set_list中），再遍历找到最小值即可。当然，这种解法会TLE，只是举个例子来体会一下暴力做法。 {linenos123456789101112131415161718import mathfrom typing import Listclass Solution: def twoCitySchedCost(self, costs: List[List[int]]) -&gt; int: L = range(len(costs)) from itertools import combinations left_set_list = [set(c) for c in combinations(list(L), len(L)//2)] min_total = math.inf for left_set in left_set_list: cost = 0 for i in L: is_left = 1 if i in left_set else 0 cost += costs[i][is_left] min_total = min(min_total, cost) return min_total O(N) AC解法 对于组合优化问题来说，例如TSP问题（解法链接 TSP问题从DP算法到深度学习1：递归DP方法 AC AIZU TSP问题），一般都是 NP-Hard问题，意味着没有多项次复杂度的解法。但是这个问题比较特殊，它增加了一个特定条件：去城市A和城市B的人数相同，也就是我们已经知道两个分组的数量是一样的。我们仔细思考一下这个意味着什么？考虑只有四个人的小规模情况，如果让你来手动规划，你一定不会穷举出所有两两分组的可能，而是比较人与人相对的两个城市的cost差。举个例子，有如下四个人的costs 12340 A:3, B:11 A:99, B:1002 A:2, B:23 A:3, B:3 虽然1号人去城市A（99）cost 很大，但是相比较他去B（100）来说，可以省下 100-99 = 1 的钱，这个钱比0号人去B不去A省下的钱 3-1 = 2 还要多，因此你一定会选择让1号人去A而让0号人去B。 有了这个想法，再整理一下，就会发现让某人去哪个城市和他去两个城市的cost 差 $ C_a - C_b$相关，如果这个值越大，那么他越应该去B。但是最后决定他是否去B取决于他的差在所有人中的排名，由于两组人数相等，因此差能大到排在前一半，则他就去B，在后一半就去A。 按照这个思路，很快能写出代码，代码写法有很多，下面略举一例。代码中由于用到排序，复杂度为 \\(O(N \\cdot \\log(N))\\) 。这里补充一点，理论上只需找数组中位数的值即可，最好的时间复杂度是 \\(O(N)\\)。 代码实现上，cost_diff_list 将每个人的在原数组的index 和他的cost差组成 pair。即 1[(0, cost_0), (1, cost_1), ... ] 这样我们可以将这个数组按照cost排序，排完序后前面N个元素属于B城市。 {linenos1234567891011121314151617# AC# Runtime: 36 ms, faster than 87.77% of Python3 online submissions# Memory Usage: 14.5 MB, less than 14.84% of Python3 onlinefrom typing import Listclass Solution: def twoCitySchedCost(self, costs: List[List[int]]) -&gt; int: L = range(len(costs)) cost_diff_lst = [(i, costs[i][0] - costs[i][1]) for i in L] cost_diff_lst.sort(key=lambda x: x[1]) total_cost = 0 for c, (idx, _) in enumerate(cost_diff_lst): is_left = 0 if c &lt; len(L) // 2 else 1 total_cost += costs[idx][is_left] return total_cost 转换成整数规划问题 这个问题对于略有算法经验的人来说，很类似于背包问题。它们都需要回答N个物品取或者不取，并同时最大最小化总cost。区别在它们的约束条件不一样。这道题的约束是去取（去城市A）和不取（去城市B）的数量一样。这一类问题即 integer programming，即整数规划。下面我们选取两个比较流行的优化库来展示如何调包解这道题。 首先我们先来formulate这个问题，因为需要表达两个约束条件，我们将每个人的状态分成是否去A和是否去B两个变量。 12x[i-th-person][0]: boolean 表示是否去 city ax[i-th-person][1]: boolean 表示是否去 city b 这样，问题转换成如下优化模型 \\[ \\begin{array}{rrclcl} \\displaystyle \\min_{x} &amp; costs[i][0] \\cdot x[i][0] + costs[i][1] \\cdot x[i][1]\\\\ \\textrm{s.t.} &amp; x[i][0] + x[i][1] =1\\\\ &amp;x[i][0] + x[i][1] + ... =N \\\\ \\end{array} \\] Google OR-Tools Google OR-Tools 是业界最好的优化库，下面为调用代码，由于直接对应于上面的数学优化问题，不做赘述。当然 Leetcode上不支持这些第三方的库，肯定也不能AC。 {linenos123456789101112131415161718192021222324252627282930313233343536from ortools.sat.python import cp_modelcosts = [[515,563],[451,713],[537,709],[343,819],[855,779],[457,60],[650,359],[631,42]]I = range(len(costs))model = cp_model.CpModel()x = []total_cost = model.NewIntVar(0, 10000, 'total_cost')for i in I: t = [] for j in range(2): t.append(model.NewBoolVar('x[%i,%i]' % (i, j))) x.append(t)# Constraints# Each person must be assigned to at exact one city[model.Add(sum(x[i][j] for j in range(2)) == 1) for i in I]# equal number of person assigned to two citiesmodel.Add(sum(x[i][0] for i in I) == (len(I) // 2))# Total costmodel.Add(total_cost == sum(x[i][0] * costs[i][0] + x[i][1] * costs[i][1] for i in I))model.Minimize(total_cost)solver = cp_model.CpSolver()status = solver.Solve(model)if status == cp_model.OPTIMAL: print('Total min cost = %i' % solver.ObjectiveValue()) print() for i in I: for j in range(2): if solver.Value(x[i][j]) == 1: print('People ', i, ' assigned to city ', j, ' Cost = ', costs[i][j]) 完整代码可以从我的github下载。 https://github.com/MyEncyclopedia/leetcode/blob/master/1029_Two_City_Scheduling/1029_ortool.py PuLP 类似的，另一种流行 python 优化库 PuLP 的代码为 {linenos1234567891011121314151617181920212223242526272829303132import pulpcosts = [[259,770],[448,54],[926,667],[184,139],[840,118],[577,469]] # 1859I = range(len(costs))items=[i for i in I]city_a = pulp.LpVariable.dicts('left', items, 0, 1, pulp.LpBinary)city_b = pulp.LpVariable.dicts('right', items, 0, 1, pulp.LpBinary)m = pulp.LpProblem(\"Two Cities\", pulp.LpMinimize)m += pulp.lpSum((costs[i][0] * city_a[i] + costs[i][1] * city_b[i]) for i in items)# Constraints# Each person must be assigned to at exact one cityfor i in I: m += pulp.lpSum([city_a[i] + city_b[i]]) == 1# create a binary variable to state that a table setting is usedm += pulp.lpSum(city_a[i] for i in I) == (len(I) // 2)m.solve()total = 0for i in I: if city_a[i].value() == 1.0: total += costs[i][0] else: total += costs[i][1]print(\"Total cost {}\".format(total)) 代码地址为 https://github.com/MyEncyclopedia/leetcode/blob/master/1029_Two_City_Scheduling/1029_pulp.py","link":"/zh/2021/leetcode-1029-two-city-scheduling/"},{"title":"深入 LeetCode 470 拒绝采样，状态转移图求期望和一道经典统计求期望题目","text":"在这篇文章中，我们从一道LeetCode 470 题目出发，通过系统地思考，引出拒绝采样（Reject Sampling）的概念，并探索比较三种拒绝采样地解法；接着借助状态转移图来定量计算采样效率；最后，我们利用同样的方法来解一道稍微复杂些的经典抛硬币求期望的统计面试题目。 Leetcode 470 用 Rand7() 实现 Rand10() 已有方法 rand7 可生成 1 到 7 范围内的均匀随机整数，试写一个方法 rand10 生成 1 到 10 范围内的均匀随机整数。 不要使用系统的 Math.random() 方法。 思考 rand7()调用次数的 期望值 是多少 ? 你能否尽量少调用 rand7() ? 思维过程 我们已有 rand7() 等概率生成了 [1, 7] 中的数字，我们需要等概率生成 [1, 10] 范围内的数字。第一反应是调用一次rand7() 肯定是不够的，因为覆盖的范围不够。那么，就需要至少2次调用 rand7() 才能生成一次 rand10()，但是还要保证 [1, 10] 的数字生成概率相等，这个是难点。 现在我们先来考虑反问题，给定rand10() 生成 rand7()。这个应该很简单，调用一次 rand10() 得到 [1, 10]，如果是 8, 9, 10 ，则丢弃，重新开始，否则返回。想必大家都能想到这个朴素的方法，这种思想就是统计模拟中的拒绝采样（Reject Sampling）。 有了上面反问题的思考，我们可能会想到，rand7() 可以生成 rand5()，覆盖 [1, 5]的范围，如果将区间 [1, 10] 分成两个5个值的区间 [1, 5] 和 [6, 10]，那么 rand7() 可以通过先等概率选择区间 [1, 5] 或 [6, 10]，再通过rand7() 生成 rand5()就可以了。这个问题就等价于先用 rand7() 生成 rand2()，决定了 [1, 5] 还是 [6, 10]，再通过rand7() 生成 rand5() 。 解法一：rand2() + rand5() 我们来实现这种解法。下图为调用两次 rand7() 生成 rand10 数值的映射关系：横轴表示第一次调用，1，2，3决定选择区间 [1, 5] ，4，5，6选择区间 [6, 10]。灰色部分表示结果丢弃，重新开始 （注，若第一次得到7无需再次调用 rand7()）。 有了上图，我们很容易写出如下 AC 代码。 {linenos123456789101112131415# AC# Runtime: 408 ms, faster than 23.80% of Python3 online submissions for Implement Rand10() Using Rand7().# Memory Usage: 16.7 MB, less than 90.76% of Python3 online submissions for Implement Rand10() Using Rand7().class Solution: def rand10(self): while True: a = rand7() if a &lt;= 3: b = rand7() if b &lt;= 5: return b elif a &lt;= 6: b = rand7() if b &lt;= 5: return b + 5 标准解法：rand49() 从提交的结果来看，第一种解法慢于多数解法。原因是我们的调用 rand7() 的采样效率比较低，第一次有 1/7 的概率结果丢弃，第二次有 2/7的概率被丢弃。 如何在第一种解法的基础上提高采样效率呢？直觉告诉我们一种做法是降低上述 7x7 表格中灰色格子的面积。此时，会想到我们通过两次 rand7() 已经构建出来 rand49()了，那么再生成 rand10() 也规约成基本问题了。 下图为 rand49() 和 rand10() 的数字对应关系。 实现代码比较简单。注意，while True 可以去掉，用递归来代替。 {linenos1234567891011# AC# Runtime: 376 ms, faster than 54.71% of Python3 online submissions for Implement Rand10() Using Rand7().# Memory Usage: 16.9 MB, less than 38.54% of Python3 online submissions for Implement Rand10() Using Rand7().class Solution: def rand10(self): while True: a, b = rand7(), rand7() num = (a - 1) * 7 + b if num &lt;= 40: return num % 10 + 1 更快的做法 上面的提交结果发现标准解法在运行时间上有了不少提高，处于中等位置。我们继续思考，看看能否再提高采样效率。 观察发现，rand49() 有 9/49 的概率，生成的值被丢弃，原因是 [41, 49] 只有 9 个数，不足10个。倘若此时能够将这种状态保持下去，那么只需再调用一次 rand7() 而不是从新开始情况下至少调用两次 rand7()， 就可以得到 rand10()了。也就是说，当 rand49() 生成了 [41, 49] 范围内的数的话等价于我们先调用了一次 rand9()，那么依样画葫芦，我们接着调用 rand7() 得到了 rand63()。63 分成了6个10个值的区间后，剩余 3 个数。此时，又等价于 rand3()，循环往复，调用了 rand7() 得到了 rand21()，最后若rand21() 不幸得到21，等价于 rand1()，此时似乎我们走投无路，只能回到最初的状态，一切从头再来了。 改进算法代码如下。注意这次击败了 92.7%的提交。 {linenos1234567891011121314151617# AC# Runtime: 344 ms, faster than 92.72% of Python3 online submissions for Implement Rand10() Using Rand7().# Memory Usage: 16.7 MB, less than 90.76% of Python3 online submissions for Implement Rand10() Using Rand7().class Solution: def rand10(self): while True: a, b = rand7(), rand7() num = (a - 1) * 7 + b if num &lt;= 40: return num % 10 + 1 a = num - 40 b = rand7() num = (a - 1) * 7 + b if num &lt;= 60: return num % 10 + 1 a = num - 60 b = rand7() num = (a - 1) * 7 + b if num &lt;= 20: return num % 10 + 1 采样效率计算 通过代码提交的结果和大致的分析，我们已经知道三个解法在采样效率依次变得更快。现在我们来定量计算这三个解法。 我们考虑生成一个 rand10() 平均需要调用多少次 rand7()，作为采样效率的标准。 一种思路是可以通过模拟方法，即将上述每个解法模拟多次，然后用总的 rand7() 调用次数除以 rand10() 的生成次数即可。下面以解法三为例写出代码 {linenos123456789101112131415161718192021222324252627282930# The rand7() API is already defined for you.rand7_c = 0rand10_c = 0def rand7(): global rand7_c rand7_c += 1 import random return random.randint(1, 7) def rand10(): global rand10_c rand10_c += 1 while True: a, b = rand7(), rand7() num = (a - 1) * 7 + b if num &lt;= 40: return num % 10 + 1 a = num - 40 # [1, 9] b = rand7() num = (a - 1) * 7 + b # [1, 63] if num &lt;= 60: return num % 10 + 1 a = num - 60 # [1, 3] b = rand7() num = (a - 1) * 7 + b # [1, 21] if num &lt;= 20: return num % 10 + 1if __name__ == '__main__': while True: rand10() print(f'{rand10_c} {round(rand7_c/rand10_c, 2)}') 运行代码，发现解法三的采样效率稳定在 2.19。 采样效率精确计算 计算解法二 为了精确计算三个解法的采样效率，我们通过代码得到对应的状态转移图来帮助计算。 例如，解法一可以对应到下图：初始状态 Start 节点中的 +2 表示经过此节点会产生 2次 rand7() 的代价。从 Start 节点有 40/49 的概率到达被接受状态 AC，有 9/49 概率到达拒绝状态 REJ。REJ 需要从头开始，则用虚线表示重新回到 Start节点，也就是说 REJ 的代价等价于 Start。注意，从某个节点出发的所有边之和为1。 有了上述状态转移关系图，我们令初始状态的平均代价为 \\(C_2\\)，则可以写成递归表达式，因为其中 REJ 的代价就是 \\(C_2\\)，即 \\[ C_2 = 2 + (\\frac{40}{49}\\cdot0 + \\frac{9}{49} C_2) \\] 解得 \\(C_2\\) \\[ C_2 = 2.45 \\] 计算解法一 同样的，对于另外两种解法，虽然略微复杂，也可以用同样的方法求得。 解法一的状态转移图为 递归方程表达式为 \\[ C_1 = 1+\\frac{3}{7} \\cdot (1+\\frac{5}{7} \\cdot 0 + \\frac{2}{7} \\cdot C_1) \\cdot2+ \\frac{1}{7} \\cdot (C_1) \\] 解得 \\(C_1\\) \\[ C_1 = \\frac{91}{30} \\approx 3.03 \\] 计算解法三 最快的解法三状态转移图为 递归方程表达式为 \\[ C_3 = 2+\\frac{40}{49} \\cdot 0 + \\frac{9}{49} (1+\\frac{60}{63} \\cdot 0 + \\frac{3}{63} \\cdot (1+\\frac{20}{21} \\cdot 0 + \\frac{1}{21} \\cdot C_3)) \\] 解得 \\(C_3\\) \\[ C_3 = \\frac{329}{150} \\approx 2.193 \\] 至此总结一下，三个解法的平均代价为 \\[ C_1 \\approx 3.03 &gt; C_2 = 2.45 &gt; C_3 \\approx 2.193 \\] 这些值和我们通过模拟得到的结果一致。 稍难些的经典概率求期望题目 至此，LeetCode 470 我们已经分析透彻。现在，我们已经可以很熟练的将此类拒绝采样的问题转变成有概率的状态转移图，再写成递推公式去求平均采样的代价（即期望）。这里，如果大家感兴趣的话不妨再来看一道略微深入的经典统计概率求期望的题目。 问题：给定一枚抛正反面概率一样的硬币，求连续抛硬币直到两个正面（正面记为H，两个正面HH）的平均次数。例如：HTTHH是一个连续次数为5的第一次出现HH的序列。 分析问题画出状态转移图：我们令初始状态下得到第一个HH的平均长度记为 S，那么下一次抛硬币有 1/2 机会是 T，此时状态等价于初始状态，另有 1/2 机会是 H，我们记这个状态下第一次遇见HH的平均长度为 H（下图蓝色节点）。从此蓝色节点出发，当下一枚硬币是H则结束，是T是返回初始状态。于是构建出下图。 这个问题稍微复杂的地方在于我们有两个未知状态互相依赖，但问题的本质和方法是一样的，分别从 S 和 H 出发考虑状态的概率转移，可以写成如下两个方程式： \\[ \\left\\{ \\begin{array}{c} S =&amp;\\frac{1}{2} \\cdot(1+H) + \\frac{1}{2} \\cdot(1+S) \\\\ H =&amp;\\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot(1+S) \\end{array} \\right. \\] 解得 \\[ \\left\\{ \\begin{array}{c} H= 4 \\\\ S = 6 \\end{array} \\right. \\] 因此，平均下来，需要6次抛硬币才能得到 HH，这个是否和你直觉的猜测一致呢？ 这个问题还可以有另外一问，可以作为思考题让大家来练习一下：第一次得到 HT 的平均次数是多少？这个是否和 HH 一样呢？","link":"/zh/2021/leetcode-470-rand10/"},{"title":"Pytorch Geometric 系列教程1：互动可视化Graph数据集","text":"Pytorch Geometric 系列将结合 pytorch geometric 类库，从原理结合实践来讲解深度图神经网络。和前几期一样，这一系列所有环境已经为大家封装成了 docker image。预制的 image 中既包含了 pytorch 1.8 和对应的 geometric，networkx，Jubyter notebook 还有画图涉及到的 matplotlib 和 hvplot 类库。除此之外，也包含了预下载的 geometric dateset 和所用到的代码。方便大家一键开始 coding。 获取预制 Docker Image 获取docker image 环境的方式，小伙伴们可以通过关注 MyEncyclopedia 公众号并在公众号对话框中回复 docker-geometric 后获得 wechat_docker 关于具体使用我制作的 docker image 这个主题，大家可以再公众号或者同名 B站账号，找到 docker 封装工程依赖 一集， 其中包括如下高级用法 在 VS Code 中运行 Jupyter notebook Remote X Window 主机显示渲染图片 docker container 中启动 Jupyter notebook 服务器 共享文件保存文件改动 添加类库后保存镜像改动 bili-docker-adv 如果觉得下载镜像麻烦的小伙伴们，也可以自己准备对应的 python package，再拷贝下方链接的代码直接运行。 https://mydoc.myencyclopedia.top/pub/code/cora-plot 这个系列涉及到的 python package 包括 - torch-geometric networkx hvplot livelossplot 启动 Jupyter Notebook 当预制的 docker image 下载并装载完毕后，通过下面命令来启动 container 内置的 Jupyter notebook 服务器，这样可以使得主机种的浏览器访问到 container 中的服务器。 1docker run -p 8888:8888 -it env-conda-x jupyter notebook --allow-root --ip 0.0.0.0 内置 Cora 数据集 下一步，我们会通过 geometric 类库加载 cora 数据集。这一步通常来说需要从网上下载，但是预制的 docker image 已经为大家下载好了 planetoid 图数据集。Planetoid 包含了 Cora，Pubmed 和 Citeseer。 因此，加载数据集这一步执行非常快 123456789101112import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch_geometric.data import Datafrom torch_geometric.nn import GATConvfrom torch_geometric.datasets import Planetoidimport torch_geometric.transforms as Tname_data = 'Cora'dataset = Planetoid(root='./data/', name=name_data) 然后我们将 cora 转换成 networkx 格式。networkx 是 python 中一个比较流行的图类库。我们在后面 visualization 中也会利用networkx 的功能。 Cora 有 7 种节点类型，我们将每种节点类型赋予不同颜色，有助于更好 visualization。 12345678910from torch_geometric.utils import to_networkxcora = to_networkx(dataset.data)print(cora.is_directed())node_classes = dataset.data.y.data.numpy()print(node_classes)node_color = [\"red\",\"blue\",\"green\",\"yellow\",\"peru\",\"violet\",\"cyan\"] node_label = np.array(list(cora.nodes)) 接着，调用 networkx 的 spring_layout 计算每个节点的弹簧布局下的位置，这一步执行会比较耗时。 1234import matplotlib.pyplot as pltimport networkx as nxpos = nx.layout.spring_layout(cora) 我们首先来看一下 matplotlib 的渲染效果。 1234567891011plt.figure(figsize=(16,12))for i in np.arange(len(np.unique(node_classes))): node_list = node_label[node_classes == i] nx.draw_networkx_nodes(cora, pos, nodelist=list(node_list), node_size=50, node_color=node_color[i], alpha=0.8)nx.draw_networkx_edges(cora, pos,width=1,edge_color=\"black\")plt.show() 因为 matplotlib 只能画出一张静态图片， 无法做 interaction，也无法动态缩放。因此渲染效果不是特别好，尤其是对于 cora 这种数据量比较大的 graph 尤为显著。 我们看到图片种尽管有七种颜色的节点，但是当中存在的这块密集的点，我们很难看出节点和节点之间的关系。 我们换一个类库 hvplot，它的渲染和交换效果如下。 代码和 matplotlib 大致一致。注意渲染的时候 hvplot 需要将多个图片数据以乘法形式返回，借助 reduce 函数我们将 7 种节点的图相乘，再乘以描绘边的图，呈现出叠加的完整图片。 123456789101112131415161718import hvplot.networkx as hvnxoptions = { 'width': 800, 'height': 1000}plt_nodes = []for i in np.arange(len(np.unique(node_classes))): nodelist = node_label[node_classes == i] plt = hvnx.draw_networkx_nodes(cora, pos, nodelist=list(nodelist), node_color=node_color[i], **options) plt_nodes.append(plt)plt_edges = hvnx.draw_networkx_edges(cora, pos, arrowstyle='-&gt;', edge_width=2, colorbar=True, **options)import functoolsimport operatorplt_edges * functools.reduce(operator.mul, plt_nodes)","link":"/zh/2021/pytorch-geometric-01-plot-cora/"},{"title":"深度强化学习之：PPO训练红白机1942","text":"本篇是深度强化学习动手系列文章，自MyEncyclopedia公众号文章深度强化学习之：DQN训练超级玛丽闯关发布后收到不少关注和反馈，这一期，让我们实现目前主流深度强化学习算法PPO来打另一个红白机经典游戏1942。 相关文章链接如下： 强化学习开源环境集 视频论文解读：PPO算法 视频论文解读：组合优化的强化学习方法 解读TRPO论文，深度强化学习结合传统优化方法 解读深度强化学习基石论文：函数近似的策略梯度方法 NES 1942 环境安装 红白机游戏环境可以由OpenAI Retro来模拟，OpenAI Retro还在 Gym 集成了其他的经典游戏环境，包括Atari 2600，GBA，SNES等。 不过，受到版权原因，除了一些基本的rom，大部分游戏需要自行获取rom。 环境准备部分相关代码如下 1pip install gym-retro 1python -m retro.import /path/to/your/ROMs/directory/ OpenAI Gym 输入动作类型 在创建 retro 环境时，可以在retro.make中通过参数use_restricted_actions指定 action space，即按键的配置。 1env = retro.make(game='1942-Nes', use_restricted_actions=retro.Actions.FILTERED) 可选参数如下，FILTERED，DISCRETE和MULTI_DISCRETE 都可以指定过滤的动作，过滤动作需要通过配置文件加载。 12345678class Actions(Enum): \"\"\" Different settings for the action space of the environment \"\"\" ALL = 0 #: MultiBinary action space with no filtered actions FILTERED = 1 #: MultiBinary action space with invalid or not allowed actions filtered out DISCRETE = 2 #: Discrete action space for filtered actions MULTI_DISCRETE = 3 #: MultiDiscete action space for filtered actions DISCRETE和MULTI_DISCRETE 是 Gym 里的 Action概念，它们的基类都是gym.spaces.Space，可以通过 sample()方法采样，下面具体一一介绍。 Discrete：对应一维离散空间，例如，Discrete(n=4) 表示 [0, 3] 范围的整数。 123from gym.spaces import Discretespace = Discrete(4)print(space.sample()) 输出是 13 Box：对应多维连续空间，每一维的范围可以用 [low，high] 指定。 举例，Box(low=-1.0, high=2, shape=(3, 4,), dtype=np.float32) 表示 shape 是 [3, 4]，每个范围在 [-1, 2] 的float32型 tensor。 1234from gym.spaces import Boximport numpy as npspace = Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)print(space.sample()) 输出是 123[[-0.7538084 0.96901214 0.38641307 -0.05045208] [-0.85486996 1.3516271 0.3222616 1.2540635 ] [-0.29908678 -0.8970335 1.4869047 0.7007356 ]] MultiBinary: 0或1的多维离散空间。例如，MultiBinary([3,2]) 表示 shape 是3x2的0或1的tensor。 123from gym.spaces import MultiBinaryspace = MultiBinary([3,2])print(space.sample()) 输出是 123[[1 0] [1 1] [0 0]] MultiDiscrete：多维整型离散空间。例如，MultiDiscrete([5,2,2]) 表示三维Discrete空间，第一维范围在 [0-4]，第二，三维范围在[0-1]。 123from gym.spaces import MultiDiscretespace = MultiDiscrete([5,2,2])print(space.sample()) 输出是 1[2 1 0] Tuple：组合成 tuple 复合空间。举例来说，可以将 Box，Discrete，Discrete组成tuple 空间：Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(n=3), Discrete(n=2))) 1234from gym.spaces import *import numpy as npspace = Tuple(spaces=(Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(n=3), Discrete(n=2)))print(space.sample()) 输出是 12(array([ 0.22640526, 0.75286865, -0.6309239 ], dtype=float32), 0, 1) Dict：组合成有名字的复合空间。例如，Dict({'position':Discrete(2), 'velocity':Discrete(3)}) 123from gym.spaces import *space = Dict({'position':Discrete(2), 'velocity':Discrete(3)})print(space.sample()) 输出是 1OrderedDict([('position', 1), ('velocity', 1)]) NES 1942 动作空间配置 了解了 gym/retro 的动作空间，我们来看看1942的默认动作空间 12env = retro.make(game='1942-Nes')print(\"The size of action is: \", env.action_space.shape) 1The size of action is: (9,) 表示有9个 Discrete 动作，包括 start, select这些控制键。 从训练1942角度来说，我们希望指定最少的有效动作取得最好的成绩。根据经验，我们知道这个游戏最重要的键是4个方向加上 fire 键。限定游戏动作空间，官方的做法是在创建游戏环境时，指定预先生成的动作输入配置文件。但是这个方式相对麻烦，我们采用了直接指定按键的二进制表示来达到同样的目的，此时，需要设置 use_restricted_actions=retro.Actions.FILTERED。 下面的代码限制了6种按键，并随机play。 12345678910111213141516171819202122232425262728293031323334action_list = [ # No Operation [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], # Left [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], # Right [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], # Down [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], # Up [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], # B [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],]def random_play(env, action_list, sleep_seconds=0.01): env.viewer = None state = env.reset() score = 0 for j in range(10000): env.render() time.sleep(sleep_seconds) action = np.random.randint(len(action_list)) next_state, reward, done, _ = env.step(action_list[action]) state = next_state score += reward if done: print(\"Episode Score: \", score) env.reset() break env = retro.make(game='1942-Nes', use_restricted_actions=retro.Actions.FILTERED)random_play(env, action_list) 来看看其游戏效果，全随机死的还是比较快。 图像输入处理 一般对于通过屏幕像素作为输入的RL end-to-end训练来说，对图像做预处理很关键。因为原始图像较大，一方面我们希望能尽量压缩图像到比较小的tensor，另一方面又要保证关键信息不丢失，比如子弹的图像不能因为图片缩小而消失。另外的一个通用技巧是将多个连续的frame合并起来组成立体的frame，这样可以有效表示连贯动作。 下面的代码通过 pipeline 将游戏每帧原始图像从shape (224, 240, 3) 转换成 (4, 84, 84)，也就是原始的 width=224，height=240，rgb=3转换成 width=84，height=240，stack_size=4的黑白图像。具体 pipeline为 MaxAndSkipEnv：每两帧过滤一帧图像，减少数据量。 FrameDownSample：down sample 图像到指定小分辨率 84x84，并从彩色降到黑白。 FrameBuffer：合并连续的4帧，形成 (4, 84, 84) 的图像输入 1234567def build_env(): env = retro.make(game='1942-Nes', use_restricted_actions=retro.Actions.FILTERED) env = MaxAndSkipEnv(env, skip=2) env = FrameDownSample(env, (1, -1, -1, 1)) env = FrameBuffer(env, 4) env.seed(0) return env 观察图像维度变换 12345env = retro.make(game='1942-Nes', use_restricted_actions=retro.Actions.FILTERED)print(\"Initial shape: \", env.observation_space.shape)env = build_env(env)print(\"Processed shape: \", env.observation_space.shape) 确保shape 从 (224, 240, 3) 转换成 (4, 84, 84) 12Initial shape: (224, 240, 3)Processed shape: (4, 84, 84) FrameDownSample实现如下，我们使用了 cv2 类库来完成黑白化和图像缩放 123456789101112131415161718192021222324class FrameDownSample(ObservationWrapper): def __init__(self, env, exclude, width=84, height=84): super(FrameDownSample, self).__init__(env) self.exclude = exclude self.observation_space = Box(low=0, high=255, shape=(width, height, 1), dtype=np.uint8) self._width = width self._height = height def observation(self, observation): # convert image to gray scale screen = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY) # crop screen [up: down, left: right] screen = screen[self.exclude[0]:self.exclude[2], self.exclude[3]:self.exclude[1]] # to float, and normalized screen = np.ascontiguousarray(screen, dtype=np.float32) / 255 # resize image screen = cv2.resize(screen, (self._width, self._height), interpolation=cv2.INTER_AREA) return screen MaxAndSkipEnv，每两帧过滤一帧 1234567891011121314151617181920212223class MaxAndSkipEnv(Wrapper): def __init__(self, env=None, skip=4): super(MaxAndSkipEnv, self).__init__(env) self._obs_buffer = deque(maxlen=2) self._skip = skip def step(self, action): total_reward = 0.0 done = None for _ in range(self._skip): obs, reward, done, info = self.env.step(action) self._obs_buffer.append(obs) total_reward += reward if done: break max_frame = np.max(np.stack(self._obs_buffer), axis=0) return max_frame, total_reward, done, info def reset(self): self._obs_buffer.clear() obs = self.env.reset() self._obs_buffer.append(obs) return obs FrameBuffer，将最近的4帧合并起来 12345678910111213141516class FrameBuffer(ObservationWrapper): def __init__(self, env, num_steps, dtype=np.float32): super(FrameBuffer, self).__init__(env) obs_space = env.observation_space self._dtype = dtype self.observation_space = Box(low=0, high=255, shape=(num_steps, obs_space.shape[0], obs_space.shape[1]), dtype=self._dtype) def reset(self): frame = self.env.reset() self.buffer = np.stack(arrays=[frame, frame, frame, frame]) return self.buffer def observation(self, observation): self.buffer[:-1] = self.buffer[1:] self.buffer[-1] = observation return self.buffer 最后，visualize 处理后的图像，同样还是在随机play中，确保关键信息不丢失 123456789101112131415161718192021def random_play_preprocessed(env, action_list, sleep_seconds=0.01): import matplotlib.pyplot as plt env.viewer = None state = env.reset() score = 0 for j in range(10000): time.sleep(sleep_seconds) action = np.random.randint(len(action_list)) plt.imshow(state[-1], cmap=\"gray\") plt.title('Pre Processed image') plt.pause(sleep_seconds) next_state, reward, done, _ = env.step(action_list[action]) state = next_state score += reward if done: print(\"Episode Score: \", score) env.reset() break matplotlib 动画输出 CNN Actor &amp; Critic Actor 和 Critic 模型相同，输入是 (4, 84, 84) 的图像，输出是 [0, 5] 的action index。 12345678910111213141516171819202122232425262728class Actor(nn.Module): def __init__(self, input_shape, num_actions): super(Actor, self).__init__() self.input_shape = input_shape self.num_actions = num_actions self.features = nn.Sequential( nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(), nn.Conv2d(64, 64, kernel_size=3, stride=1), nn.ReLU() ) self.fc = nn.Sequential( nn.Linear(self.feature_size(), 512), nn.ReLU(), nn.Linear(512, self.num_actions), nn.Softmax(dim=1) ) def forward(self, x): x = self.features(x) x = x.view(x.size(0), -1) x = self.fc(x) dist = Categorical(x) return dist PPO核心代码 先计算 \\(r_t(\\theta)\\)，这里采用了一个技巧，对 \\(\\pi_\\theta\\) 取 log，相减再取 exp，这样可以增强数值稳定性。 1234dist = self.actor_net(state)new_log_probs = dist.log_prob(action)ratio = (new_log_probs - old_log_probs).exp()surr1 = ratio * advantage surr1 对应PPO论文中的 \\(L^{CPI}\\) 然后计算 surr2，对应 \\(L^{CLIP}\\) 中的 clip 部分，clip可以由 torch.clamp 函数实现。\\(L^{CLIP}\\) 则对应 actor_loss。 12surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantageactor_loss = - torch.min(surr1, surr2).mean() 最后，计算总的 loss \\(L_t^{CLIP+VF+S}\\)，包括 actor_loss，critic_loss 和 policy的 entropy。 1234entropy = dist.entropy().mean()critic_loss = (return_ - value).pow(2).mean()loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy 上述完整代码如下 1234567891011121314151617181920212223for _ in range(self.ppo_epoch): for state, action, old_log_probs, return_, advantage in sample_batch(): dist = self.actor_net(state) value = self.critic_net(state) entropy = dist.entropy().mean() new_log_probs = dist.log_prob(action) ratio = (new_log_probs - old_log_probs).exp() surr1 = ratio * advantage surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantage actor_loss = - torch.min(surr1, surr2).mean() critic_loss = (return_ - value).pow(2).mean() loss = actor_loss + 0.5 * critic_loss - 0.001 * entropy # Minimize the loss self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() loss.backward() self.actor_optimizer.step() self.critic_optimizer.step() 补充一下 GAE 的计算，advantage 根据公式 可以转换成如下代码 123456789def compute_gae(self, next_value): gae = 0 returns = [] values = self.values + [next_value] for step in reversed(range(len(self.rewards))): delta = self.rewards[step] + self.gamma * values[step + 1] * self.masks[step] - values[step] gae = delta + self.gamma * self.tau * self.masks[step] * gae returns.insert(0, gae + values[step]) return returns 外层 Training 代码 外层调用代码基于随机 play 的逻辑，agent.act()封装了采样和 forward prop，agent.step() 则封装了 backprop 和参数学习迭代的逻辑。 12345678910111213141516for i_episode in range(start_epoch + 1, n_episodes + 1): state = env.reset() score = 0 timestamp = 0 while timestamp &lt; 10000: action, log_prob, value = agent.act(state) next_state, reward, done, info = env.step(action_list[action]) score += reward timestamp += 1 agent.step(state, action, value, log_prob, reward, done, next_state) if done: break else: state = next_state 训练结果 让我们来看看学习的效果吧，注意我们的飞机学到了一些关键的技巧，躲避子弹；飞到角落尽快击毙敌机；一定程度预测敌机出现的位置并预先走到位置。","link":"/zh/2021/rl-ppo-1984/"},{"title":"MyEncyclopedia Arxiv Trending 发现服务","text":"获取服务地址 1. 最直接方式 在桌面版或者手机版浏览器中打开链接 https://app.myencyclopedia.top/paper 2. 微信扫码 3. 从公众号中获取 关注 MyEncyclopedia 公众号 在公众号对话中回复 ? 点击 AI在线服务 在微信浏览器中直接打开 或者获取链接 进入服务后需要首次使用需要登录，见文末附录部分。 开始搜索主题关键词 已最常见的 deep learning 为例，在搜索框中输入 deep learning 后稍等片刻，出现如下和其关联的词组对。 点击节点展开更多关联 由于和 deep learning 关联的概念词语较多，每次只会展现 10 个。单击节点可以加载更多关联词。 探索其他节点 双击节点增加至查询论文条件 查看综述 增加联合条件 登录流程 目前提供两种方式。 在国内的小伙伴建议使用微信公众号的方式登录，在国外的朋友们可以使用 Google 账号登录。 微信登录，按提示在为公众号中输入验证码 Google登录","link":"/zh/2021/service-arxiv/"},{"title":"MyEncyclopedia 重磅推出 mathpix 服务：图片转换成Latex源码","text":"大家一定都用过神奇的 mathpix 服务，它可以方便大家将数学公式图片转换成 Latex 数学公式。现在 MyEncyclopedia 也推出了手机版和桌面版的 mathpix 服务啦！ 使用流程 登录服务 1. 关注 MyEncyclopedia 公众号 2. 在公众号对话中回复 ? 3. 点击 AI在线服务 4. 在非微信浏览器中打开链接 5. 登录 目前提供两种方式。 在国内的小伙伴建议使用微信公众号的方式登录，在国外的朋友们可以使用 Google 账号登录。 5.1 微信登录，按提示在为公众号中输入验证码 5.2 Google登录 使用流程 点击右上方 + 选择图片 等待结果 Output Source 为转换的 Latex 源码 Output Render 为转换源码对应的渲染效果","link":"/zh/2021/service-math2latex/"},{"title":"[Harvard 2021] ECON 2355 - Unleashing Novel Data at Scale","text":"BiliBili 20:19:08 [Harvard 2021] ECON 2355 - Unleashing Novel Data at Scale 125 0 视频 MyEncyclopedia公号 Lecture 1 - Introduction to Unleashing Novel Data at Scale Why data accessibility is so central to the advancement of knowledge in economics (with some historical background) An overview of the data curation pipeline Step 1: Detect document layouts Step 2: OCR Step 3: Post-processing and database assembly Step 4: Convert information into computable format Why is the material covered in this course useful to social scientists? Why there won’t be an app/commercial product capable of end-to-end processing of social science documents anytime soon Why manual data entry often falls short Why our problems differ from those that are the central focus of computer science and the digital humanities At its core, deep learning is an optimization problem, which economists are well-trained to understand. It would be really unfortunate if we did not take full advantage of the very powerful methods that deep learning offers, which we are well poised to utilize Lecture 2 - Why Deep Learning? This post compares rule-based and deep learning-based approaches to data curation. It discusses their requirements and why rule-based approaches often (but not always) produce disappointing results when applied to social science data. An overview of the syllabus (ultimately, the course had a few deviations from the original syllabus, based on student interests; final syllabus posted in the course section of this website) There are two distinct approaches to automated data curation Tell the computer how to process the data by defining a set of rules Let the computer learn how to process the data from empirical examples, using deep learning Overview of rules, how they are used to process image scans and text, why they often fail, and why they sometimes succeed Deep learning, how it contrasts with rule-based approaches, and its requirements Does the noise from rule-based approaches really matter? Lecture 4 - Convolutional Neural Networks A brief overview of convolutions Benchmark datasets for image classification (following the ConvNent literature requires familiarity with the benchmarks) Image classification with a linear classifier (and its shortcomings) CNN architectures AlexNet VGG GoogLeNet ResNet ResNext Lecture 5 - Image Classification; Training Neural Nets This post covers two topics: using CNNs for image classification (a very useful task) and training neural networks in practice. Much of the information about training neural nets is essential to implementing deep learning-based approaches, whether with CNNs or with some other architecture. Image Classification Loss functions for classification SVM Softmax Deep document classification Training Neural Nets Activation functions Data pre-processing Initialization Optimization Regularization Batch normalization Dropout Data augmentation Transfer learning Setting hyperparameters Monitoring the learning process Lecture 6 - Other Computer Vision Problems (Including Object Detection) This post covers object detection as well as the related problems of semantic segmentation, localization, and instance segmentation. Object detection is core to document image analysis, as it is used to determine the coordinates and classes of different document layout regions. The other problems covered are closely related. Semantic segmentation Localization Object detection Region CNNs Fast R-CNN Faster R-CNN Mask R-CNN Features pyramids Instance segmentation Other frameworks (YOLO) Lecture 7 - Object Detection in Practice Selecting an object detection model Overview of Detectron2 How-to in D2 Lecture 8 - Labeling and Deep Visualization Labeling Active learning for layout annotation Labeling hacks Deep visualization Basic visualization approaches Gradient based ascent Deep Dream Lecture 9 - Generative Adversarial Networks Overview: supervised and unsupervised learning; generative models Generative adversarial networks CycleGAN Lecture 10 - OCR Architecture Overview of the OCR problem Recurrent neural networks LSTMs Connectionist temporal classification Putting it together Lecture 11 - OCR and Post-Processing in Practice This post discusses OCR, both off-the-shelf and how to implement a customized OCR model. It discusses how Layout Parser can be used for end-to-end document image analysis, and provides concrete examples of creating variable domains during post-processing. It also provides an overview of the second half of the knowledge base, which covers NLP. Off-the-shelf OCR Designing customized OCR Putting it altogether (and Layout Parser) Creating variable domains An overview of the second half of the course (NLP) Lecture 12 - Models of Words Traditional models of words Word2Vec GloVe Evaluation Interpreting word vectors Problems with word vectors Lecture 13 - Language Modeling and Other Topics in NLP This post provides an introduction to language modeling, as well as several other important topics: dependency parsing, named entity recognition (NER), and labeling for NLP. Due to time constraints, the course is able to provide only a very brief introduction to topics like dependency parsing and NER, which have traditionally been quite central questions in NLP research. Language Modeling Count based models Bag of words RNN (review) LSTM (review) Dependency parsing Named entity recognition Labeling for NLP Lecture 14 - Seq2Seq and Machine Translation Machine translation has pioneered some of the most productive innovations in neural-based NLP and hence is useful to study even for those who care little about machine translation per se. We will focus in particular on seq2seq and attention. Statistical machine translation Neural machine translation Lecture 15 - Attention is All You Need This post introduces the Transformer, a seq2seq model based entirely on attention that has transformed NLP. Given the importance of this paper, there are a bunch of very well-done web resources about it, cited in the lecture and below, that I recommend checking out directly (there are others who have much more of a comparative advantage in presenting seminal NLP papers than I do!). A recap of attention The Transformer The encoder Encoder self-attention Positional embeddings Add and normalize The decoder Encoder-decoder attention Decoder self-attention Linear and softmax layers Training Lecture 16 - Transformer-Based Language Models This post provides an overview of various Transformer-based language models, discussing their architectures and which are best-suited for different contexts. Overview Contextualized word embeddings Models GPT BERT RoBERTa DistilBERT ALBERT T5 GPT2/GPT3 Transformers XL XLNet Longformer BigBird Recap and what to use Lecture 17 - Understanding Transformers, Visualization, and Sentiment Analysis This post covers a variety of topics around Transformer-based language models: understanding how Transformer attention works, understanding what information is contained in their embeddings, visualizing embeddings, and using Transformer-based models to conduct sentiment analysis. What do Transformer-based models attend to? What’s in an embedding? Visualizing embeddings Sentiment analysis Lecture 18 - NLP with Noisy Text The Canonical Deep NLP Training Corpus A definition of noise The problem with noise Approaches for denoising Lecture 19 - Retrieval and Question Answering Reading comprehension Open-domain question answering Lecture 20 - Zero-Shot and Few-Shot Learning in NLP What it means to learn in just a few shots Zero-shot and few-shot learning in practice Lecture 21 - Transformers for Computer Vision Transformers for computer vision Transformers for image classification Transformers for object detection","link":"/zh/2021/share-harvard-ECON2355-Unleashing-Novel-Data-at-Scale/"},{"title":"公众号共享资源","text":"docker 环境，conda X Window 公众号回复 docker-conda-x docker 环境，Pytorch OpenNE 公众号回复 docker-openne 图神经网络 GCN Pytorch 版实现代码讲解 docker 环境，公众号回复 docker-pygcn 新书推荐&amp;课程视频网盘下载：强化学习与最优控制 公众号回复 adp 课程视频共享：Illinois大学 CS598 统计强化学习 B站链接 分享精品深度强化学习讲座 Berkeley Deep RL Bootcamp 2017 公众号回复 rl-bootcamp-ucb-2017","link":"/zh/2021/share-wechat-resources/"},{"title":"深入形象地理解极大似然估计(MLE) 1: 引入问题","text":"导读：极大似然估计(MLE) 是统计机器学习中最基本的概念，但是能真正全面深入地理解它的性质和背后和其他基本理论的关系不是件容易的事情。极大似然估计和以下概念都有着紧密的联系：随机变量，无偏性质（unbiasedness），一致估计（consistent），asymptotic normality，最优化（optimization），Fisher Information，MAP（最大后验估计），KL-Divergence，sufficient statistics等。在众多阐述 MLE 的文章或者课程中，总体来说都比较抽象，注重公式推导。本系列文章受3blue1brown 可视化教学的启发，坚持从第一性原理出发，通过数学原理结合模拟和动画，深入浅出地让读者理解极大似然估计。 相关链接 抛硬币问题 我们来思考这个老套问题，考虑手上有一枚硬币，旋转（抛）硬币得到正反面的概率固定（令正面概率为\\(\\theta^{\\star}\\)）但未知，我们如何能通过实验推测出 \\(\\theta^{\\star}\\) 朴素的想法是，不断尝试抛硬币，随着次数 n 的增多，正面的比例会趋近于 \\(\\theta^{\\star}\\) 对应到数学形式上，令我们对于 \\(\\theta^{\\star}\\) 的估计为 \\(\\hat{\\theta}_{n}\\)，则希望 \\[ \\hat{\\theta}_n = {n_{head} \\over n} \\to \\theta^{\\star} \\text{ as n } \\to \\infty \\] 模拟试验代码 假设我们尝试了n次，每次的结果为 \\(x_i\\)，\\(x_i\\)为1（正面） 或 0（反面）。比如试了三次的结果是 [1, 0, 1]，则 \\(x_1=1, x_2=0, x_3=1\\)。一般，我们将观察到的数据写成向量形式 \\[X=[x_1, x_2, x_3]^T=[1, 0, 1]^{T}\\] 我们知道硬币的正反结果符合伯努利分布，也就是 \\[ \\begin{align*} P_{ber}(x;\\theta) = \\left\\lbrace \\begin{array}{r@{}l} \\theta &amp;\\text{ if x=1} \\\\ 1-\\theta &amp;\\text{ if x=0} \\end{array} \\right. \\end{align*} \\] 因为 x 只有0，1两种取值，因此上式也可以写成等价如下的不含条件分支的形式 \\[ P_{ber} = \\theta^x \\cdot (1-\\theta)^x \\] 假设 \\(\\theta^{\\star} = 0.7\\)，如果做 n=10 次试验，结果应该比较接近7个1，3个0。 下面我们来模拟一下 n=10，看看结果如何。 下面代码的实现上我们直接使用了pytorch 内置的 bernoulli 函数生成 n 个随机变量实例 12345def gen_coins(theta, n=1): import torch theta_vec = torch.tensor(n*[theta]) random_values = torch.bernoulli(theta_vec) return random_values 让我们来做三次 n=10 的试验 123456for i in range(3): coins = gen_coins(theta=0.7, n=10) print(f'trial {i}') print(f'head #: {sum(coins)}') print(f'tail #: {sum(1-coins)}') print() 能发现 7个1，3个0 确实是比较可能的结果。 1234567891011trial 0head #: 7.0tail #: 3.0trial 1head #: 9.0tail #: 1.0trial 2head #: 7.0tail #: 3.0 生成概率 直觉告诉我们，当 \\(\\theta^{\\star}=0.7\\) 时，根据 $P_{ber}(x;) $，7个1，3个0 出现的概率应该是最大，6个1，4个0 或者 8个1，2个0 这两种情况出现概率稍小，其他的情况概率更小。通过基本概率和伯努利公式，重复 n 次试验 1和0出现的概率可以由下面公式算出。（注：7个1，3个0不是单一事件，需要乘以组合数算出实际概率） \\[ P_{X} = \\theta^{heads} \\cdot (1-\\theta)^{tails} \\cdot {n \\choose heads} \\] P(X) head=0 0.000006 head=1 0.000138 head=2 0.000032 head=3 0.001447 head=4 0.036757 head=5 0.102919 head=6 0.200121 head=7 0.266828 head=8 0.233474 head=9 0.121061 head=10 0.028248 画出图看的很明显，1出现7次的概率确实最大。 回到我们的问题，我们先假定 \\(\\theta^{\\star} = 0.7\\) 的硬币做 n=10 次试验的结果就是 7个1，3个0，或者具体序列为 [1, 0, 0, 1, 0, 1, 1, 1, 1, 1]。那么我们希望按照某种方法推测的估计值 \\(\\hat\\theta\\) 也为 0.7。 若将这个方法也记做 \\(\\hat\\theta\\)，它是\\(X\\) 的函数，即 \\(\\hat\\theta(X=[1, 0, 0, 1, 0, 1, 1, 1, 1, 1]^T)=0.7\\) 我们如何构建这个方法呢？很显然，\\(X\\) 中 1 的个数就可以胜任，\\(\\hat\\theta=\\bar X\\)。这个方式确实是正确的，后面的文章我们也会证明它是MLE在伯努利分布参数估计时的计算方法。 但是伯努利分布参数估计的问题中是最简单的情况，背后对应的更一般的问题是：假设我们知道某个过程或者实验生成了某种分布 P，但是不知道它的参数 \\(\\theta\\)，如何能通过反复的试验来推断 \\(\\theta\\)，同时，我们希望随着试验次数的增多，\\(\\hat\\theta\\) 能逼近 \\(\\theta\\)。 由于过程是有随机性，试验结果 \\(X\\) 并不能确定一定是从 \\(\\hat\\theta\\) 生成的，因此我们需要对所有 \\(\\theta\\) 打分。对于抛硬币试验来说，我们穷举所有在 [0, 1] 范围内的 \\(\\theta\\)，定义它的打分函数 \\(f(\\theta)\\)，并且希望我们定义的 \\(f(\\theta;X=[1, 0, 0, 1, 0, 1, 1, 1, 1, 1]^T)\\) 在 \\(\\theta=0.7\\) 时得分最高。推广到一般场景，有如下性质 \\[ f(\\theta^\\star;X) &gt;= f(\\theta;X) \\] 如此，我们将推测参数问题转换成了优化问题 \\[ \\hat\\theta = \\theta^{\\star} = \\operatorname{argmax}_{\\theta} f(\\theta; X) = 0.7 \\] 朴素方法 一种朴素的想法是，由于 \\(\\theta^\\star=0.7\\)，因此我们每次的结果应该稍微偏向 1，如果出现了 1，就记0.7分，出现了0，记0.3分，那么我们可以用10个结果的总分来定义总得分，即最大化函数 \\[ \\begin{equation*} \\begin{aligned} &amp;\\operatorname{argmax}_{\\theta} f(\\theta) \\\\ =&amp; \\operatorname{argmax}_{\\theta} P(x_1) + P(x_2) + ... + P(x_n) \\\\ =&amp; \\operatorname{argmax}_{\\theta} P(x_1|\\theta) + P(x_2|\\theta) + ... + P(x_n|\\theta) \\\\ =&amp; \\operatorname{argmax}_{\\theta} \\sum P(x_i|\\theta) \\\\ \\end{aligned} \\end{equation*} \\] 很可惜，我们定义的 f 并不符合 \\(\\theta=0.7\\) 时取到最大的原则。下面画出了 \\(\\theta\\) 在 [0, 1] 范围内 f 值，X 固定为 [1, 0, 0, 1, 0, 1, 1, 1, 1, 1]。显然，极值在 0.5 左右。 这种对于观察到的变量实例在整个参数空间打分的方法是最大似然方法的雏形。我们将每次试验结果对于不同 \\(\\theta\\) 的打分就是似然函数的概念。 伯努利似然函数（Likelihood) 伯努利单个结果的似然函数 \\(l(\\theta)\\) 视为 \\(\\theta\\) 的函数，x视为给定值，它等价于概率质量函数 PMF \\[ l(\\theta|x) = \\theta^x \\cdot (1-\\theta)^x \\] 极大似然估计(MLE) 有了单个结果的似然函数，我们如何定义 \\(f(\\theta)\\) 呢？我们定义的 \\(f(\\theta)\\) 需要满足，在 \\(\\theta^\\star=0.7\\) ，\\(n=10\\) 的情况下，试验最有可能的结果是 7 个1，3个0，此时 f 需要在 \\(\\theta=0.7\\) 时取到最大值。 极大似然估计(MLE) 为我们定义了合理的 \\(f(\\theta)\\) ，和朴素的想法类似，但是这次用单个结果的似然函数连乘而非连加 \\[ L(\\theta|X) = l(\\theta|x_1) \\cdot l(\\theta|x_2) \\cdot ...l(\\theta|x_n) = \\prod l(\\theta|x_i) \\] 我们再来看一下当 $X=[1, 0, 0, 1, 0, 1, 1, 1, 1, 1] $ 时 \\(L\\) 在 \\(\\theta\\) 空间的取值情况，果然，MLE 能在 0.7时取到最大值。 对数似然函数 最大似然函数 $_{} L() $ 能让我们找到最可能的 \\(\\theta\\)，但现实中，我们一般采用最大其 log 的形式。 \\[ \\begin{equation*} \\begin{aligned} &amp;\\operatorname{argmax}_{\\theta} \\log L(\\theta|X) \\\\ =&amp; \\operatorname{argmax}_{\\theta} \\log [l(\\theta|x_1) \\cdot l(\\theta|x_2) \\cdot ... \\cdot l(\\theta|x_n)] \\\\ =&amp; \\operatorname{argmax}_{\\theta} \\log l(\\theta|x_1) + \\log l(\\theta|x_2) \\cdot ... + \\log l(\\theta|x_n) \\end{aligned} \\end{equation*} \\] 理论能证明，最大对数似然函数得到的极值等价于最大似然函数。但这么做有什么额外好处呢？ 我们先将对数似然函数画出来 它的极大值也在 0.7，但是我们发现对数似然函数是个 concave 函数。在优化领域，最大化 concave 函数或者最小化 convex 函数可以有非常高效的解法。再仔细看之前的似然函数，它并不是一个 concave 函数。另一个非常重要的好处是，随着 n 的增大，连乘会导致浮点数 underflow，而单个点的对数似然函数的和的形式就不会有这个问题。 Pytorch MLE 代码 就让我们来实践一下，通过 pytorch 梯度上升来找到极值点。 1234567891011121314151617181920212223242526272829303132from stats.coin import gen_coinsfrom collections import dequedef train(num_head: int, num_tail: int) -&gt; float: import torch theta = torch.tensor(0.5, requires_grad=True) recent = deque(3*[100], maxlen=3) lr = 0.00001 for iter in range(2000): loss = -(num_head * torch.log(theta) + num_tail * torch.log(1 - theta)) loss.backward() with torch.no_grad(): theta -= lr * theta.grad # print(f'{iter}: {theta}, {theta.grad}') recent.append(theta.grad.item()) if all(map(lambda x: abs(x) &lt; 1, recent)): break theta.grad.zero_() return theta.item()if __name__ == '__main__': data = gen_coins(0.6, n=200) num_head = (data.detach() == 1).sum().item() num_tail = (data.detach() == 0).sum().item() print(num_head, num_tail) print(train(num_head, num_tail)) 一点需要说明的是，在迭代过程中，我们保存最后三个导数的值，当最新的三个导数都很小时就退出迭代。 1if all(map(lambda x: abs(x) &lt; 1, recent)) 运行代码，能发现最大化对数似然函数能很稳定的找到 \\(\\theta\\)。 现在大家对于伯努利MLE有了一定了解，接着，我们来思考一下最大化似然函数方法是否随着观察次数的增多能不断逼近真实的 \\(\\theta^\\star\\)呢？ MLE \\(\\theta\\) 估计的收敛性 \\(\\theta^\\star=0.7\\) 的情况下，我们来这样做试验，第一次做 n=1生成观察数据 \\(X_{1}\\)，第二次做 n=2生成观察数据 \\(X_{2}\\) \\[ X_1,X_2, X_3, ..., X_N \\] 对于每个数据集 \\(X_i\\) 通过最大似然方法求得估计的 \\(\\hat\\theta\\) \\[ \\hat\\theta_1=MLE(X_1), \\hat\\theta_2=MLE(X_2), ..., \\hat\\theta_N=MLE(X_N) \\] 将这些 \\(\\hat\\theta_i\\) 画出来，可以看到，随着 \\(n \\to \\infty\\)，\\(\\hat\\theta_i \\to \\theta^\\star=0.7\\) 换一个角度来看一下，我们将 \\(\\hat\\theta_i\\) 数列按照顺序，离散化后再归一化比例，如下图画出来，红色的柱代表了最新的值 \\(\\hat\\theta\\)。可以发现，初始时候，\\(\\hat\\theta\\) 在较远离 0.7 的地方出现，随着 n 的增大，出现的位置比较接近 0.7。 MLE \\(\\theta\\) 估计的偏差和方差 我们已经知道 MLE 方法可以通过观察数据推测出最有可能的 \\(\\hat\\theta\\)，由于观察数据 \\(X\\) 是伯努利过程产生的，具有随机性，那么 \\(\\hat\\theta\\) 可以看成是 \\(\\theta^\\star\\) 的随机变量。我们已经通过上面的试验知道随着试验次数的增大，我们的估计会越来越逼近真实值，现在的问题是对于固定的n，\\(\\hat\\theta\\) 的方差是多少，它的均值是否是无偏的呢？ 带着这样的疑问，我们现在做如下试验： 固定 n=10，重复做实验，画出随着次数增多 \\(\\hat\\theta\\) 的分布，见图中绿色部分。同样的，红色是 n=80 不断试验的分布变换。 看的出来，随着试验次数的增多 - \\(\\hat\\theta_{10}, \\hat\\theta_{80}\\) 都趋近于正态分布 \\(\\hat\\theta_{10}\\) 的分散度比 $ _{80}$ 要大，即方差要大 \\(\\hat\\theta_{10}, \\hat\\theta_{80}\\) 的均值都在 0.7","link":"/zh/2021/stat-ml-mle-1/"},{"title":"Paper Explained: MINE - Mutual Information Neural Estimation (2018)","text":"YouTube .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } BiliBili","link":"/zh/2021/vlog-paper-mine/"},{"title":"视频论文解读：组合优化的强化学习方法","text":"YouTube .embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; } BiliBili","link":"/zh/2021/vlog-paper-neural-combinatorial-optimization/"},{"title":"手机和微信中完美重排和阅读 Arxiv 论文","text":"如果你是一个重度论文阅读者，一定会在自己的笔记本上全副武装。但是也许只是偶尔，也会心血来潮地在手机端查看一篇论文，此时，你打开的这篇从 Arxiv 下载的论文，就像下图上半部分所示，时间稍长，便会觉得文字，公式和图片不忍直视。你多么希望在手机端查看PDF，也像图中下半部分一样重新排版来轻松阅读。 Arxiv 论文手机智能重排和阅读服务 现在，MyEncyclopedia 推出了智能的手机端重排和阅读 arxiv 论文服务。手机端只需浏览器甚至在微信中就能将指定的 arxiv 论文重排成适应移动端的格式，并在线浏览。 具体效果如下 微信操作步骤 下面，以手机微信为例，讲解详细步骤。其他浏览器可以直接点击 https://mydoc.myencyclopedia.top/app，操作步骤类似。 进入 MyEncyclopedia 公众号，回复 ? 点击 AI在线服务。 拷贝显示的口令 返回到公众号后回复口令，登录使用服务 输入 Arxiv ID，点击 Go to Page 在新页面点击 Convert Html 等待大约十几分钟，再次进入服务页面，此时，若下载转换任务成功，出现如下页面，点击View Html，即可在微信浏览器中完美查看论文了。 PDF 论文双列切割成单列服务 此外，也提供将 arxiv PDF从双列切割成单列服务。即上图中的Convert PDF 按钮，切割后的PDF文件可以在线浏览，或者下载到手机离线查看。也欢迎大家尝试。","link":"/zh/2022/app-arxiv-mobile/"},{"title":"","text":"如果你是一个重度论文阅读者，一定会在自己的笔记本上全副武装。但是也许只是偶尔，也会心血来潮地在手机端查看一篇论文，此时，你打开的这篇从 Arxiv 下载的论文，就像下图上半部分所示，时间稍长，便会觉得文字，公式和图片不忍直视。你多么希望在手机端查看PDF，也像图中下半部分一样重新排版来轻松阅读。 Arxiv 论文手机智能重排和阅读服务 现在，MyEncyclopedia 推出了智能的手机端重排和阅读 arxiv 论文服务。手机端只需浏览器甚至在微信中就能将指定的 arxiv 论文重排成适应移动端的格式，并在线浏览。 具体效果如下 微信操作步骤 下面，以手机微信为例，讲解详细步骤。其他浏览器可以直接点击 https://mydoc.myencyclopedia.top/app，操作步骤类似。 进入 MyEncyclopedia 公众号，回复 ? 点击 AI在线服务。 拷贝显示的口令 返回到公众号后回复口令，登录使用服务 输入 Arxiv ID，点击 Go to Page 在新页面点击 Convert Html 等待大约十几分钟，再次进入服务页面，此时，若下载转换任务成功，出现如下页面，点击View Html，即可在微信浏览器中完美查看论文了。 PDF 论文双列切割成单列服务 此外，也提供将 arxiv PDF从双列切割成单列服务。即上图中的Convert PDF 按钮，切割后的PDF文件可以在线浏览，或者下载到手机离线查看。也欢迎大家尝试。","link":"/zh/2022/app-arxiv-search/"},{"title":"油管搬运大学计划纪念B站千粉千赞","text":"五一期间突然发现我的B站频道一点一滴地攒到1000个粉丝和1000个获赞了，微不足道的小成就，和很多粉丝过万甚至十几万的大UP主差的很远，不过就自然增长而言，B站上的流量不会像公众号那样随着时间的推移而减少，只要原创内容精良，可以一直发光发热下去，这一点和 Youtube 很类似。 原创方向 对于未来的原创计划我是很有信心的，会在之前的基础上做好几个方向的内容创造 深度学习经典或前沿论文讲解，涉及到强化学习，NLP 和 CV 等通用方向。 可视化基础的数学课程，例如正在制作连载中的 MIT18.06线性代数，还有未来的统计，优化等课程，。同样的，也会在合适的时候开始基础机器学习经典教程，计划中有 Bishop 的 PRML (Pattern Recognition &amp; Machine Learning) 。这些经典教程都会精雕细琢，通过代码，尤其是 manim 可视化来完成。 机器学习实践系列：用 docker 封装各种机器学习或者深度学习的代码教学实例。 原创 AI 服务，例如目前更新中的 Mark-ME 等。 油管搬运大学 此外，平时我会经常从 Youtube 下载最新的各大名校的公开课。之后也会同时搬运一些，来源以 MIT，Stanford，Google，UC Berkeley 为主，课程类别不限于计算机，AI或者数学，以飨读者，同时我可以释放海量硬盘空间（家里已经有4台电脑，6个移动硬盘，总计20T的容量了 (lll￢ω￢)）。 上次搬运 Microsoft Research 的杰出AI学者炉边雅谈系列时实现了自动提供中英文字幕，效果如下（中文字幕由油管机翻，效果一般般，看看就行）。 英文字幕 中文字幕 奇怪的是好像我无法在手机端加载字幕，有知道的同学能否告知一下？","link":"/zh/2022/bili-youtube/"},{"title":"实战入门 faiss 搜索bert 最邻近句子：docker CPU镜像开箱即用，无需额外安装下载","text":"在这一期中，我们延续上一期 Bert 中文短句相似度计算 Docker CPU镜像，继续使用 huggingface transformer 和 sentence-transformer 类库，并将英语句子生成 bert embedding，然后引入 faiss 类库来建立索引，最后查询最接近的句子。 Docker 镜像获取方式 本期 docker 镜像获取方式为，关注 MyEncyclopedia 公众号后回复 docker-faiss-transformer 即可获取如下完整命令。 1docker run -p 8888:8888 myencyclopedia/faiss-demo bash -c 'jupyter notebook --allow-root --port 8888 --NotebookApp.token= --ip 0.0.0.0' 然后打开浏览器，输入 http://localhost:8888/notebooks/faiss_demo.ipynb faiss 简介 Faiss 的全称是Facebook AI Similarity Search，是由 Facebook 开发的适用于稠密向量匹配的开源库，作为向量化检索开山鼻祖，Faiss 提供了一套查询海量高维数据集的解决方案，它从两个方面改善了暴力搜索算法存在的问题：降低空间占用和加快检索速度。此外，Faiss 提供了若干种方法实现数据压缩，包括 PCA、Product-Quantization等。 Faiss 主要特性： 支持相似度检索和聚类； 支持多种索引方式； 支持CPU和GPU计算； 支持Python和C++调用； Faiss 使用流程 使用 faiss 分成两部，第一步需要对原始向量建立索引文件，第二步再对索引文件进行向量 search 操作。 在第一次建立索引文件的时候，需要经过 train 和 add 两个过程；后续如果有新的向量需要被添加到索引文件，只需要一个 add 操作来实现增量索引更新，但是如果增量的量级与原始索引差不多的话，整个向量空间就可能发生了一些变化，这个时候就需要重新建立整个索引文件，也就是再用全部的向量来走一遍 train 和 add，至于具体是如何 train 和 add的，就和特定的索引类型有关了。 1. IndexFlatL2 &amp; indexFlatIP 对于精确搜索，例如欧式距离 faiss.indexFlatL2 或 内积距离 faiss.indexFlatIP，没有 train 过程，add 完直接可以 search。 1234567891011121314151617import faiss # 建立索引, 定义为dimension d = 128index = faiss.IndexFlatL2(d) # add vectors, xb 为 (100000,128)大小的numpyindex.add(xb) print(index.ntotal) # 索引中向量的数量, 输出100000# 求4-近邻k = 4# xq为query embedding, 大小为(10000,128)D, I = index.search(xq, k) ## D shape (10000,4)，表示每个返回点的embedding 与 query embedding的距离,## I shape (10000,4)，表示和query embedding最接近的k个物品id，print(I[:5]) 2. IndexIVFFlat IndexFlatL2 的结果虽然精确，但当数据集比较大的时候，暴力搜索的时间复杂度很高，因此我们一般会使用其他方式的索引来加速。比如 IndexIVFFlat，将数据集在 train 阶段分割为几部分，技术术语为 Voronoi Cells，每个数据向量只能落在一个cell中。Search 时只需要查询query向量落在cell中的数据了，降低了距离计算次数。这个过程本质就是高维 KNN 聚类算法。search 阶段使用倒排索引来。 IndexIVFFlat 需要一个训练的阶段，其与另外一个索引 quantizer 有关，通过 quantizer 来判断属于哪个cell。IndexIVFFlat 在搜索阶段，引入了nlist(cell的数量)与nprob(执行搜索的cell数)参数。增大nprobe可以得到与brute-force更为接近的结果，nprobe就是速度与精度的调节器。 12345678910111213141516import faissnlist = 100k = 4# 建立索引, 定义为dimension d = 128quantizer = faiss.IndexFlatL2(d)# 使用欧式距离 L2 建立索引。index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)## xb: (100000,128)index.train(xb) index.add(xb) index.nprobe = 10 # 默认 nprobe 是 1 ,可以设置的大一些试试D, I = index.search(xq, k)print(I[-5:]) # 最后五次查询的结果 3. IndexIVFPQ IndexFlatL2 和 IndexIVFFlat都要存储所有的向量数据。对于超大规模数据集来说，可能会不大现实。因此IndexIVFPQ 索引可以用来压缩向量，具体的压缩算法就是 Product-Quantization，注意，由于高维向量被压缩，因此 search 时候返回也是近似的结果。 1234567891011121314import faissnlist = 100# 每个向量分8段m = 8 # 求4-近邻k = 4 quantizer = faiss.IndexFlatL2(d) # 内部的索引方式依然不变index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8) # 每个向量都被编码为8个字节大小index.train(xb)index.add(xb)index.nprobe = 10 D, I = index.search(xq, k) # 检索print(I[-5:]) 在本期中，我们仅使用基本的 IndexIVFFlat 和 IndexFlatIP 完成 bert embedding 的索引和搜索，后续会有篇幅来解读 Product-Quantization 的论文原理和代码实践。 ag_news 新闻数据集 ag_news 新闻数据集 3.0 包含了英语新闻标题，training 部分包含 120000条数据， test 部分包含 7600条数据。 ag_news 可以通过 huggingface datasets API 自动下载 12345678910def load_dataset(part='test') -&gt; List[str]: ds = datasets.load_dataset(\"ag_news\") list_str = [r['text'] for r in ds[part]] return list_str list_str = load_dataset(part='train')print(f'{len(list_str)}')for s in list_str[:3]: print(s) print('\\n') 显示前三条新闻标题为 123456789120000Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums. sentence-transformer 和上一期一样，我们利用sentence-transformer 生成句子级别的embedding。其原理基于 Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks （https://arxiv.org/abs/1908.10084）这篇论文。基本思想很直接，将句子中的每个词的 bert embedding ，输进入一个池化层(pooling)，例如选择最简单的平均池化层，将所有token embedding 的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence embedding。 结果展示 数据集 train 部分由于包含的样本比较多，需要一段时间生成 bert embedding，大家可以使用 load_dataset(part='test') 来快速体验。下面我们演示一个查询 how to make money 的最接近结果。 1234index = load_index('news_train.index')list_id = query(model, index, 'how to make money')for id in list_id: print(list_str[id]) 123456789Profit From That Traffic Ticket Got a traffic ticket? Can't beat 'em? Join 'em by investing in the company that processes those tickets.Answers in the Margins By just looking at operating margins, investors can find profitable industry leaders.Types of Investors: Which Are You? Learn a little about yourself, and it may improve your performance.Target Can Aim High Target can maintain its discount image while offering pricier services and merchandise.Finance moves Ford into the black US carmaker Ford Motor returns to profit, as the money it makes from lending to customers outweighs losses from selling vehicles. 核心代码 所有可运行代码和数据都已经包含在 docker 镜像中了，下面列出核心代码 建立索引 1234567891011121314151617181920def train_flat(index_name, id_list, embedding_list, num_clusters): import numpy as np import faiss dim = 768 m = 16 embeddings = np.asarray(embedding_list) quantiser = faiss.IndexFlatIP(dim) index = faiss.IndexIVFFlat(quantiser, dim, num_clusters, faiss.METRIC_INNER_PRODUCT) index.train(embeddings) ## clustering ids = np.arange(0, len(id_list)) ids = np.asarray(ids.astype('int64')) index.add_with_ids(embeddings, ids) print(index.is_trained) print(\"Total Number of Embeddings in the index\", index.ntotal) faiss.write_index(index, index_name) 查询结果 1234567def query(model, index, query_str: str) -&gt; List[int]: topk = 5 q_embed = model.encode([query_str]) D, I = index.search(q_embed, topk) print(D) print(I) return I[0].tolist()","link":"/zh/2022/docker-faiss-transformer/"},{"title":"玩转transformer+flair zero shot 短文本分类：无需翻墙或额外下载模型和数据集的CPU docker镜像","text":"在这一期中，我们来体验两个知名的 NLP 预训练类库 flair 和 transformer 的 zero-shot 短文本分类。所谓zero-shot 的意思是完全不需要数据集来训练，直接掉包解决问题。和以往一样，本期的 docker 镜像已经预装了 flair，transformer，pytorch，jupyter notebook等包依赖，并且还预先下载了 flair 和 transformer 的两个预训练模型和 yahoo 短文本主题数据集，整个 docker 镜像达到12GB，为了就是让大家无需翻墙下载额外数据或者模型，并且使用CPU就能体验最新的NLP zero shot 文本分类。 Docker 镜像获取方式 关注 MyEncyclopedia 公众号后回复 docker-transformer-zero-shot 即可获取镜像地址和启动命令。 Flair zero shot 先来看一个 flair 短文本 zero shot 短文本分类的例子。下面的代码将句子 Spain beat Swiss for first Nations League win 归类到 politics, sports，health 之一。 12345678910111213from flair.models import TARSClassifierfrom flair.data import Sentenceimport flair, torchflair.device = torch.device('cpu')text = 'Spain beat Swiss for first Nations League win'tars = TARSClassifier.load('tars-base')sentence = Sentence(text)classes = ['politics', 'sports', 'health']tars.predict_zero_shot(sentence, classes)print(sentence)print(sentence.to_dict()) 最后两行输出如下，all labels 字段显示概率最高的是 sports类别，达到 0.99+。 12345678910111213141516171819202122Sentence: \"Spain beat Swiss for first Nations League win\" → sports (0.9952){ 'text': 'Spain beat Swiss for first Nations League win', 'all labels': [{'value': 'sports', 'confidence': 0.9952359795570374}]}注意，在上面的代码中，`flair.device = torch.device('cpu')` 强制使用了 cpu 资源，否则 flair 默认使用 gpu 会报错。## Transformer zero shot再来看看大名鼎鼎的 transformer zero shot 的结果。这里使用了默认的 transformer zero shot 分类的模型 Transformer Bart，小伙伴们可以使用其他模型，但是有些不兼容 zero shot 分类。代码如下​```pythonfrom transformers import pipelinetext = 'Spain beat Swiss for first Nations League win'classes = ['politics', 'sports', 'health']classifier = pipeline(\"zero-shot-classification\", device=-1)result = classifier(text, classes, multi_label=False)print(result)print(result['labels'][0]) 最后两行输出为 123456{ 'sequence': 'Spain beat Swiss for first Nations League win', 'labels': ['sports', 'health', 'politics'], 'scores': [0.9476209878921509, 0.03594793379306793, 0.016431059688329697]}sports result 的 labels中会按照最大概率排序输出类别和对应的分数。对于这句句子，也分的相当准确，sports 为 0.94+。 也注意到 pipeline(\"zero-shot-classification\", device=-1) 语句中 -1 表示强制使用 cpu。 Yahoo 短文本主题数据分类效果 最后，来看一个真实数据集中这两者的实际效果，yahoo_answers_topics 是 huggingface的一个短文本分类数据集，可以通过以下命令下载并加载 1yahoo = load_dataset('yahoo_answers_topics') 它的具体类别为 123456789101112['Society &amp; Culture', 'Science &amp; Mathematics', 'Health', 'Education &amp; Reference', 'Computers &amp; Internet', 'Sports', 'Business &amp; Finance', 'Entertainment &amp; Music', 'Family &amp; Relationships', 'Politics &amp; Government'] 由于数量比较大，这里只取随机的1000个来测试，一些数据点如下 Text Topic A Permanent resident of Canada may stay out of Canada 3 years without losing status. Politics &amp; Government The official major league opening game occurred on April 10, 2006, as the Cardinals defeated the Milwaukee Brewers 6-4. (Day Game) Sports Hold down the Command key while dragging and dropping files. Computers &amp; Internet 接着，对于每条短文本用 flair 和 transformer 来预测类别，最终统计准确率。 结果是 flair 准确率为 0.275，Transformer Bart 为 0.392，果然 transformer 显著胜出。其实，在 Yahoo数据集上取得 0.3 - 0.4 左右的效果已经不错了，毕竟有十个类别，全随机的准确率是 0.1。如果大家觉得这个效果一般的话，可以试试 tweet 情感分类数据集（具体在下面的链接中），Transformer 能达到惊人的 0.73。 下面附部分代码，完整代码可以从镜像中获得，或者感兴趣的小伙伴也可以访问 https://github.com/nlptown/nlp-notebooks/blob/master/Zero-Shot%20Text%20Classification.ipynb 获取所有五个数据集的代码，不过由于类库版本的关系，部分代码和模型或数据无法兼容，需要自行调试。 12345678910111213141516171819def evaluate_flair(dataset, default_name='neutral'): classifier = TARSClassifier.load('tars-base') total, correct = 0, 0 for item, gold_label_idx in tqdm(zip(dataset[\"test_texts\"], dataset[\"test_labels\"]), total=len(dataset[\"test_texts\"])): sentence = Sentence(item) classifier.predict_zero_shot(sentence, dataset[\"class_names\"]) sorted_labels = sorted(sentence.to_dict()['all labels'], key=lambda k: k['confidence'], reverse=True) gold_label = dataset[\"class_names\"][gold_label_idx] if len(sorted_labels) &gt; 0: predicted_label = sorted_labels[0]['value'] else: predicted_label = default_name if predicted_label == gold_label: correct += 1 total += 1 return correct / total 1234567891011121314151617181920def evaluate_huggingface(dataset): classifier = pipeline(\"zero-shot-classification\", device=-1) correct = 0 predictions, gold_labels = [], [] for text, gold_label_idx in tqdm(zip(dataset[\"test_texts\"], dataset[\"test_labels\"]), total=len(dataset[\"test_texts\"])): result = classifier(text, dataset[\"class_names\"], multi_label=False) predicted_label = result['labels'][0] gold_label = dataset[\"class_names\"][gold_label_idx] predictions.append(predicted_label) gold_labels.append(gold_label) if predicted_label == gold_label: correct += 1 accuracy = correct / len(predictions) return accuracy","link":"/zh/2022/docker-flair-transformer-zero-shot/"},{"title":"一键即起开箱即用部署强悍中英文OCR本地服务","text":"PaddleOCR CPU 镜像 本期和大家分享一个一键就可以开箱集用的OCR docker 本地服务。 众所周知 PaddleOCR 是一个百度 PaddlePaddle 中非常有名的OCR框架，它包含了丰富的中英文模型。官方也提供给了 GPU 的 docker 镜像，但是基于 GPU 的镜像，对于很多小伙伴来说有点大材小用了。因为很多小伙伴不需要去训练自己的数据集和调参，只要用成熟的模型就足够了。另外，无论在 Windows 还是 Linux 中，大家配置安装 nvidia GPU 镜像都是比较麻烦。因此，这次特地为大家打造了这样一个 CPU 的 PaddleOCR 镜像，并且把常规的模型都预装到镜像中了，大家一键就能部署强悍的本地 OCR 服务。只需要安装 Docker 即可，也支持 Mac。 文字效果 先来看一个文字识别的效果吧，图片中这家店铺使用了非常规的中文字体，但PaddleOCR依然完全识别了出来。 获取镜像 具体获取方式为：关注 MyEncyclopedia 公众号，回复 docker-paddleocr 下面的命令一键起了这个镜像，注意，大家要将 me-paddleocr 替换成公众号中的镜像名。 1docker run -it me-paddleocr bash 进入容器后，我们已经在 /proj目录，通过 ls 可以发现有如下文件，三个图片demo输入文件，scene.png, engtest.jpg, me.png 和 demo_ch.py。 命令行识别 通过 PaddleOCR 内置的命令可以最快捷地识别文字，命令格式如下。 1paddleocr --image_dir scene.png --use_angle_cls true --lang ch --use_gpu false 输出为，最后两行为识别出的两处文字及其位置。 123456ppocr INFO: **********scene.png**********ppocr DEBUG: dt_boxes num : 2, elapse : 0.1184682846069336ppocr DEBUG: cls num : 2, elapse : 0.016251802444458008ppocr DEBUG: rec_res num : 2, elapse : 0.041384220123291016ppocr INFO: [[[152.0, 83.0], [402.0, 81.0], [402.0, 135.0], [153.0, 137.0]], ('匠牛饺子', 0.94130754)]ppocr INFO: [[[412.0, 183.0], [450.0, 183.0], [450.0, 206.0], [412.0, 206.0]], ('匠牛', 0.7368352)] 代码识别 当然了，为了方便查看结果，需要通过代码来实现。下面展示镜像内置的三个demo图片的执行python 代码 demo_ch.py 后的输出效果。 结果展示 图片一：门头照片 图片二：中英文混合的试卷 图片三：中英文混合Logo demo_ch.py 附上 demo_ch.py 源代码，不做赘述。 123456789101112131415161718192021222324252627282930313233from paddleocr import PaddleOCRimport numpy as npimport cv2import matplotlib.pyplot as pltfrom PIL import Image, ImageDraw, ImageFontdef cv2ImgAddText(img, text, left, top, textColor=(0, 0, 0), textSize=20): if (isinstance(img, np.ndarray)): img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) draw = ImageDraw.Draw(img) fontStyle = ImageFont.truetype('wqy-zenhei.ttc', textSize, encoding=\"utf-8\") draw.text((left, top), text, textColor, font=fontStyle) return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)ocr = PaddleOCR()img_path = '/proj/scene.png'result = ocr.ocr(img_path, rec=True)print(f\"The predicted text box of {img_path} are follows.\")image = cv2.imread(img_path)boxes = [[line[0][0], line[0][1], line[0][2], line[0][3]] for line in result]texts = [line[1][0] for line in result]for box, text in zip(boxes, texts): box = np.reshape(np.array(box), [-1, 1, 2]).astype(np.int64) print(box) image = cv2.polylines(np.array(image), [box], True, (255, 0, 0), 2) image = cv2ImgAddText(image, text, box[0][0][0] - 10, box[0][0][1] - 10, textColor=(0, 255, 0), textSize=20)plt.figure(figsize=(10, 10))plt.imshow(image)plt.savefig('/proj/result.jpg') 结束语 PaddleOCR最为市面上最好的开源中文 OCR 引擎之一，其强悍的效果可以达到开箱即用，本期基于 CPU 的预制镜像更降低了大家使用的门槛。本系列后续会和大家分享如何在 PaddleOCR GPU 镜像中去训练自己的数据集，来提升特定字体的准确度，大家喜欢的话不要忘记一键三连哦。同时也可以关注 MyEncyclopedia 微信公众号以及B站频道，下次再见。","link":"/zh/2022/docker-paddleocr/"},{"title":"Bert 中文短句相似度计算 Docker CPU镜像","text":"在这一期中，我们还是制作了一个集数据，模型，代码一体的 docker 环境，给大家开箱即用体验中文BERT句子embedding体验。具体地，我们基于 BERT-wwm-ext，huggingface transformer 和 sentence-transformer 把玩中文句子embedding 并寻找和查询短语相似度最接近的句子。 Docker 镜像获取方式 本期 docker 镜像获取方式为，关注 MyEncyclopedia 公众号后回复 docker-sentence-transformer 即可获取镜像地址和启动命令。 哈工大讯飞中文 Bert 在中文预训练领域，哈工大讯飞联合实验室（HFL）发布的基于全词Mask的中文预训练模型 BERT-wwm-ext 是业界的标杆之一。BERT-wwm-ext 支持 Tensorflow, Pytorch （通过 huggingface transformer 接口）以及 PaddleHub 的接口或者类库，使用起来十分方便。下面的代码为官网中通过 huggingface transformer 接口直接下载并加载到 Pytorch 平台中。Github 地址为 https://github.com/ymcui/Chinese-BERT-wwm 12345from transformers import BertTokenizer, BertModelmodel_name = 'hfl/chinese-bert-wwm'tokenizer = BertTokenizer.from_pretrained(model_name)model = BertModel.from_pretrained(model_name) 通过 huggingface transformer 的好处在于 sentence-transformer 也支持 huggingface，因此，通过 huggingface，我们无需手动串联 BERT-wwm-ext 和 sentence-transformer，少写了不少代码。 sentence-transformer sentence-transformer 顾名思义是利用 transformer 词向量的预训练模型来生成句子级别的embedding。原理基于这篇论文 Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks （https://arxiv.org/abs/1908.10084）。基本思想直接了当：将句子中的每个词经 bert embedding 后，输入池化层 (pooling)，例如选择最简单的平均池化层，再将所有token embedding 的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence embedding。 下面的代码是其官网的一个基本例子，底层通过 huggingface 接口自动下载并加载 bert 词向量，并计算三句英语句子的 sentence embedding。 12345678910111213141516from sentence_transformers import SentenceTransformermodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')#Our sentences we like to encodesentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']#Sentences are encoded by calling model.encode()embeddings = model.encode(sentences)#Print the embeddingsfor sentence, embedding in zip(sentences, embeddings): print(\"Sentence:\", sentence) print(\"Embedding:\", embedding) print(\"\") 当然，我们也可以绕过 sentence-transformer API，直接使用 pytorch API 和 huggingface 手动实现平均池化层，生成句子的 sentence embedding。 1234567891011121314151617181920212223242526272829from transformers import AutoTokenizer, AutoModelimport torch#Mean Pooling - Take attention mask into account for correct averagingdef mean_pooling(model_output, attention_mask): token_embeddings = model_output[0] #First element of model_output contains all token embeddings input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float() sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) return sum_embeddings / sum_mask#Sentences we want sentence embeddings forsentences = ['This framework generates embeddings for each input sentence', 'Sentences are passed as a list of string.', 'The quick brown fox jumps over the lazy dog.']#Load AutoModel from huggingface model repositorytokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")#Tokenize sentencesencoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')#Compute token embeddingswith torch.no_grad(): model_output = model(**encoded_input)#Perform pooling. In this case, mean poolingsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask']) 中文最相近的句子 有了上面每个组件的使用方法，让我们生成下面中文句子的embedding 1234567891011121314sentences = [ '今天晚上想吃牛排', 'MyEncyclopedia公众号全栈人工智能', '人工智能需要懂很多数学么', '上海疫情有完没完', '教育部：连续7天社会面无疫情 高校可组织校园招聘', '福建舰\"下水！100秒看中国航母高光时刻', '医保承担多少核酸检测费用？压力多大？', '张家口过度防疫整改后又被曝光：要证明牛是阴性', '上海多家银行天天排队爆满 有老人凌晨2点开始排队', 'A股不惧海外暴跌！走出独立行情沪指收复3300点', '俄方称已准备好重启俄乌和谈', '《自然》：奥密克戎感染后嗅觉丧失症状比原来少了'] 接着我们给出如下三个短语的查询，找到和每个查询最匹配的三个句子 123q1 = '码农的春天来了么'q2 = '国际局势'q3 = '健康' 运行结果如下 12345678910111213141516171819202122Query: 码农的春天来了么Top 3 most similar sentences in corpus:人工智能需要懂很多数学么 (Cosine Score: 0.7606)MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.7498)上海疫情有完没完 (Cosine Score: 0.7449)----------------------------------------------Query: 国际局势Top 3 most similar sentences in corpus:俄方称已准备好重启俄乌和谈 (Cosine Score: 0.7041)MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.6897)上海疫情有完没完 (Cosine Score: 0.6888)----------------------------------------------Query: 健康Top 3 most similar sentences in corpus:上海疫情有完没完 (Cosine Score: 0.5882)MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.5870)今天晚上想吃牛排 (Cosine Score: 0.5815) 结果发现 上海疫情有完没完 是一切问题的关键。。。 完整代码 附上完整代码 123456789101112131415161718192021222324252627282930313233343536373839from sentence_transformers import SentenceTransformermodel_name = 'hfl/chinese-bert-wwm'model = SentenceTransformer(model_name)sentences = [ '今天晚上想吃牛排', 'MyEncyclopedia公众号全栈人工智能', '人工智能需要懂很多数学么', '上海疫情有完没完', '教育部：连续7天社会面无疫情 高校可组织校园招聘', '福建舰\"下水！100秒看中国航母高光时刻', '医保承担多少核酸检测费用？压力多大？', '张家口过度防疫整改后又被曝光：要证明牛是阴性', '上海多家银行天天排队爆满 有老人凌晨2点开始排队', 'A股不惧海外暴跌！走出独立行情沪指收复3300点', '俄方称已准备好重启俄乌和谈', '《自然》：奥密克戎感染后嗅觉丧失症状比原来少了']sentence_embeddings = model.encode(sentences)q1 = '码农的春天来了么'q2 = '国际局势'q3 = '健康'queries = [q1, q2, q3]query_embeddings = model.encode(queries)import scipynumber_top_matches = 3for query, query_embedding in zip(queries, query_embeddings): distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, \"cosine\")[0] results = zip(range(len(distances)), distances) results = sorted(results, key=lambda x: x[1]) print(\"\\nQuery:\", query) print(\"\\nTop {} most similar sentences in corpus:\".format(number_top_matches)) for idx, distance in results[0:number_top_matches]: print(sentences[idx].strip(), \"(Cosine Score: %.4f)\" % (1-distance))","link":"/zh/2022/docker-sentence-transformer-chinese/"},{"title":"Strang MIT 18.06 线性代数精髓01：矩阵乘法的五种理解","text":"在 MIT Gilbert Strang 教授经典的线性代数课程第三节中详细解释了矩阵相乘的5种方法及其理解，本文用彩色的 Latex 公式呈现出来，便于大家回顾复习。 01 - 矩阵乘法的五种理解 02 - 解方程组的意义和过程 假定 \\(A \\times B =C\\) 中， A 是 \\(m\\) 行 \\(n\\) 列的矩阵， \\(B\\) 是 \\(n\\) 行 \\(p\\) 列的矩阵，\\(C\\) 为 \\(m\\) 行 \\(p\\) 列矩阵： \\[ A_{m \\cdot n} \\times B_{n \\cdot p} = C_{m \\cdot p} \\] B矩阵的列组合：列列切分 这是最经典的理解方式，沿袭了第一部分 \\(A \\vec x= \\vec b\\) 的方式。 回顾可以将 \\(A \\vec x\\) 视为 \\(A\\) 的列向量关于每个 \\(x\\) 分量的线性组合。 那么 \\(A B\\) 相乘可以理解为将矩阵 \\(B\\) 按列切分成列向量，即 \\[ B = [\\vec B_{col_1} \\vec B_{col_2} \\cdots \\vec B_{col_p} ] \\] 如此，结果矩阵的第 \\(j\\) 列就是 \\(A \\vec B_{col_j}\\)：\\(A\\) 的列向量关于 $ B_{col_j} $ 的线性组合。 由于我们将 \\(A\\) 和 \\(B\\) 都按列来切，这种方式可以助记成 列列 切分。 A矩阵行组合：行行切分 同样的，对应与 \\(A\\) 右乘向量等同于\\(A\\)列的组合，\\(A\\) 左乘行向量等同于 \\(A\\) 行的组合： \\[ [x_1, x_2, ..., x_n]\\begin{bmatrix} A_{row_1} \\\\ A_{row_2} \\\\ \\vdots \\\\A_{row_n} \\end{bmatrix} = x_1 \\cdot A_{row_1} + x_2 \\cdot A_{row_2} +...+ x_n \\cdot A_{row_n} \\] 其结果是一个行向量。 那么 \\(A B\\) 相乘可以理解为将矩阵 \\(A\\) 按行切分成行向量，即 \\[ A = \\begin{bmatrix} A_{row_1} \\\\ A_{row_2}\\\\ \\vdots \\\\ A_{row_m} \\end{bmatrix} \\] 如此，结果矩阵的第 \\(i\\) 行就是 \\(\\vec A_{row_i} B\\)：\\(B\\) 的行向量关于 $ A_{row_i} $ 的线性组合。 这种方式可以助记成 行行 切分。 A行 x B列 点乘：行列切分 如 \\(A\\) 矩阵按行切，\\(B\\) 矩阵按列切，即住记成 行列 切分时如下所示。 \\[ \\begin{eqnarray*} &amp;&amp; A_{m \\cdot n} \\times B_{n \\cdot p} \\\\ &amp;=&amp; \\begin{bmatrix} A_{row_1} \\\\ A_{row_2} \\\\ \\vdots \\\\ A_{row_i} \\\\ \\vdots \\\\ \\end{bmatrix}_{m \\cdot n} \\begin{bmatrix} B_{col_1} &amp; B_{col_2} &amp; \\cdots &amp; B_{col_p} \\end{bmatrix}_{n \\cdot p} \\\\ &amp;=&amp; \\begin{bmatrix} &amp;\\vdots&amp;\\\\&amp; A_{row_i}&amp;\\\\&amp;\\vdots&amp; \\end{bmatrix}_{m \\cdot n} \\begin{bmatrix} &amp;&amp;\\\\\\cdots&amp; B_{col_j} &amp; \\cdots\\\\&amp;&amp; \\end{bmatrix}_{n \\cdot p} \\\\ &amp;=&amp; \\begin{bmatrix} &amp;\\vdots&amp;\\\\\\cdots&amp;c_{ij}&amp;\\cdots\\\\&amp; \\vdots \\end{bmatrix}_{m \\cdot p} \\end{eqnarray*} \\] 行乘以列即列向量点乘，结果是一个标量。因此 \\(c_{ij}\\) 为结果矩阵 \\(C\\) 的第 \\(i\\) 行 \\(j\\) 列的值。 \\[ c_{ij}=A_{row_i} \\cdot B_{col_j} = \\begin{bmatrix} a_{i1} &amp; a_{i2} &amp; \\cdots &amp; a_{in} \\end{bmatrix} \\begin{bmatrix} b_{1j} \\\\ b_{2j} \\\\ \\vdots \\\\ b_{nj} \\end{bmatrix} =\\sum_{k=i}^na_{ik}b_{kj} \\] A列 x B行 矩阵和：列行切分 最后，也可也按列行来切分。注意列乘以行时的结果是一个矩阵。 分块相乘 第五种方式是分块相乘，可以认为是点乘理解下的扩展。","link":"/zh/2022/linear-algebra-strang-01-matrix-product/"},{"title":"Strang MIT 18.06 线性代数精髓 02：解方程组的意义和过程","text":"本文继续以彩色Latex 方式总结了方程组的行视角，列视角的几何意义；并回顾了解方程的两个步骤：消元和回代。内容对应于MIT 18.06 Gilbert Strang 线性代数视频课程第一，二节。 本系列链接如下 01 - 矩阵乘法的五种理解 02 - 解方程组的意义和过程 方程组两种几何解释 二元方程组 来看一个具体的二元线性方程组 \\[ \\begin{cases} 2x&amp;-y&amp;=0 \\\\ -x&amp;+2y&amp;=3 \\end{cases} \\] 写成矩阵形式 \\[ \\begin{bmatrix} 2&amp;-1 \\\\ -1&amp;2 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix} \\] 行视角 回顾 $2x - y = 0 $ 为所有满足此条件的 (x, y) 的集合，即集合组成二维平面的一条直线，如下图蓝线所示。 $ -x + 2y = 3$ 则对应绿线。 因此方程组的解 x = 1, y = 2 为两条直线的交点，这就是方程组的行视角：将系数矩阵按行切分，则每一行表示一个约束条件，几何意义是N维空间的一个子空间。在二元方程中，一行表示一条线，三元方程中，一行表示一个平面（详见后一小节）。 \\[ \\begin{bmatrix} \\color{blue} 2 x &amp; {\\color{blue} -1 y} \\\\ \\color{green} -1 x &amp; \\color{green} 2 y \\end{bmatrix} = \\begin{bmatrix} \\color{blue} 0 \\\\ \\color{green} 3 \\end{bmatrix} \\] 列视角 若将系数矩阵按列切分，则每一列表示一个向量，方程组的解 (x, y) 表示每个列向量以 x, y 为权重的线性组合刚好形成 b 向量。这个就是方程组 \\(Ax=b\\) 有解的条件：b 在 A 的列空间，此时，x 为 列向量的组合系数。 \\[ x \\begin{bmatrix} \\color{Green} 2 \\\\ \\color{Green} -1 \\end{bmatrix} + y \\begin{bmatrix} \\color{Turquoise} -1 \\\\ \\color{Turquoise} 2 \\end{bmatrix} = \\begin{bmatrix} \\color{Red} 0\\\\ \\color{Red} 3 \\end{bmatrix} \\] 三元方程组 行视角 对于三元方程组行视角来说，每一行的方程组成一个三维空间中的一个平面。解是三个平面的交点，通常来说为一个点。 \\[ \\begin{bmatrix} \\color{magenta} m_{11} &amp; \\color{magenta} m_{12} &amp; \\color{magenta} m_{13} \\\\ \\color{cyan} m_{21} &amp; \\color{cyan} m_{22} &amp; \\color{cyan} m_{23} \\\\ \\color{green} m_{31} &amp; \\color{green} m_{32} &amp; \\color{green} m_{33} \\\\ \\end{bmatrix} \\begin{bmatrix} w_{1} \\\\ w_{2} \\\\ w_{3} \\end{bmatrix}= \\begin{bmatrix} \\color{magenta} y_{1} \\\\ \\color{cyan} y_{2} \\\\ \\color{green} y_{3} \\end{bmatrix} \\] 图片来自 Introduction to Linear Algebra for Applied Machine Learning with Python，解为一直线而非一个点。 列视角 三元列视角下，A 的列向量为三维平面的一个向量，x（下图为 w） 表示每个列向量取多少倍数可以组成 b 向量。 \\[ \\color{cyan} w_1 \\color{black} \\begin{bmatrix} \\color{cyan} m_{11} \\\\ \\color{cyan} m_{21} \\\\ \\color{cyan} m_{31} \\end{bmatrix}+ \\color{magenta} w_2 \\color{black} \\begin{bmatrix} \\color{magenta} m_{12} \\\\ \\color{magenta} m_{22} \\\\ \\color{magenta} m_{32} \\end{bmatrix}+ \\color{green} w_3 \\color{black} \\begin{bmatrix} \\color{green} m_{13} \\\\ \\color{green} m_{23} \\\\ \\color{green} m_{33} \\end{bmatrix}= \\begin{bmatrix} \\color{red} y_{1} \\\\ \\color{red} y_{2} \\\\ \\color{red} y_{3} \\end{bmatrix} \\] 解方程的步骤 总结了二元三元方程组的行和列视角后，我们回顾求解方程的具体步骤。用两个过程，消元和回代便可以解得方程。 消元的目的是将系数矩阵表示成变量依次依赖的上矩阵形式 (Upper Triangular Matrix)。 \\[ \\begin{bmatrix} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} \\\\ \\end{bmatrix} \\underrightarrow{Eliminate} \\begin{bmatrix} \\color{red} a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} \\\\ \\color{gray} 0 &amp; \\color{red} a'_{22} &amp; \\cdots &amp; a'_{2n} \\\\ \\color{gray} 0 &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\color{gray} 0 &amp; \\color{gray} 0 &amp; \\color{gray} 0 &amp; \\color{red} a'_{nn} \\\\ \\end{bmatrix} \\] 回代则在上矩阵的基础上依次解得每个分量的值。 三元方程示例 举个三元方程组为例 \\[ \\begin{cases} x&amp;+2y&amp;+z&amp;=2 \\\\ 3x&amp;+8y&amp;+z&amp;=12 \\\\ &amp;+4y&amp;+z&amp;=2 \\end{cases} \\] 写成矩阵形式为 \\[ \\begin{bmatrix}1&amp;2&amp;1\\\\3&amp;8&amp;1\\\\0&amp;4&amp;1\\end{bmatrix}\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}2\\\\12\\\\2\\end{bmatrix} \\] 消元过程 在消元过程中，有两类操作，一是将上一行乘以某系数后被下一行减去，依次消除这一行的元。第二类操作是交换当前行和后面某行，行交换用于当某行对应的元已经为0的情况下。 以上述三元方程为例，第二行消元的具体过程为第二行减去3倍的第一行。接着，再进行第三行消元。 \\[ \\left[\\begin{array}{c|c}A&amp;b\\end{array}\\right] = \\left[\\begin{array}{ccc|c} \\color{red} 1&amp;2&amp;1&amp;2 \\\\ 3&amp;8&amp;1&amp;12 \\\\ 0&amp;4&amp;1&amp;2 \\end{array}\\right] \\xrightarrow{row_2-3row_1} \\left[\\begin{array}{ccc|c} \\color{red} 1&amp;2&amp;1&amp;2 \\\\ \\color{blue} \\boxed{0} &amp; \\color{red} 2&amp; \\color{blue} -2&amp; \\color{blue}6 \\\\ 0&amp;4&amp;1&amp;2 \\end{array}\\right] \\xrightarrow{row_3-2row_2} \\left[\\begin{array}{ccc|c} \\color{red}1&amp;2&amp;1&amp;2 \\\\ 0&amp; \\color{red} 2&amp;-2&amp;6 \\\\ \\color{blue} 0 &amp; \\color{blue} \\boxed{0} &amp; \\color{red} 5&amp; \\color{blue}-10 \\end{array}\\right] \\] 最终，上矩阵为 \\[ U=\\begin{bmatrix} \\color{red} 1 &amp; 2&amp; 1 \\\\ \\color{gray} 0 &amp; \\color{red} 2 &amp; -2 \\\\ \\color{gray} 0 &amp; \\color{gray} 0&amp; \\color{red} 5 \\end{bmatrix} \\] 回代过程 回代过程比较直白，由上矩阵 U 对应方程组 \\[ \\begin{cases} \\color{red} x&amp;+2y&amp;+z&amp;=2 \\\\ &amp; \\color{red} +2y &amp;-2z&amp;=6 \\\\ &amp;&amp; \\color{red} +5z&amp;=-10 \\end{cases} \\] 自下向上，容易解得 \\[ \\begin{cases} \\color{red}x \\color{black}= 2 \\\\ \\color{red}y \\color{black}= 1 \\\\ \\color{red}z \\color{black}= -2 \\end{cases} \\] 消元的行视角意义 注意到消元时的两类操作都不改变系数矩阵的行空间，只是改变了行空间的线性组合方式。 由于每一行代表一个拘束子空间，因此每一次消元改变了该行的拘束子空间。 举个例子，对于二元方程组和行空间 \\[ \\begin{bmatrix} \\color{blue} 2 x &amp; \\color{blue} -1 y \\\\ \\color{green} -1 x &amp; \\color{green} 2 y \\end{bmatrix} = \\begin{bmatrix} \\color{blue} 0 \\\\ \\color{green} 3 \\end{bmatrix} \\] 对应了两条直线 第二行的 x 消除后其几何意义为：蓝色直线不变，绿色直线从包含 x 的成分变成不含 x 成分，并且维持交点 （1, 2）不变。 \\[ \\begin{bmatrix} \\color{blue} 2 x &amp; \\color{blue} -1 y \\\\ &amp; \\color{green} {3 \\over 2} y \\end{bmatrix} = \\begin{bmatrix} \\color{blue} 0 \\\\ \\color{green} 3 \\end{bmatrix} \\] 最后还有一个问题可以思考一下：消元对于列视角的几何意义是什么呢？","link":"/zh/2022/linear-algebra-strang-02-solving-equations/"},{"title":"Strang MIT 18.06 线性代数精髓 03：消元的矩阵表示，矩阵的逆","text":"本系列用彩色的 Latex 公式呈现总结 MIT Gilbert Strang 教授经典的线性代数课程18.06之精华，便于大家回顾复习。 本文在矩阵乘法和解方程组的基础上引入消元对应的矩阵乘法和矩阵的逆。 消元操作的矩阵表示 回顾上一节，解方程组的意义和过程 中，在消元步骤时，将矩阵 A 依行顺序向下，每一行通过减去上面行的若干倍，将矩阵 A 逐行规范成上三角矩阵形式。 那么每一行规范的操作，例如操作 \\(E_{21}\\) （第二行减去三倍第一行）是否可以通过矩阵乘法表示出来呢？ 结合矩阵乘法的五种理解一节中行行切分，我们将 \\(E_{21}\\) 视为三个行相加。 如此，第一个矩阵 \\([1 \\quad 0 \\quad 0]\\) 乘以 \\(A\\) 形成了乘积矩阵的第一行，具体来说， \\([1 \\quad 0 \\quad 0]\\) 表示取 \\(A\\) 的1个第一行，0个第二行，0个第三行的和组成乘积矩阵第一行。 同样地，乘积矩阵第二行由第二个矩阵 \\([-3 \\quad 1 \\quad 0]\\) 形成，表示取 \\(A\\) 的-3个第一行，1个第二行，0个第三行之和。 具体转换如下 至此，操作 \\(E_{21}\\) 可以用矩阵 \\(E_{21}\\) 右乘以 \\(A\\) 来表示，即 上述矩阵乘法可以用简单地 Python 代码来核实 1234567891011121314import numpy as npA = np.array([ [1., 2., 1.], [3., 8., 1.], [0., 4., 1.]])e_21 = np.array([ [1., 0., 0.], [-3., 1., 0.], [0.,0.,1.]])e_21 @ A 运行结果为 123array([[ 1., 2., 1.], [ 0., 2., -2.], [ 0., 4., 1.]]) 将 \\(A\\) 矩阵消元的第二个操作是 \\(E_{32}\\)，第三行减去2倍第二行，这个操作依样画葫芦可以写成 将 \\(E_{32}\\) 右乘以上面的中间结果可以得到最后的上三角矩阵形式，完成消元步骤。 即 \\(A\\) 在两次左乘相应矩阵后变成上三角矩阵 \\(U\\) 由于乘法结合律，可以将两次操作 \\(E_{32}, E_{21}\\) 视为统一的操作 \\(E\\) 注意上式中， \\(E_{32},E_{21}\\) 为下三角矩阵。 我们再用代码来验证下矩阵乘法结合律 12345678910111213141516171819import numpy as npe_32 = np.array([ [1., 0., 0.], [0., 1., 0.], [0., -2., 1.]])e_21 = np.array([ [1., 0., 0.], [-3., 1., 0.], [0.,0.,1.]])A = np.array([ [1., 2., 1.], [3., 8., 1.], [0., 4., 1.]]) 12e_32 @ e_21 @ Ae_32 @ (e_21 @ A) 结果都为 123array([[ 1., 2., 1.], [ 0., 2., -2.], [ 0., 0., 5.]]) 因此，矩阵 \\(A\\) 的消元过程可以用一个矩阵 \\(E\\) 表示出来。 矩阵的逆 以基本行操作 \\(E_{21}^{-1}\\) 为例，此变换是从第二行中减去三倍的第一行 那么其逆变换就是给第二行加上三倍的第一行，所以逆矩阵就是 \\(E_{32}^{-1}\\) 也是同样 我们已经知道这两个行变换的综合效果为 \\(E\\) 那么 \\(E^{-1}\\) 和 \\(E_{32}^{-1},E_{21}^{-1}\\) 有什么关系呢？ 发现综合结果的逆以相反的顺序相乘，这个也容易理解，因为逆操作将一个上三角矩阵 \\(U\\) 恢复成 \\(A\\)，这个过程是自下而上的行变换。 最后，来核实 \\(E E^{-1} = I\\)。","link":"/zh/2022/linear-algebra-strang-03-eliminating-matrix-and-inverse/"},{"title":"Strang MIT 18.06 线性代数精髓 04：Gauss-Jordan 求矩阵逆","text":"本系列用彩色的 Latex 公式呈现总结 MIT Gilbert Strang 教授经典的线性代数课程18.06之精华，便于大家回顾复习。 Gauss-Jordan 算法是一种手工计算矩阵逆的技术。现在，你几乎不应该手动计算矩阵逆，即使是在计算机上，但 Gauss-Jordan 仍然有用，因为 它有助于我们理解逆矩阵何时以及为何存在。 它给了我们另一个例子来帮助我们理解消元的结构 回顾：矩阵的逆 线性算子的 \\(A^{-1}\\) 是“撤消”操作的操作 \\(A\\) ，对于任何 \\(x\\)，有: 即 \\[\\boxed{Ax=b \\implies x = A^{-1} b}\\] 这意味着 对于m×m 方阵 \\(A\\)， \\(A^{-1}\\) 存在仅当 \\(A\\) 具有 m 个主元的 因为对于非方阵或具有一个或多个“零主元”的矩阵，我们不能总是解决 \\(Ax=b\\) （会在反向替换期间除以零）。也很容易看出 \\(\\boxed{(A^{-1})^{-1} = A}\\) , 即 \\(A\\) 撤消操作 \\(A^{-1}\\) . 等价地 这里， \\(I\\) 是 m×m 单位矩阵——在线性代数中，我们通常从上下文来推断 \\(I\\) 的形状，但如果它是模棱两可时，可以写成 \\(I_m\\). 乘积的逆： (AB)⁻¹ = B⁻¹A⁻¹ 很容易看出乘积 \\(AB\\) 的逆是两者逆的反向乘积： \\[\\boxed{(AB)^{-1} = B^{-1} A^{-1}}\\] 直观理解为，当反转一系列操作时，总是需要以倒序的顺序回溯操作。 容易证明这个结论，因为 例如，我们看到高斯消元对应于因式分解 \\(A = LU\\)， 此时，\\(U\\) 是消除的结果，并且 \\(L\\) 只是消除步骤的记录。 然后 注意：逆允许我们“除以矩阵”，但我们总是要清楚我们是在左边还是右边。以下符号很方便，可用于 Julia 和 Matlab 中 通过线性方程求逆 方程 \\(A A^{-1} = I\\) 实际上给了我们计算的算法 \\(A^{-1}\\) . 假设我们将 \\(A\\) 以及 \\(I\\) 表示成列的形式，即 那么 这里的关键事实是，将 \\(A\\) 乘以右侧的矩阵相当于将 \\(A\\) 乘以该矩阵的每一列，通过写出计算可以很容易地看到这一点。 如此，有了 m 个方程组，第 k 个方程组对应 \\(A x_k = e_k\\)，解得这个方程组就可以得到 \\(A^{-1}\\) 的第 k 列 。也就是说，要找到 m×m 矩阵 \\(A\\) 的 \\(A^{-1}\\)，我们必须求解 m 个 \\(Ax=b\\)。 换句话说，对于任何矩阵 \\(B\\) , \\(Be_k\\) 为 \\(B\\) 的第 k 列。 所以 \\(A^{-1}\\) 第 k 列是 \\(x_k = A^{-1} e_k\\) ，即 \\(Ax_k = e_k\\) 的解 理想情况下，当我们进行一次高斯消元 \\(A=LU\\) 后，就可以计算出 \\(x_k = U^{-1} L^{-1} e_k\\) ，做法是通过在 \\(I\\) 的每一列上做向前和向后替换（这本质上是计算机的做法）。 举例计算 L⁻¹ = E 例如，我们如何计算我们在上一课中从高斯消元中得到的 \\(L\\) 矩阵的逆矩阵，我们知道，结果应该是 \\(L^{-1} = E\\) 对于每个 \\(e_1,e_2,e_3\\)，我们解对应方程组 \\(e_k\\) 是 3×3 \\(I\\) 的第 k 列。 例如对于 \\(e_1\\), 找到第一列 \\(x_1\\) 的 \\(L^{-1} = E\\): 通过前向替换（从上到下），我们解得 The Gauss–Jordan 算法 Gauss-Jordan 可以被视为求解的一种技巧（主要用于手工计算）\\(A b_k = e_k\\). 但是用代数的方式思考也很好——这是我们高斯消元的“矩阵观点”的一个很好的应用。 简而言之，Gauss-Jordan 的想法是：如果我们对 \\(A\\) 执行一些行操作以获得 \\(I\\)，那么对 \\(I\\) 执行相同的行操作会得到 \\(A^{-1}\\)。为什么？ 行操作对应于从 \\(A\\) 左边乘以一组矩阵 \\(E=\\cdots E_2 E_1\\) 所以，对 \\(A\\) 做行操作将其变成 \\(I\\) 意思等价于 \\(EA = I\\)， 因此 \\(E = A^{-1}\\) . 对 \\(I\\) 执行相同的行操作，相当于 \\(I\\) 左乘矩阵 \\(E\\) , 即 \\(EI\\)，因为 \\(EI = E\\) 并且 \\(E = A^{-1}\\)，所以结果就是 \\(A^{-1}\\) 这就是我们可以用扩展矩阵来进行高斯消除，对 \\(A\\) 和 \\(I\\) 同时执行相同的行操作，即 \\(A \\to I\\) 消除步骤 下面是具体将 \\(A\\) 通过行操作变换成 \\(I\\) 的步骤 做普通高斯消元“向下”将 \\(A\\) 转变成 \\(U\\) （上三角矩阵）。 然后，做高斯消去“向上” \\(U\\) 消除对角线以上的元素，将 \\(U\\) 转换成对角矩阵 \\(D\\) 最后，将 \\(D\\) 的每一行除以对角线元素值，得到 \\(I\\) Gauss-Jordan 示例 让我们执行这些 \\(A \\to I\\) 消除步骤 \\(3 \\times 3\\) 矩阵 \\(A\\) : 先向下消除获得 \\(U\\) , 然后向上消除得到 \\(D\\) ，最后除以对角线元素值得到 \\(I\\): 没问题！很容易看出，只要 \\(A\\) 具有所有主元（即它是非奇异的），这将起作用。 永远不要计算矩阵逆！ 矩阵逆很有趣，它在分析操作中非常方便，因为它们允许您轻松地将矩阵从方程的一侧移动到另一侧。但是，在“严肃的”数值计算中几乎从不计算逆矩阵。每当你看到 \\(A^{-1} B\\) 或者 \\(A^{-1} b\\)，当您在计算机上实现它时，您应该阅读 \\(A^{-1} B\\) 作为“解决 \\(AX = B\\) 通过某种方法。”例如，通过 \\(A \\backslash B\\) 或通过首先计算 \\(LU\\) 分解来解决它 \\(A\\) 然后用它来解决 \\(AX = B\\) . 你通常不计算逆矩阵的一个原因是它很浪费：一旦你有 \\(A=LU\\)（稍后我们将把它概括为“\\(PA = LU\\)\")，你可以解决 \\(AX=B\\) 直接不用找 \\(A^{-1}\\), 和计算 \\(A^{-1}\\) 如果您只需要解决几个右手边，则需要更多的工作。 另一个原因是，对于很多特殊的矩阵，有办法解决 \\(AX=B\\) 比你能找到的要快得多 \\(A^{-1}\\). 例如，实践中的许多大矩阵是稀疏的（大部分为零），通常对于稀疏矩阵，您可以安排 \\(L\\) 和 \\(U\\) 也要稀疏。稀疏矩阵比一般的“密集”矩阵更有效，因为您不必乘以（甚至存储）零。即使 \\(A\\) 是稀疏的，然而， \\(A^{-1}\\) 通常是非稀疏的，因此如果计算逆矩阵，则会失去稀疏的特殊效率。 例如： 如果你看到 \\(U^{-1} b\\) 在哪里 \\(U\\) 是上三角，不用计算 \\(U^{-1}\\) 明确！只要解决 \\(Ux = b\\) 通过反向替换（从底行向上）。 如果你看到 \\(L^{-1} b\\) 在哪里 \\(L\\) 是下三角，不用计算 \\(L^{-1}\\) 明确！只要解决 \\(Lx = b\\) 通过前向替换（从顶行向下）。","link":"/zh/2022/linear-algebra-strang-04-gauss-jordan/"},{"title":"Markme 0.3.0 发布：支持图片转latex，微信公众号排版，上传下载图片","text":"Mark-ME 在几个月的开发之后，发布了 v0.3.0 版本。Mark-ME 致力于做最好的跨平台（Win, Mac, Linux）markdown 编辑器，面向AI辅助写作，一站式多平台发布。超越 Typora，mdnice 等。本版本增加了不少高级的功能，包括 导出兼容 mdnice 样式的微信公众号文章 mathpix 图片转 latex 代码 通过 PicGo 上传本地图片到图床 下载文字远程图片到本地 下载地址： https://github.com/MyEncyclopedia/Mark-ME/releases/download/v0.3.0/Mark-ME.0.3.0.exe Markdown 显示效果 1. 图片显示 注意，本地和远程图片都支持 2. 代码高亮 3. 数学公式 除了普通行内公式和块公式，还支持 Typora， mdnice 不支持的扩展 Mathjax 表达式。例如图中的彩色矩阵。 同样的表达式在 Typora 和 mdnice 中均不支持 4. 内嵌 Html 标签 完美显示内嵌 html 标签。 注意，第二个例子中内嵌了 svg 的 div 标签，渲染结果为 \\(x^2\\)。这个功能很方便将 latex 导出成 svg 后嵌入 markdown 文章中，即保护了 latex 源码，也无需担心多平台的显示效果。后续会加强此功能。 兼容 mdnice 样式导出到微信公众号 将 mdnice 的样式填入右边的输入中，完美换皮。 Mathpix 图片转 Latex 下载远程图片到本地 通过 PicGo 上传本地图片到图床 其他 菜单展示 缩放 配置窗口 注册 一些高级功能虽然免费，但是需要用户注册 myencyclopedia，以便更好的了解用户。","link":"/zh/2022/markme-0.3.0/"},{"title":"程序员的纯代数视频和教材指南","text":"在入门机器人视觉和机器人运动后，开始逐步接触到了3D计算机视觉中的高阶数学概念，包括三维物体到二维图片的变换（术语称之为射影几何）；三维欧氏空间的物体运动坐标系变换，分为主动变换（active transformations）和被动变换（passive transformation）；另外在更高阶的计算机渲染中常会用到Mesh和黎曼曲面；此外，几何深度学习（Geometric Deep Learning）中也涉及到群论，李群等。这些迷之概念使得我对于本科高等数学课程（多元微积分，线性代数，概率论）后面的纯数学感到兴趣。本来一直觉得纯数学会非常难学，但是当我写了很多年代码和阅读了多个AI领域的众多论文之后，总有一些本质问题萦绕在心，得不到解释： 代码抽象的极限在哪里？高阶纯函数编程？ 深度学习多个领域（CV，NLP，RL，GNN）是否有统一视角？目前深度学习的论文汗牛充栋，既有很多相似的pattern，也有每个领域的特性，是否有通用设计原则 第一个问题在范畴论（category theory）中广泛探讨，不过范畴论过于深奥，是抽象代数后的一门纯函数编程的理论基础。第二个问题在几何深度学习中被探讨。不过，这两个问题肯定不会被完全回答，但是，即使无法得到最好的解释和回答，就如同统计学中频率派和贝叶斯派各有所长，没有公认的一家通吃，但是对于这些根本问题的探究的阶段性成功也是非常有意义的。正如理查德·费曼的名言： I would rather have questions that can't be answered than answers that can't be questioned 鉴于AI从业人员乃至程序员进阶掌握高阶数学的武器，我斗胆总结一份从我这个非数学系的理工毕业生角度出发的自学纯数学指南。以下所有课程视频都已经搬运到了我的B站频道（MyEncyclopedia公号）中，此外，视频课程有对应教材的也一并列出。 在学习这些知识时候，发现至今的联系是非常广泛的，这个倒是非常出乎我意料，也让学习非常有成就感。以下总结了若干贯穿所有领域的重要概念。 群的最一般概念和离散数学群 群论体系下构建的抽象代数结构 有限维向量空间的线性映射和基变换，并且此视角下的矩阵（线性代数）理解 从基变换的角度理解傅里叶变换，即傅里叶变换是函数表示的特殊的基。 有限维向量空间上定义了距离（内积空间）后，向量空间有了几何结构。 从线性映射推广到多变量线性映射（multilinear map），这是张量（Tensor）的数学意义 从离散数学群论到连续函数映射，形成了微分流形（Differential Manifold）的概念。在此基础上进而发展出了李群，本质是为了解决例如旋转矩阵群虽然在乘法上封闭（因此组成群）但是在加法上不封闭的问题。 一般群论扩展出范畴论，这是函数式编程的数学原理。 学习顺序 下图为我推荐的课程学习顺序。 当对张量和李群掌握充分后，原则上可以更上一层楼，学习广义相对论和量子力学。不过我并没有计划涉足这两个艰深的领域，也就不多言了。 1234567891011121314graph TD; map[高等线性代数] --&gt;abs; map[高等线性代数] --&gt;T[Tensor]; group[群论]--&gt;abs[抽象代数]; abs--&gt;C[范畴论]; map--&gt;F; F--&gt;W[小波变换]; F--&gt;FA[泛函分析]; abs--&gt;F[傅里叶变换]; T--&gt;m[Manifold]; m--&gt;L[李群]; T--&gt;L; L--&gt;G[广义相对论]; L--&gt;Q[量子力学]; 123456789graph TD; map[高等线性代数] --&gt;T[Tensor]; T--&gt;m[Manifold]; m--&gt;L[李群]; T--&gt;L; T--&gt;G; T--&gt;Q; L--&gt;G[广义相对论]; L--&gt;Q[量子力学]; 线性映射下的高等线性代数 【Sheldon Axler】线性代数应该这样学 B站链接 https://www.bilibili.com/video/BV1Dm4y1c7Cn Sheldon Axler Linear Algebra Done Right (线性代数这样学) 原作者亲自讲解 评价 本课程的教师是配套同名教材线性代数应该这样学（Linear Algebra Done Right）这本书的作者。课程和教材都是从线性映射(linear map)角度来看待矩阵。由最基本的向量空间和内积空间定义出发导出对偶空间（Dual Space），特征值，特征根，对角矩阵，正交基，直到最核心的谱论（Spectral theorem）。 此课程说实话不适宜第一次学线性代数的同学。同时，不适合数学系的，因为跟高代相比太浅；不适合工程类的，因为太深而且没有跟应用联系起来。但是，本书却广受赞誉，中文也有翻译本。其最大的特点是通过空间变换和矩阵联系起来，从最基本的定义导出矩阵中的最核心概念：特征值，特征根，对角矩阵，正交基等。 配套教材 【Nathaniel Johnston】 Advanced Linear Algebra B站链接 https://www.bilibili.com/video/BV1uV4y157XE Nathaniel Johnston Advanced Linear Algebra 评价 课程关于线性映射背后的基变换等讲述的非常清楚。不过对比Nathaniel Johnston的教材Advanced Linear Algebra and Matrix Algebra来说，视频课程只涉及到教材的一二章的内容，但并没有涉及到第三章Tensor的部分，有点可惜。视频包含很多向量空间和线性映射的例子和证明，可以作为线性代数应该这样学的很好补充。同时推荐教材第三章Tensor，适合作为Tensor概念的入门理论学习。 配套教材 【Matthew Macauley】Advanced Linear Algebra Math B站链接 https://www.bilibili.com/video/BV1Me4y1a7Uk Advanced Linear Algebra Math 4120 Modern Algebra, Matthew Macauley @ Clemson 评价 Matthew Macauley教授讲课节奏条理都很赞，示例也比较充足，他的多个数学课程都很棒，尤其是下面群论部分的Visual Group Theory最为经典。本课程可以作为上述两门自成体系的课程很好的补充，尤其是Tensor作为multilinear map的部分。课程对应教材为Peter Lax的经典老教材线性代数及其应用。 配套教材 群论，抽象代数 【Matthew Macauley】可视化群论 B站链接 https://www.bilibili.com/video/BV1ka411378X Visual Group Theory Math 4120 Modern Algebra, Matthew Macauley @ Clemson 配套教材 评价 课程基于Nathan Carter负有盛名的Visual Group Theory 这本非常非常棒的群论入门书。视频课程本身在教材的基础上补充了不少例子和定理证明，使抽象难以理解的群里严重降低了入门难度。我作为同时仔细看过视频和教材的读者，觉得视频课程有以下几个优势：首先是建立概念时一步一步带你walk through，比如最核心的群同态概念，使得概念的建立更加深刻形象；其次课程补充了不少代数方法的严格证明，有的时候，代数方法比视觉方法更显示了问题的本质和联系。强烈推荐视频和教材，因为群论是现代数学的基石。 【哈佛Benedict Gross】抽象代数 B站链接 https://www.bilibili.com/video/BV1Yt4y1F72S 【Harvard MATH E-222】 Abstract Algebra (Benedict Gross) 评价 很老的课程了，对应的教材为经典的 Michael Artin 的代数。学完本视频和配套教材会对抽象代数有扎实深入的理解。 配套教材 张量 eigenchris 系列 B站链接 【eigenchris】 Tensors for Beginners https://www.bilibili.com/video/BV1zW4y1n7Cy 【eigenchris】 Tensor Calculus https://www.bilibili.com/video/BV1Ae4y167ty 【eigenchris】 Tensors for Beginners 【eigenchris】 Tensor Calculus 评价 eigenchris 的 tensor 入门系列讲的应该是最好理解的，举了很多具体的例子，例如关于重要的概念行向量作为covector。可惜 XylyXylyX B站链接 【XylyXylyX】 What is a Tensor https://www.bilibili.com/video/BV1NK411Q7p4 【XylyXylyX】 What is a Tensor 相关教材 并非上述两个视频课程的配套教材，我找到一些市面上不错的纯 Tensor 类入门书籍 李群 如之前所说，李群的本质是为了解决例如旋转矩阵群虽然在乘法上封闭（因此组成群）但是在加法上不封闭的问题。因为在做优化或者其他运算时，旋转代表的矩阵必然用到矩阵加法。李群的解决方式是将小量上的映射到李空间上做完运算后映射回李群。不过一般李群过于复杂，学习内容过于庞大，因此简单的入门李群本身也比较困难。 XylyXylyX B站链接 【XylyXylyX】 Lie Groups and Lie Algebras] https://www.bilibili.com/video/BV1Z14y1j78c 【XylyXylyX】 Lie Groups and Lie Algebras 评价 XylyXylyX的李群视频是众多李群入门中比较不错的，有不少重复，适合初学。配套 Robert Gilmore的如下教材（教材后面部分过于复杂）。 配套教材 Jonathan Evans B站链接 【Jonathan Evans】 Lie Groups and Lie Algebras 配套教材 视频有专门一节推荐了众多不错的教材，不过李群这个领域过于深奥，我也只是刚入门。因此在此仅列其一：Springer 的GTM（研究生数学系列）表示理论 傅里叶变换 傅里叶变换的本质是将函数分解成一组相对卷积算子的对角化基，是进阶泛函分析的重要概念。 【斯坦福 Brad Osgood】傅里叶变换和应用 B站链接 【Stanford】 The Fourier Transforms and its Applications 配套教材 可由下方地址下载 https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf 微分流形 XylyXylyX B站链接 【XylyXylyX】What is a Manifold 程序员的范畴论 B站链接 【Bartosz Milewski】 Category Theory for Programmers 【Bartosz Milewski】 Category Theory for Programmers II 【Bartosz Milewski】 Category Theory for Programmers III 配套教材 其他 B站链接 【NJ Wildberger】 Famous Math Problems","link":"/zh/2022/pure-math-course/"},{"title":"","text":"斯坦福大学的 CS 课程上线了一门经典课程：《CS 25: Transformers United》。自 2017 年推出以来，Transformer 彻底改变了自然语言处理 (NLP)领域。现在，Transformer 在深度学习中被广泛使用，无论是计算机视觉 (CV)、强化学习 (RL)、生成对抗网络 (GAN)、语音甚至是生物学。除此之外，Transformer 还能够创建强大的语言模型（如 GPT-3），并在 AlphaFold2 中发挥了重要作用，该算法解决了蛋白质折叠问题。 目前这门课程在 Youtube 上日更连载中，地址为 https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM MyEncyclopedia Bilibili 为大家每日搬运同步视频，至今天7/16日已经有6集 明星讲课阵容 在今天公布的第一节课中，讲师为斯坦福大学硕士生 Divyansh Garg、软件工程师 Chetanya Rastogi（毕业于斯坦福大学）、软件工程师 Advay Pal（毕业于斯坦福大学）。 此外，第一节课的指导教授为 Christopher Manning，他是斯坦福大学计算机与语言学教授，也是将深度学习应用于自然语言处理领域的领军者。 从之前的课程描述来看，CS 25 课程邀请了来自不同领域关于 Transformer 研究的前沿人士进行客座讲座。OpenAI 的研究科学家 Mark Chen，主要介绍基于 Transformers 的 GPT-3、Codex；Google Brain 的科学家 Lucas Beyer，主要介绍 Transformer 在视觉领域的应用；Meta FAIR 科学家 Aditya Grover，主要介绍 RL 中的 Transformer 以及计算引擎等。 值得一提的是，AI 教父 Geoff Hinton 也带来了一次讲座。 课程明细 1. (Sep 20) Introduction to Transformers Recommended Readings: Attention Is All You Need The Illustrated Transformer The Annotated Transformer (Assignment) 2. (Sept 27) Transformers in Language: GPT-3, Codex Speaker: Mark Chen (OpenAI) Recommended Readings: - Language Models are Few-Shot Learners - Evaluating Large Language Models Trained on Code 3. (Oct 4) Applications in Vision Speaker: Lucas Beyer (Google Brain) Recommended Readings: - An Image is Worth 16x16 Words (Vision Transfomer) - Additional Readings: - How to train your ViT? 4. (Oct 11) Transformers in RL &amp; Universal Compute Engines Speaker: Aditya Grover (FAIR) Recommended Readings: - Pretrained Transformers as Universal Computation Engines - Decision Transformer: Reinforcement Learning via Sequence Modeling 5. (Oct 18) Scaling transformers Speaker: Barret Zoph (Google Brain) with Irwan Bello and Liam Fedus Recommended Readings: - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity - ST-MoE: Designing Stable and Transferable Sparse Expert Models 6. (Oct 25) Perceiver: Arbitrary IO with transformers Speaker: Andrew Jaegle (DeepMind) Recommended Readings: - Perceiver: General Perception with Iterative Attention - Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs 7. (Nov 1) Self Attention &amp; Non-Parametric Transformers Speaker: Aidan Gomez (University of Oxford) Recommended Readings: - Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning 8. (Nov 8) GLOM: Representing part-whole hierarchies in a neural network Speaker: Geoffrey Hinton (UoT) Recommended Readings: How to represent part-whole hierarchies in a neural network 9. (Nov 15) Interpretability with transformers Speaker: Chris Olah (AnthropicAI) Recommended Readings: - Multimodal Neurons in Artificial Neural Networks Additional Readings: - The Building Blocks of Interpretability 10. (Nov 29) Transformers for Applications in Audio, Speech and Music: From Language Modeling to Understanding to Synthesis Speaker: Prateek Verma (Stanford)","link":"/zh/2022/share-cs25-transformer-united/"},{"title":"初识时间序列预测神器 NeuralProphet 实战预测股票指数","text":"NeuralProphet 深度学习版 Prophet NeuralProphet 是 Facebook开发的新一代 Prophet 时间序列预测框架。加入了 PyTorch 的自回归 （AR）模块，并用 PyTorch 重新实现了 Prophet，它的显著特性如下 使用 PyTorch 的优化，性能比原始 Prophet 快不少 引入 AR-Net 建模时间序列自回归，并配有非线性层 自定义损失和指标 滞后协变量（lagged covariates） 和 AR 本地上下文特性 （local context with support for auto-regression） NeuralProphet 继承了 Prophet 模块可接受性的特点，将预测的值分解到趋势、季节性、AR、事件（节日）几个模块。其中 AR 部分的神经网络实现由 AR-Net: A simple Auto-Regressive Neural Network for time-series 这篇论文详细描述。 尽管 NeuralProphet 有不少优势，但是使用起来小问题不断，主要表现为文档不甚详细，API 设计的比较智能（隐晦），坑不少。这一期我们来实战体验一下，后续会深入代码和论文。 相关论文链接 [Prophet] Forecasting at scale https://peerj.com/preprints/3190/ NeuralProphet: Explainable Forecasting at Scale https://arxiv.org/abs/2111.15397 AR-Net: A simple Auto-Regressive Neural Network for time-series https://arxiv.org/abs/1911.12436 安装NeuralProphet 使用命令通过 pip 就可以安装 NeuralProphet。 1pip install neuralprophet==0.5.0 如果在 Jupyter Notebook 中使用 NeuralProphet，最好安装实时版本，允许你实时可视化模型损失。 1pip install neuralprophet[live]==0.5.0 要注意一点，安装 neuralprophet 会关联安装 Pytorch CPU版本库，如果你需要使用 GPU 或者不希望覆盖原有的 Pytorch 版本，请手动安装。 此外，MyEncyclopedia 和往常一样，为大家准备了一个 docker 镜像，预装最新的 NeuralProphet 库，镜像中还包含预下载的数据集和本文所有的 Jupyter Notebook 代码。大家关注MyEncyclopedia 公众号，执行下面命令后网页打开 http://localhost:8888/ 开箱即用 12docker pull myencyclopedia/neuralprophet-tutdocker run -p 8888:8888 myencyclopedia/neuralprophet-tut bash -c 'jupyter notebook --port 8888 --NotebookApp.token='' --NotebookApp.password='' --ip 0.0.0.0 --allow-root' 标准普尔 500 指数数据集 这次实战我们使用过去 10 年标准普尔 500 指数的每日股价数据。可以通过如下命令下载数据集，使用 docker 镜像的同学无需额外下载。 123456789import pandas_datareader as pdrfrom datetime import datetimeimport matplotlib.pyplot as plt%matplotlib inlinestart = datetime(2010, 12, 13)end = datetime(2020, 12, 11)df_sp500 = pdr.get_data_fred('sp500', start, end)plt.figure(figsize=(10, 7))plt.plot(df_sp500)plt.title('S&amp;P 500 Prices') 从图中我们可以清楚地看到标准普尔 500 指数总体呈上升趋势，其中有几个点的价格大幅上涨或下跌。我们可以将这些点视为趋势变化点。鉴于此，我们先从一个仅有趋势模块的 NeuralProphet 模型开始，逐渐加入季节性，AR和节日模块，观察其预测表现和API 具体使用。 使用 NeuralProphet，我们必须确保数据的格式包含如下两列：日期列名ds，目标变量列名 y。 1df_sp500 = df_sp500.reset_index().rename(columns={'DATE': 'ds', 'sp500': 'y'}) 12len(df_sp500[~df_sp500.y.isnull()])&gt;&gt;&gt; 2007 总结 SP 500 数据观察到的特点，后面会反复和过程变量做对比： 总共2080条数据中非空数据有2007条 开始日期为 2012-12-24，结束日期 2020-12-11 在有效时间段 2012-12-24 至 2020-12-11，非交易的日期（周末，节日）没有在列。 模块一：趋势 使用 NeuralProphet，我们可以通过指定几个重要参数来对时间序列数据中的趋势进行建模。 n_changepoints — 指定趋势发生变化的点数。 trend_reg — 控制趋势变化点的正则化参数。较大的值 (~1–100) 将惩罚更多的变化点。较小的值 (~0.001–1.0) 将允许更多的变化点。 changepoints_range — 默认值 0.8，表示后20%的训练数据无 changepoints 123456model = NeuralProphet(n_changepoints=100, trend_reg=0.05, yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False, epochs=100) 训练模型 1df_train, df_val = model.split_df(df_sp500, freq=\"D\", valid_p=0.2) 12345metrics = model.fit(df_train, freq='D', validation_df=df_val, progress=\"plot\" ) 训练最终趋于稳定。我们来看看 split_df API 的细节。 df_train 共1606 行，为前 80% 记录，df_val 共401 行，为后20% 记录，两者没有交集，合计 2007 行数据，等于 df_sp500 有效数据数。原来默认情况下 split_df 会扔掉 y 值为 NaN 数据。这里两者没有交集，大家注意对比启用AR后的数据切分两者会有交集。原有是启用自回归后，预测需要过去 k 个点作为输入。 预测验证集 接着来看看验证集，即 df_val 上的预测表现。 1234future = model.make_future_dataframe(df_sp500, periods=60, n_historic_predictions=True)forecast = model.predict(future)fig = model.plot(forecast)fig.show() make_future_dataframe 准备好待预测的数据格式，参数 periods=60，n_historic_predictions=True 意义扩展 df_sp500 到未来60天后，同时保留所有所有现有 df_sp500 的数据点，这些历史点也将做预测。我们 dump 出 make_future_dataframe 后的 future 变量。 future 序列扩展了 df_sp500，有 y 值的共2007条，和 df_sp500 一致。时间扩展到了 2021-02-09，大约是 2021-12-11 后的60天，这个也和总条数 2140 一致，等于 df_sp500 总条数 2080 加上 periods=60 的部分。 接着来看 predict 后的 forecast 变量。y 列依然有 2007 条，多了 yhat1 和 trend 两列。 最后，model.plot(forecast) 会绘制出事实点和预测点的曲线，注意图中预测值比实际值要稍长一些，因为预测值到 2021-02-09，实际值仅到 2020-12-11。 模块归因 1fig_components = model.plot_components(forecast) 由于只启用了趋势，只有一个模块输出。 很明显，我们的模型捕捉到了标准普尔 500 指数的总体上涨趋势，但该模型存在欠拟合问题，尤其是当我们查看未知未来的60天的预测，更能发现问题。 仅预测未来 同样的预测代码，将 n_historic_predictions 改成 False 会只预测未知未来60天。 1234future = model.make_future_dataframe(df_sp500, periods=60, n_historic_predictions=False)forecast = model.predict(future)fig = model.plot(forecast)fig.show() 12print(len(future), len(forecast))&gt;&gt;&gt; 60 60 根据上图，我们可以看到模型对未来的预测遵循一条直线，天天上涨的股票，还在这里看什么，还不赶紧去买！ 模块二：季节性 真实世界的时间序列数据通常涉及季节性模式。即使对于股票市场也是如此，一月效应等趋势可能会逐年出现。我们可以通过添加年度季节性来使之前的模型更加完善。 1234567891011121314model = NeuralProphet(n_changepoints=100, trend_reg=0.05, yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False, epochs=100)df_train, df_val = model.split_df(df_sp500, freq=\"D\", valid_p=0.2)metrics = model.fit(df_train, freq='D', validation_df=df_val, progress=\"bar\" ) 预测验证集 和之前一条直线相比，现在对数据的预测显得更现实些。 模块归因 现在预测的 Y 值是两个部分的模块的加和了。 1fig_components = model.plot_components(forecast) 标准普尔 500 指数预测具有年度季节性，包括历史数据。 仅预测未来 1plot_forecast(model, sp500_data, periods=60, historic_predictions=False, highlight_steps_ahead=60) 根据上图，我们可以看到这个模型更真实一些，但仍然存在欠拟合问题。因此，我们再引入自回归模型 AR 来进一步拟合。 模块三：自回归 AR AR-Net 是一种用于时间序列预测的自回归神经网络。自回归模型使用来自先前时间步长的过去历史数据来预测，这就是自回归一词的来源。 例如，为了预测标准普尔 500 指数的价格，我们可以训练一个模型，使用过去 60 天的价格来预测未来 60 天的价格。分别对应以下代码中的n_lags和n_forecasts参数。 123456789101112model = NeuralProphet( n_forecasts=60, n_lags=60, changepoints_range=0.95, n_changepoints=100, yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False, batch_size=32, epochs=100, learning_rate=1.0,) 训练模型 123456df_train, df_val = model.split_df(df_sp500, freq=\"D\", valid_p=0.2)metrics = model.fit(df_train, freq='D', validation_df=df_val, progress=\"plot\" ) 切分训练和验证集代码一样，但是由于引入 AR，df_train，df_val 之间有60条数据重合，这是因为，在验证或者预测过程中，传入的 dataframe 前60条不做预测，从61条开始预测，预测会使用当前日期前60条作为 AR 模块的输入。 12len(set(df_train.ds.tolist()).intersection(set(df_val.ds.tolist())))&gt;&gt;&gt; 60 不过奇怪的是，df_train 加上 df_val 总共有 2305 + 665 = 2970 条记录，时间跨度依然是 2012-12-24 至 2020-12-11。但是去除重复的60条记录后居然剩余2910 条， 比 df_sp500 2080 条记录数还要多不少。 这里笔者稍微花了点时间终于弄清楚：df_train 和 df_val 会填充 2012-12-24 至 2020-12-11 所有的 missing 日期，并使用插值填充 y！ 预测验证集 这一次，我们将 periods 设成 0，也就是不扩展 df_sp500 时间到未知的未来。 1future = model.make_future_dataframe(df_sp500, periods=0, n_historic_predictions=True) 1forecast = model.predict(future) forecast 格式变得复杂，引入了 yhat1, yhat2, ...，yhat60，ar1, ar2, ...，ar60 等众多列，这里的60对应于 n_forecasts=60 第一个预测值开始于 forecast 的第61条记录，对应于 n_lags = 60 1forecast[~forecast.yhat1.isnull()] 12fig = model.plot(forecast)fig.show() 模块归因 1plot_forecast(model, sp500_data, periods=60, historic_predictions=False, highlight_steps_ahead=60) 仅预测未来 模块四：事件（节日） 我们还可以配置模型以考虑节假日因素，因为节假日很可能会影响股市走势。 1234567891011121314model = NeuralProphet( n_forecasts=60, n_lags=60, changepoints_range=0.95, n_changepoints=100, yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False, batch_size=32, epochs=100, learning_rate=1.0,)model = model.add_country_holidays(\"US\", mode=\"additive\", lower_window=-1, upper_window=1) 只需 add_country_holidays 一条语句就可以启用预定义的美国节假日。 1plot_forecast(model, sp500_data, periods=60, historic_predictions=False, highlight_steps_ahead=60) 预测验证集 模块归因 仅预测未来","link":"/zh/2023/docker-neuralprophet/"}],"tags":[{"name":"Maths","slug":"Maths","link":"/tags/Maths/"},{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"LeetCode","slug":"LeetCode","link":"/tags/LeetCode/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Gaming","slug":"Gaming","link":"/tags/Gaming/"},{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","link":"/tags/Reinforcement-Learning/"},{"name":"OpenAI Gym","slug":"OpenAI-Gym","link":"/tags/OpenAI-Gym/"},{"name":"Pygame","slug":"Pygame","link":"/tags/Pygame/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Dynamic Programming","slug":"Dynamic-Programming","link":"/tags/Dynamic-Programming/"},{"name":"RNN","slug":"RNN","link":"/tags/RNN/"},{"name":"PyTorch","slug":"PyTorch","link":"/tags/PyTorch/"},{"name":"Markov Chain","slug":"Markov-Chain","link":"/tags/Markov-Chain/"},{"name":"Representation Learning","slug":"Representation-Learning","link":"/tags/Representation-Learning/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"arxiv","slug":"arxiv","link":"/tags/arxiv/"},{"name":"Policy Iteration","slug":"Policy-Iteration","link":"/tags/Policy-Iteration/"},{"name":"Monte Carlo","slug":"Monte-Carlo","link":"/tags/Monte-Carlo/"},{"name":"MCTS","slug":"MCTS","link":"/tags/MCTS/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"Statistics","slug":"Statistics","link":"/tags/Statistics/"},{"name":"Simulation","slug":"Simulation","link":"/tags/Simulation/"},{"name":"Probability","slug":"Probability","link":"/tags/Probability/"},{"name":"Leetcode","slug":"Leetcode","link":"/tags/Leetcode/"},{"name":"Functional Programming","slug":"Functional-Programming","link":"/tags/Functional-Programming/"},{"name":"DFS","slug":"DFS","link":"/tags/DFS/"},{"name":"Paper Dive","slug":"Paper-Dive","link":"/tags/Paper-Dive/"},{"name":"DQN","slug":"DQN","link":"/tags/DQN/"},{"name":"sharing","slug":"sharing","link":"/tags/sharing/"},{"name":"Course","slug":"Course","link":"/tags/Course/"},{"name":"GNN","slug":"GNN","link":"/tags/GNN/"},{"name":"强化学习","slug":"强化学习","link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"Optimization","slug":"Optimization","link":"/tags/Optimization/"},{"name":"Sampling","slug":"Sampling","link":"/tags/Sampling/"},{"name":"Deep Graph","slug":"Deep-Graph","link":"/tags/Deep-Graph/"},{"name":"Geometric","slug":"Geometric","link":"/tags/Geometric/"},{"name":"DRL","slug":"DRL","link":"/tags/DRL/"},{"name":"AI服务","slug":"AI服务","link":"/tags/AI%E6%9C%8D%E5%8A%A1/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"全栈","slug":"全栈","link":"/tags/%E5%85%A8%E6%A0%88/"},{"name":"OCR","slug":"OCR","link":"/tags/OCR/"},{"name":"wechat","slug":"wechat","link":"/tags/wechat/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Animation","slug":"Animation","link":"/tags/Animation/"},{"name":"Service","slug":"Service","link":"/tags/Service/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Deep Learning Practice","slug":"Deep-Learning-Practice","link":"/tags/Deep-Learning-Practice/"},{"name":"Markme","slug":"Markme","link":"/tags/Markme/"},{"name":"transformer","slug":"transformer","link":"/tags/transformer/"},{"name":"share","slug":"share","link":"/tags/share/"},{"name":"notebook","slug":"notebook","link":"/tags/notebook/"}],"categories":[{"name":"Tech Blog","slug":"Tech-Blog","link":"/categories/Tech-Blog/"}]}