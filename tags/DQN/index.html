<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>标签: DQN - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/tags/DQN/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/tags/DQN/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#DQN</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-dqn-mario/" itemprop="url">深度强化学习之：DQN训练超级玛丽闯关</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-12-04T18:45:01.000Z" itemprop="datePublished">12月 5 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            12 分钟 读完 (约 1743 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>上一期 MyEncyclopedia公众号文章 <a href="/zh/2020/rl-dqn-mario/!--swig￼9--">从Q-Learning
演化到
DQN</a>，我们从原理上讲解了DQN算法，这一期，让我们通过代码来实现任天堂游戏机中经典的超级玛丽的自动通关吧。本文所有代码在
https://github.com/MyEncyclopedia/reinforcement-learning-2nd/tree/master/super_mario。</p>
<h2 id="dqn-算法回顾">DQN 算法回顾</h2>
<p>上期详细讲解了DQN中的两个重要的技术：Target Network 和 Experience
Replay，正是有了它们才使得 Deep Q
Network在实战中容易收敛，以下是Deepmind 发表在Nature 的 Human-level
control through deep reinforcement learning 的完整算法流程。</p>
<figure>
<img src="/zh/2020/rl-dqn-mario/dqn_alg_nature.png">
<figcaption>
</figcaption>
</figure>
<h2 id="超级玛丽-nes-openai-环境">超级玛丽 NES OpenAI 环境</h2>
<p>安装基于OpenAI gym的超级玛丽环境执行下面的 pip 命令即可。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym-super-mario-bros</span><br></pre></td></tr></tbody></table></figure>
<p>我们先来看一下游戏环境的输入和输出。下面代码采用随机的action来和游戏交互。有了
<a href="/zh/2020/rl-dqn-mario/!--swig￼10--">组合游戏系列3: 井字棋、五子棋的OpenAI Gym
GUI环境</a> 对于OpenAI Gym
接口的介绍，现在对于其基本的交互步骤已经不陌生了。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> gym_super_mario_bros</span><br><span class="line"><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> random, randrange</span><br><span class="line"><span class="hljs-keyword">from</span> gym_super_mario_bros.actions <span class="hljs-keyword">import</span> RIGHT_ONLY</span><br><span class="line"><span class="hljs-keyword">from</span> nes_py.wrappers <span class="hljs-keyword">import</span> JoypadSpace</span><br><span class="line"><span class="hljs-keyword">from</span> gym <span class="hljs-keyword">import</span> wrappers</span><br><span class="line"></span><br><span class="line">env = gym_super_mario_bros.make(<span class="hljs-string">'SuperMarioBros-v0'</span>)</span><br><span class="line">env = JoypadSpace(env, RIGHT_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Play randomly</span></span><br><span class="line">done = <span class="hljs-literal">False</span></span><br><span class="line">env.reset()</span><br><span class="line"></span><br><span class="line">step = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">    action = randrange(<span class="hljs-built_in">len</span>(RIGHT_ONLY))</span><br><span class="line">    state, reward, done, info = env.step(action)</span><br><span class="line">    <span class="hljs-built_in">print</span>(done, step, info)</span><br><span class="line">    env.render()</span><br><span class="line">    step += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></tbody></table></figure>
<p>游戏render效果如下</p>
<p>。。。</p>
<p>注意我们在游戏环境初始化的时候用了参数
RIGHT_ONLY，它定义成五种动作的list，表示仅使用右键的一些组合，适用于快速训练来完成Mario第一关。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RIGHT_ONLY = [</span><br><span class="line">    [<span class="hljs-string">'NOOP'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'A'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'B'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'A'</span>, <span class="hljs-string">'B'</span>],</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>观察一些 info 输出内容，coins表示金币获得数量，flag_get
表示是否取得最后的旗子，time 剩余时间，以及 Mario 大小状态和所在的
x，y位置。 </p><figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">   "coins":0,</span><br><span class="line">   "flag_get":False,</span><br><span class="line">   "life":2,</span><br><span class="line">   "score":0,</span><br><span class="line">   "stage":1,</span><br><span class="line">   "status":"small",</span><br><span class="line">   "time":381,</span><br><span class="line">   "world":1,</span><br><span class="line">   "x_pos":594,</span><br><span class="line">   "y_pos":89</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="游戏图像处理">游戏图像处理</h2>
<p>Deep Reinforcement Learning 一般是 end-to-end learning，意味着游戏的
screen image
作为observation直接视为真实状态，喂给神经网络训练。于此相反的另一种做法是，通过游戏环境拿到内部状态，例如所有相关物品的位置和属性作为模型输入。这两种方式的区别有两点。第一点，用观察到的屏幕像素代替真正的状态
s，在partially observable 的环境时可能因为 non-stationarity
导致无法很好的工作，而拿内部状态利用了额外的作弊信息，在partially
observable环境中也可以工作。第二点，第一种方式屏幕像素维度比较高，输入数据量大，需要神经网络的大量训练拟合，第二种方式，内部真实状态往往维度低得多，训练起来很快，但缺点是因为除了内部状态往往还需要游戏相关规则作为输入，因此generalization能力不如前者强。</p>
<figure>
<img src="/zh/2020/rl-dqn-mario/dqn.jpg">
<figcaption>
</figcaption>
</figure>
<p>这里，我们当然采样屏幕像素的 end-to-end
方式了，自然首要任务是将游戏帧图像有效处理。超级玛丽游戏环境的屏幕输出是
(240, 256, 3) shape的 numpy
array，通过下面一系列的转换，尽可能的在不影响训练效果的情况下减小采样到的数据量。</p>
<ol type="1">
<li><p>MaxAndSkipFrameWrapper：每4个frame连在一起，采取同样的动作，降低frame数量。</p></li>
<li><p>FrameDownsampleWrapper：将原始的 (240, 256, 3) down sample 到
(84, 84, 1)</p></li>
<li><p>ImageToPyTorchWrapper：转换成适合 pytorch 的 (1, 84, 84)
shape</p></li>
<li><p>FrameBufferWrapper：保存最后4次屏幕采样</p></li>
<li><p>NormalizeFloats：Normalize 成 [0., 1.0] 的浮点值</p></li>
</ol>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">wrap_environment</span>(<span class="hljs-params">env_name: <span class="hljs-built_in">str</span>, action_space: <span class="hljs-built_in">list</span></span>) -&gt; Wrapper:</span></span><br><span class="line">    env = make(env_name)</span><br><span class="line">    env = JoypadSpace(env, action_space)</span><br><span class="line">    env = MaxAndSkipFrameWrapper(env)</span><br><span class="line">    env = FrameDownsampleWrapper(env)</span><br><span class="line">    env = ImageToPyTorchWrapper(env)</span><br><span class="line">    env = FrameBufferWrapper(env, <span class="hljs-number">4</span>)</span><br><span class="line">    env = NormalizeFloats(env)</span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<h2 id="cnn-模型">CNN 模型</h2>
<p>模型比较简单，三个卷积层后做
softmax输出，输出维度数为离散动作数。act() 采用了epsilon-greedy
模式，即在epsilon小概率时采取随机动作来
explore，大于epsilon时采取估计的最可能动作来 exploit。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQNModel</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_shape, num_actions</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(DQNModel, self).__init__()</span><br><span class="line">        self._input_shape = input_shape</span><br><span class="line">        self._num_actions = num_actions</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(self.feature_size, <span class="hljs-number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="hljs-number">512</span>, num_actions)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span></span><br><span class="line">        x = self.features(x).view(x.size()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, state, epsilon, device</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> random() &gt; epsilon:</span><br><span class="line">            state = torch.FloatTensor(np.float32(state)).unsqueeze(<span class="hljs-number">0</span>).to(device)</span><br><span class="line">            q_value = self.forward(state)</span><br><span class="line">            action = q_value.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].item()</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            action = randrange(self._num_actions)</span><br><span class="line">        <span class="hljs-keyword">return</span> action</span><br></pre></td></tr></tbody></table></figure>
<h2 id="experience-replay-缓存">Experience Replay 缓存</h2>
<p>实现采用了 Pytorch CartPole DQN 的官方代码，本质是一个最大为 capacity
的 list 保存采样的 (s, a, r, s', is_done) 五元组。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="hljs-string">'Transition'</span>, (<span class="hljs-string">'state'</span>, <span class="hljs-string">'action'</span>, <span class="hljs-string">'reward'</span>, <span class="hljs-string">'next_state'</span>, <span class="hljs-string">'done'</span>))</span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayMemory</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, capacity</span>):</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">push</span>(<span class="hljs-params">self, *args</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="hljs-literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="hljs-number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, batch_size</span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.memory)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="dqnagent">DQNAgent</h2>
<p>我们将 DQN 的逻辑封装在 DQNAgent 类中。DQNAgent 成员变量包括两个
DQNModel，一个ReplayMemory。</p>
<p>train() 方法中会每隔一定时间将 Target Network
的参数同步成现行Network的参数。在td_loss_backprop()方法中采样
ReplayMemory 中的五元组，通过minimize TD error方式来改进现行 Network
参数 <span class="math inline">\(\theta\)</span>。Loss函数为：</p>
<p><span class="math display">\[
L\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right)
\sim \mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}}
Q_{target}\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s,
a ; \theta_{i}\right)\right)^{2}\right]
\]</span></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQNAgent</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, state, episode_idx</span>):</span></span><br><span class="line">        self.update_epsilon(episode_idx)</span><br><span class="line">        action = self.model.act(state, self.epsilon, self.device)</span><br><span class="line">        <span class="hljs-keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process</span>(<span class="hljs-params">self, episode_idx, state, action, reward, next_state, done</span>):</span></span><br><span class="line">        self.replay_mem.push(state, action, reward, next_state, done)</span><br><span class="line">        self.train(episode_idx)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, episode_idx</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.replay_mem) &gt; self.initial_learning:</span><br><span class="line">            <span class="hljs-keyword">if</span> episode_idx % self.target_update_frequency == <span class="hljs-number">0</span>:</span><br><span class="line">                self.target_model.load_state_dict(self.model.state_dict())</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            self.td_loss_backprop()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">td_loss_backprop</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        transitions = self.replay_mem.sample(self.batch_size)</span><br><span class="line">        batch = Transition(*<span class="hljs-built_in">zip</span>(*transitions))</span><br><span class="line"></span><br><span class="line">        state = Variable(FloatTensor(np.float32(batch.state))).to(self.device)</span><br><span class="line">        action = Variable(LongTensor(batch.action)).to(self.device)</span><br><span class="line">        reward = Variable(FloatTensor(batch.reward)).to(self.device)</span><br><span class="line">        next_state = Variable(FloatTensor(np.float32(batch.next_state))).to(self.device)</span><br><span class="line">        done = Variable(FloatTensor(batch.done)).to(self.device)</span><br><span class="line"></span><br><span class="line">        q_values = self.model(state)</span><br><span class="line">        next_q_values = self.target_net(next_state)</span><br><span class="line"></span><br><span class="line">        q_value = q_values.gather(<span class="hljs-number">1</span>, action.unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)</span><br><span class="line">        next_q_value = next_q_values.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]</span><br><span class="line">        expected_q_value = reward + self.gamma * next_q_value * (<span class="hljs-number">1</span> - done)</span><br><span class="line"></span><br><span class="line">        loss = (q_value - expected_q_value.detach()).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)</span><br><span class="line">        loss = loss.mean()</span><br><span class="line">        loss.backward()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="外层-training-代码">外层 Training 代码</h2>
<p>最后是外层调用代码，基本和以前文章一样。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">env, args, agent</span>):</span></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args.num_episodes):</span><br><span class="line">        episode_reward = <span class="hljs-number">0.0</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">            action = agent.act(state, episode_idx)</span><br><span class="line">            <span class="hljs-keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            next_state, reward, done, stats = env.step(action)</span><br><span class="line">            agent.process(episode_idx, state, action, reward, next_state, done)</span><br><span class="line">            state = next_state</span><br><span class="line">            episode_reward += reward</span><br><span class="line">            <span class="hljs-keyword">if</span> done:</span><br><span class="line">                <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{episode_idx}</span>: <span class="hljs-subst">{episode_reward}</span>'</span>)</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-qlearning-to-dqn/" itemprop="url">通过代码学Sutton强化学习：从Q-Learning 演化到 DQN</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-10-29T18:45:01.000Z" itemprop="datePublished">10月 30 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 分钟 读完 (约 2717 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>上一期 MyEncyclopedia公众号文章 <a href="/zh/2020/rl-qlearning-to-dqn/!--swig￼1--">SARSA、Q-Learning和Expected
SARSA时序差分算法训练CartPole</a>中，我们通过CartPole的OpenAI
Gym环境实现了Q-learning算法，这一期，我们将会分析Q-learning算法面临的maximization
bias 问题和提出double learning算法来改进。接着，我们将tabular
Q-learning算法扩展到用带参函数来近似 Q(s, a)，这就是Deepmind
在2015年Nature上发表的Deep Q Network
（DQN）思想：用神经网络结合Q-learning算法实现超越人类玩家打Atari游戏的水平。</p>
<h2 id="q-learning-回顾">Q-Learning 回顾</h2>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Q-learning (off-policy TD Control) for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy
derived from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
\max_{a}Q(S^{\prime}, a) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<p>在<a href="/zh/2020/rl-qlearning-to-dqn/!--swig￼2--">SARSA、Q-Learning和Expected
SARSA时序差分算法训练CartPole</a>&nbsp;中，我们实现了同样基于 <span class="math inline">\(\epsilon\)</span>-greedy
策略的Q-learning算法和SARSA算法，两者代码上的区别确实不大，但本质上Q-learning是属于
off-policy 范畴而 SARSA却属于 on-policy
范畴。一种理解方式是，Q-learning相比于SARSA少了第二次从 <span class="math inline">\(\epsilon\)</span>-greedy
策略采样出下一个action，即S, A, R', S', A'
五元组中最后一个A'，而直接通过max操作去逼近 <span class="math inline">\(q^{*}\)</span>。如此，Q-learning并没有像SARSA完成一次完整的GPI（Generalized
Policy Iteration），缺乏on-policy的策略迭代的特点，故而 Q-learning
属于off-policy方法。我们也可以从另一个角度来分析两者的区别。注意到这两个算法不是一定非要使用
<span class="math inline">\(\epsilon\)</span>-greedy
策略的。对于Q-learning来说，完全可以使用随机策略，理论上已经证明，只要保证每个action以后依然有几率会被探索下去，Q-learning
最终会收敛到最优策略。Q-learning使用 <span class="math inline">\(\epsilon\)</span>-greedy
是为了能快速收敛。对于SARSA算法来说，则无法使用随机策略，因为随机策略无法形成策略提升。而
<span class="math inline">\(\epsilon\)</span>-greedy
策略却可以形成策略迭代，完成策略提升，当然，<span class="math inline">\(\epsilon\)</span>-greedy 策略在 SARSA
算法中也可以保证快速收敛。因此，尽管两者都使用 <span class="math inline">\(\epsilon\)</span>-greedy
策略再借由环境产生reward和state，它们的作用并非完全一样。至此，我们可以体会到on-policy和off-policy本质的区别。</p>
<h3 id="收敛条件">收敛条件</h3>
Tabular Q-Learning 收敛到最佳Q函数的条件如下[2]:
<div>
<p><span class="math display">\[
\Sigma^{\infty}_{n=0} \alpha_{n} = {\infty} \quad \text{  AND  } \quad
\Sigma^{\infty}_{n=0} \alpha^2_{n} \lt {\infty}
\]</span></p>
</div>
<p>一种方式是将 <span class="math inline">\(\alpha\)</span>设置成 (s,
a)访问次数的倒数：<span class="math inline">\(\alpha_{n}(s,a) = 1/ n(s,a
)\)</span></p>
<p>则整体更新公式为</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha_n(s, a)[R+\gamma
\max_{a^{\prime}}Q(s^{\prime}, a^{\prime}) - Q(s, a)]
\]</span></p>
<h3 id="q-learning-最大化偏差问题">Q-Learning 最大化偏差问题</h3>
<p>Q-Learning 会产生最大化偏差问题（Maximization Bias，在Sutton
教材6.7节），它的原因是用估计值中取最大值去估计真实值中最大是有偏的。这个可以做如下试验来模拟，若有5个
[-3, 3] 的离散均匀分布 <span class="math inline">\(d_i\)</span>，<span class="math inline">\(\max(\mathbb{E}[d_i]) =
0\)</span>，但是若我们用单批采样 <span class="math inline">\(x_i \sim
d_i\)</span>来估算 <span class="math inline">\(\mathbb{E}[d_i]\)</span>在取max的话，<span class="math inline">\(\mathbb{E}[{\max(x_i)]}\)</span>
是有bias的。但是如果我们将这个过程分解成选择最大action和评估其值两步，每一步用独立的采样集合就可以做到无偏，这个改进方法称为double
learning。具体过程为第一步在<span class="math inline">\(Q_1\)</span>集合中找到最大的action，第二步在<span class="math inline">\(Q_2\)</span>中返回此action值，即：</p>
<div>
<p><span class="math display">\[
\begin{align*}
A^{\star} = \operatorname{argmax}_{a}Q_1(a) \\
Q_2(A^{\star}) = Q_2(\operatorname{argmax}_{a}Q_1(a))
\end{align*}
\]</span></p>
</div>
<p>则无限模拟后结果是无偏的：<span class="math inline">\(\mathbb{E}[Q_2(A^{\star})] = q(A^{\star})\)</span>
下面是简单模拟试验两种方法的均值比较</p>
<figure>
<img src="/zh/2020/rl-qlearning-to-dqn/double_sampling.png">
<figcaption>
Maximization Bias
</figcaption>
</figure>
<p>试验完整代码如下</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> random</span><br><span class="line"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">uniform</span>(<span class="hljs-params">a: <span class="hljs-built_in">int</span>, b: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">    u = random.random()</span><br><span class="line">    <span class="hljs-keyword">return</span> a + floor((b - a + <span class="hljs-number">1</span>) * u)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span><br><span class="line">    total_max_bias = <span class="hljs-number">0</span></span><br><span class="line">    avgs_max_bias = []</span><br><span class="line">    total_double_sampling = <span class="hljs-number">0</span></span><br><span class="line">    avgs_double_sampling = []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>):</span><br><span class="line">        samples = np.array([uniform(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])</span><br><span class="line">        max_sample = <span class="hljs-built_in">max</span>(samples)</span><br><span class="line">        total_max_bias += max_sample</span><br><span class="line">        avgs_max_bias.append(total_max_bias / e)</span><br><span class="line"></span><br><span class="line">        samples2 = np.array([uniform(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])</span><br><span class="line">        total_double_sampling += samples2[np.argmax(samples)]</span><br><span class="line">        avgs_double_sampling.append(total_double_sampling / e)</span><br><span class="line"></span><br><span class="line">    df = pd.DataFrame({<span class="hljs-string">'Max of Samples'</span>: avgs_max_bias, <span class="hljs-string">'Double Samples'</span>: avgs_double_sampling})</span><br><span class="line">    <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">    sns.lineplot(data=df)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>回到Q-learning 中使用的 <span class="math inline">\(\epsilon\)</span>-greedy策略，Q-learning可以保证随着<span class="math inline">\(\epsilon\)</span> 的减小，最大化偏差会
asymptotically 趋近于真实值，但是double learning
可以更快地趋近于真实值。</p>
<figure>
<img src="/zh/2020/rl-qlearning-to-dqn/double_learning_vs_max_bias.png">
<figcaption>
Maximization Bias vs Double learning
</figcaption>
</figure>
<p>下面是Sutton 强化学习第二版6.7节中完整的Double Q-learning算法。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Double Q-learning, for estimating } Q_1 \approx  Q_2
\approx q_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q_1(s,a), \text{ and } Q_2(s,a) \text{, for all
} s \in \mathcal{S}^{+}, a \in \mathcal{A}(s) \text{, such that }
Q(terminal, \cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy }
\epsilon\text{-greedy in } Q_1 + Q_2 \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad \text{With 0.5 probability:} \\
&amp; \quad \quad \quad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha \left (
R+\gamma Q_2(S^{\prime}, \operatorname{argmax}_{a}Q_1(S^{\prime}, a)) -
Q_1(S,A) \right )\\
&amp; \quad \quad \text{else:} \\
&amp; \quad \quad \quad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha \left (
R+\gamma Q_2(S^{\prime}, \operatorname{argmax}_{a}Q_1(S^{\prime}, a)) -
Q_1(S,A) \right )\\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<p>更详细内容，可以参考 Hado V. Hasselt 的 Double Q-learning paper
[3]。</p>
<h2 id="gradient-q-learning">Gradient Q-Learning</h2>
<p>Tabular
Q-learning由于受制于维度爆炸，无法扩展到高维状态空间，一般近似解决方案是用
approximating function来逼近Q函数。即我们将状态抽象出一组特征 <span class="math inline">\(s = \vec x= [x_1, x_2, ..., x_n]^T\)</span>，Q
用一个 x 的函数来近似表达 <span class="math inline">\(Q(s, a) \approx
g(\vec x;
\theta)\)</span>，如此，就联系起了深度神经网络。有了函数表达，深度学习还必须的元素是损失函数，这个很自然的可以用
TD
error。至此，问题转换成深度学习的几个要素均已具备，Q-learning算法改造成了深度学习中的有监督问题。</p>
<p>估计值：<span class="math inline">\(Q\left(s, a ;
\theta\right)\)</span></p>
<p>目标值：<span class="math inline">\(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta\right)\)</span></p>
<p>损失函数：</p>
<p><span class="math display">\[
L\left(\theta\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim
\mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime},
a^{\prime} ; \theta\right)-Q\left(s, a ; \theta\right)\right)^{2}\right]
\]</span></p>
<h2 id="收敛性分析">收敛性分析</h2>
<p>首先明确一点，至此 gradient q-learning 和 tabular Q-learning
一样，都是没有记忆的，即对于一个新的环境产生的 sample 去做 stochastic
online update。</p>
<p>若Q函数是状态特征的线性函数，即 <span class="math inline">\(Q(s, a;
\theta) = \Sigma_i w_i x_i\)</span> ，那么线性Gradient
Q-learning的收敛条件和Tabular Q-learning 一样，也为</p>
<div>
<p><span class="math display">\[
\Sigma^{\infty}_{n=0} \alpha_{n} = {\infty} \quad \text{  AND  } \quad
\Sigma^{\infty}_{n=0} \alpha^2_{n} \lt {\infty}
\]</span></p>
</div>
<p>若Q函数是非线性函数，即使符合上述条件，也无法保证收敛，本质上源于改变
<span class="math inline">\(\theta\)</span> 使得 Q 值在 (s, a)
点上减小误差会影响 (s, a) 周边点的误差。</p>
<h2 id="dqn减少不收敛的两个技巧">DQN减少不收敛的两个技巧</h2>
<ol type="1">
<li><span class="math inline">\(\theta_{i-1} \rightarrow
\theta_{i}\)</span> 改变导致max中的估计值和目标值中的Q同时变化，面临着
chasing its own tail
的问题。解决的方法是使用不同的参数来parameterize两个Q，并且目标值的Q网络参数固定一段时间产生一批固定策略下的环境采样。这个技巧称为
Target Network。引入这个 trick 后深度学习的要素变成</li>
</ol>
<p>估计值：<span class="math inline">\(Q\left(s, a ;
\theta_{i}\right)\)</span></p>
<p>目标值：<span class="math inline">\(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta_i^{-}\right)\)</span></p>
<p>损失函数，DQN在Nature上的loss函数： <span class="math display">\[
L\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right)
\sim \mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ;
\theta_{i}\right)\right)^{2}\right]
\]</span></p>
<ol start="2" type="1">
<li>尽管目标值的 <span class="math inline">\(Q(;\theta^{-})\)</span>固定了，但是<span class="math inline">\(\theta_{i-1} \rightarrow \theta_{i}\)</span>
还会使得估计值的 <span class="math inline">\(Q(s, a;\theta_i)\)</span>
在变化的同时影响其他的 <span class="math inline">\(Q(s_k,
a_j;\theta_i)\)</span>，让之前训练过的 (s,
a)的点的损失值发生变化，解决的办法是将 online stochastic 改成 batch
gradient，也就是将最近的一系列采样值保存下来，这个方法称为 experience
replay。</li>
</ol>
<p>有了这两个优化，Deep Q
Network投入实战效果就容易收敛了，以下是Deepmind 发表在Nature 的
Human-level control through deep reinforcement learning [1]
的完整算法流程。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Deep Q-learning with experience replay}\\
&amp; \text{Initialize replay memory } D\text{ to capacity } N \\
&amp; \text{Initialize action-value function } Q \text{ with random
weights } \theta \\
&amp; \text{Initialize target action-value function } \hat{Q} \text{
with weights } \theta^{-} = \theta \\
&amp; \textbf{For} \text{ episode = 1, } M \textbf{ do} \\
&amp; \text{Initialize sequences } s_1 = \{x_1\} \text{ and preprocessed
sequence } \phi_1 = \phi(s_1)\\
&amp; \quad \textbf{For } t=\text{ 1, T }\textbf{ do} \\
&amp; \quad \quad \text{With probability }\epsilon \text{ select a
random action } a_t \\
&amp; \quad \quad \text{otherwise select } a_t =
\operatorname{argmax}_{a}Q(\phi(s_t), a; \theta)\\
&amp; \quad \quad \text{Execute action } a_t \text{ in emulator and
observe reward } r_t \text{ and image  }x_{t+1}\\
&amp; \quad \quad \text{Set } s_{t+1} = s_t, a_t, x_{t+1} \text{ and
preprocess } \phi_{t+1} = \phi(s_{t+1})\\
&amp; \quad \quad \text{Store transition } (\phi_t, a_t, r_t,
\phi_{t+1}) \text{ in } D\\
&amp; \quad \quad \text{Sample random minibatch of transitions }
(\phi_j, a_j, r_j, \phi_{j+1}) \text{ from } D\\
&amp; \quad \quad \text{Set } y_j=
    \begin{cases}
      r_j \quad \quad\quad\quad\text{if episode terminates at step
j+1}\\
      r_j + \gamma \max_{a^{\prime}}\hat Q(\phi_{j+1}, a^{\prime};
\theta^{-}) \quad \text { otherwise}\\
    \end{cases}       \\
&amp; \quad \quad \text{Perform a gradient descent step on } (y_j -
Q(\phi_j, a_j; \theta))^2 \text{ with respect to the network parameters
} \theta\\
&amp; \quad \quad \text{Every C steps reset } \hat Q = Q\\
&amp; \quad \textbf{End For} \\
&amp; \textbf{End For}
\end{align*}
\]</span></p>
</div>
<h3 id="dqn-with-double-q-learning">DQN with Double Q-Learning</h3>
<p>DQN 算法和 Double Q-Learning 能不能结合起来呢？Hado van Hasselt 在
Deep Reinforcement Learning with Double Q-learning [4] 中提出参考 Double
Q-learning 将 DQN
的目标值改成如下函数，可以进一步提升最初DQN的效果。</p>
<p>目标值：<span class="math inline">\(r+\gamma Q(s^{\prime}, \max
_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}; \theta_t\right);
\theta_t^{-})\)</span></p>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><p><strong>Human-level control through deep reinforcement
learning</strong> Volodymyr Mnih, Koray Kavukcuoglu, David Silver
(2015)</p></li>
<li><p>CS885 Reinforcement Learning Lecture 4b: May 11, 2018</p></li>
<li><p><strong>Double Q-learning</strong> Hado V. Hasselt
(2010)</p></li>
<li><p><strong>Deep Reinforcement Learning with Double
Q-learning</strong> Hado van Hasselt, Arthur Guez, David Silver
(2015)</p></li>
</ol>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/tags/DQN/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/tags/DQN/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>