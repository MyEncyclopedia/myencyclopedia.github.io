<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>标签: nlp - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/tags/nlp/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/tags/nlp/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#nlp</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/share-cs25-transformer-united/" itemprop="url"></a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-07-15T18:45:01.000Z" itemprop="datePublished">7月 16 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            6 分钟 读完 (约 871 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>斯坦福大学的 CS 课程上线了一门经典课程：《CS 25: Transformers
United》。自 2017 年推出以来，Transformer 彻底改变了自然语言处理
(NLP)领域。现在，Transformer 在深度学习中被广泛使用，无论是计算机视觉
(CV)、强化学习 (RL)、生成对抗网络
(GAN)、语音甚至是生物学。除此之外，Transformer
还能够创建强大的语言模型（如 GPT-3），并在 AlphaFold2
中发挥了重要作用，该算法解决了蛋白质折叠问题。</p>
<p>目前这门课程在 <code>Youtube</code> 上日更连载中，地址为
https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM</p>
<p><strong>MyEncyclopedia Bilibili
为大家每日搬运同步视频，至今天7/16日已经有6集</strong></p>
<p><img src="/zh/2022/share-cs25-transformer-united/my_bili.jpg"> <img src="/zh/2022/share-cs25-transformer-united/course_home.jpg"></p>
<h2 id="明星讲课阵容">明星讲课阵容</h2>
<p>在今天公布的第一节课中，讲师为斯坦福大学硕士生 Divyansh
Garg、软件工程师 Chetanya Rastogi（毕业于斯坦福大学）、软件工程师 Advay
Pal（毕业于斯坦福大学）。</p>
<p>此外，第一节课的指导教授为 Christopher
Manning，他是斯坦福大学计算机与语言学教授，也是将深度学习应用于自然语言处理领域的领军者。</p>
<p>从之前的课程描述来看，CS 25 课程邀请了来自不同领域关于 Transformer
研究的前沿人士进行客座讲座。OpenAI 的研究科学家 Mark Chen，主要介绍基于
Transformers 的 GPT-3、Codex；Google Brain 的科学家 Lucas
Beyer，主要介绍 Transformer 在视觉领域的应用；Meta FAIR 科学家 Aditya
Grover，主要介绍 RL 中的 Transformer 以及计算引擎等。</p>
<p>值得一提的是，AI 教父 <strong>Geoff Hinton</strong>
也带来了一次讲座。</p>
<h2 id="课程明细">课程明细</h2>
<h4 id="sep-20-introduction-to-transformers">1. (Sep 20) Introduction to
Transformers</h4>
<p>Recommended Readings:</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762.pdf">Attention Is All
You Need</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">The
Illustrated Transformer</a></p></li>
<li><p><a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The
Annotated Transformer (Assignment)</a></p></li>
</ul>
<h4 id="sept-27-transformers-in-language-gpt-3-codex">2. (Sept 27)
Transformers in Language: GPT-3, Codex</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=5fU-QMwAAAAJ&amp;hl=en"><strong>Mark
Chen</strong> (OpenAI)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot
Learners</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.03374">Evaluating Large Language
Models Trained on Code</a></p>
<h4 id="oct-4-applications-in-vision">3. (Oct 4) Applications in
Vision</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=p2gwhK4AAAAJ&amp;hl=en"><strong>Lucas
Beyer</strong> (Google Brain)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">An
Image is Worth 16x16 Words (Vision Transfomer)</a><br>
- Additional Readings:<br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.10270">How to train your
ViT?</a></p>
<h4 id="oct-11-transformers-in-rl-universal-compute-engines">4. (Oct 11)
Transformers in RL &amp; Universal Compute Engines</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://aditya-grover.github.io/"><strong>Aditya
Grover</strong> (FAIR)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.05247">Pretrained Transformers as
Universal Computation Engines</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.01345">Decision Transformer:
Reinforcement Learning via Sequence Modeling</a></p>
<h4 id="oct-18-scaling-transformers">5. (Oct 18) Scaling
transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://barretzoph.github.io/"><strong>Barret
Zoph</strong> (Google Brain)</a> with <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=mY6p8gcAAAAJ&amp;hl=en"><strong>Irwan
Bello</strong></a> and <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=-ZfwQOkAAAAJ&amp;hl=en"><strong>Liam
Fedus</strong></a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2101.03961">Switch Transformers: Scaling to
Trillion Parameter Models with Simple and Efficient Sparsity</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.08906">ST-MoE: Designing Stable
and Transferable Sparse Expert Models</a></p>
<h4 id="oct-25-perceiver-arbitrary-io-with-transformers">6. (Oct 25)
Perceiver: Arbitrary IO with transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://www.drewjaegle.com/"><strong>Andrew
Jaegle</strong> (DeepMind)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.03206">Perceiver: General Perception
with Iterative Attention</a><br>
- <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2107.14795">Perceiver IO: A General
Architecture for Structured Inputs &amp; Outputs</a></p>
<h4 id="nov-1-self-attention-non-parametric-transformers">7. (Nov 1)
Self Attention &amp; Non-Parametric Transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://scholar.google.ca/citations?user=2oq9614AAAAJ&amp;hl=en"><strong>Aidan
Gomez</strong> (University of Oxford)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.02584">Self-Attention Between
Datapoints: Going Beyond Individual Input-Output Pairs in Deep
Learning</a></p>
<h4 id="nov-8-glom-representing-part-whole-hierarchies-in-a-neural-network">8.
(Nov 8) GLOM: Representing part-whole hierarchies in a neural
network</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~hinton/"><strong>Geoffrey
Hinton</strong> (UoT)</a> Recommended Readings:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.12627">How to represent
part-whole hierarchies in a neural network</a></li>
</ul>
<h4 id="nov-15-interpretability-with-transformers">9. (Nov 15)
Interpretability with transformers</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://colah.github.io"><strong>Chris
Olah</strong> (AnthropicAI)</a></p>
<p>Recommended Readings: - <a target="_blank" rel="noopener" href="https://distill.pub/2021/multimodal-neurons/">Multimodal Neurons
in Artificial Neural Networks</a> Additional Readings: - <a target="_blank" rel="noopener" href="https://distill.pub/2018/building-blocks/">The Building Blocks of
Interpretability</a></p>
<h4 id="nov-29-transformers-for-applications-in-audio-speech-and-music-from-language-modeling-to-understanding-to-synthesis">10.
(Nov 29) Transformers for Applications in Audio, Speech and Music: From
Language Modeling to Understanding to Synthesis</h4>
<p>Speaker: <a target="_blank" rel="noopener" href="https://ai.stanford.edu/~prateekv/"><strong>Prateek
Verma</strong> (Stanford)</a></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-faiss-transformer/" itemprop="url">实战入门 faiss 搜索bert 最邻近句子：docker CPU镜像开箱即用，无需额外安装下载</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-07-08T18:45:01.000Z" itemprop="datePublished">7月 9 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            13 分钟 读完 (约 1886 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们延续上一期 <em>Bert 中文短句相似度计算 Docker
CPU镜像</em>，继续使用 <code>huggingface transformer</code> 和
<code>sentence-transformer</code> 类库，并将英语句子生成 bert
embedding，然后引入 <code>faiss</code>
类库来建立索引，最后查询最接近的句子。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>本期 docker 镜像获取方式为，关注 <code>MyEncyclopedia</code>
公众号后回复 <code>docker-faiss-transformer</code>
即可获取如下完整命令。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8888:8888 myencyclopedia/faiss-demo bash -c <span class="hljs-string">'jupyter notebook --allow-root --port 8888 --NotebookApp.token= --ip 0.0.0.0'</span></span><br></pre></td></tr></tbody></table></figure>
<p>然后打开浏览器，输入
<code>http://localhost:8888/notebooks/faiss_demo.ipynb</code></p>
<h2 id="faiss-简介">faiss 简介</h2>
<p>Faiss 的全称是Facebook AI Similarity Search，是由 Facebook
开发的适用于稠密向量匹配的开源库，作为向量化检索开山鼻祖，Faiss
提供了一套查询海量高维数据集的解决方案，它从两个方面改善了暴力搜索算法存在的问题：降低空间占用和加快检索速度。此外，Faiss
提供了若干种方法实现数据压缩，包括 PCA、Product-Quantization等。</p>
<p><strong>Faiss 主要特性：</strong></p>
<ul>
<li>支持相似度检索和聚类；</li>
<li>支持多种索引方式；</li>
<li>支持CPU和GPU计算；</li>
<li>支持Python和C++调用；</li>
</ul>
<h3 id="faiss-使用流程">Faiss 使用流程</h3>
<p>使用 faiss
分成两部，第一步需要对原始向量建立索引文件，第二步再对索引文件进行向量
<code>search</code> 操作。</p>
<p>在第一次建立索引文件的时候，需要经过 <code>train</code> 和
<code>add</code>
两个过程；后续如果有新的向量需要被添加到索引文件，只需要一个
<code>add</code>
操作来实现增量索引更新，但是如果增量的量级与原始索引差不多的话，整个向量空间就可能发生了一些变化，这个时候就需要重新建立整个索引文件，也就是再用全部的向量来走一遍
<code>train</code> 和 <code>add</code>，至于具体是如何
<code>train</code> 和 <code>add</code>的，就和特定的索引类型有关了。</p>
<h3 id="indexflatl2-indexflatip"><strong>1. IndexFlatL2</strong> &amp;
indexFlatIP</h3>
<p>对于精确搜索，例如欧式距离 faiss.indexFlatL2 或 内积距离
faiss.indexFlatIP，没有 <code>train</code> 过程，<code>add</code>
完直接可以 <code>search</code>。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss </span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 建立索引, 定义为dimension d = 128</span></span><br><span class="line">index = faiss.IndexFlatL2(d)</span><br><span class="line"></span><br><span class="line"> <span class="hljs-comment"># add vectors, xb 为 (100000,128)大小的numpy</span></span><br><span class="line">index.add(xb)                 </span><br><span class="line"><span class="hljs-built_in">print</span>(index.ntotal) </span><br><span class="line"><span class="hljs-comment"># 索引中向量的数量, 输出100000</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 求4-近邻</span></span><br><span class="line">k = <span class="hljs-number">4</span></span><br><span class="line"><span class="hljs-comment"># xq为query embedding, 大小为(10000,128)</span></span><br><span class="line">D, I = index.search(xq, k)     </span><br><span class="line"><span class="hljs-comment">## D shape (10000,4)，表示每个返回点的embedding 与 query embedding的距离,</span></span><br><span class="line"><span class="hljs-comment">## I shape (10000,4)，表示和query embedding最接近的k个物品id，</span></span><br><span class="line"><span class="hljs-built_in">print</span>(I[:<span class="hljs-number">5</span>])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="indexivfflat"><strong>2. IndexIVFFlat</strong></h3>
<p>IndexFlatL2
的结果虽然精确，但当数据集比较大的时候，暴力搜索的时间复杂度很高，因此我们一般会使用其他方式的索引来加速。比如
IndexIVFFlat，将数据集在 <code>train</code> 阶段分割为几部分，技术术语为
<code>Voronoi
Cells</code>，每个数据向量只能落在一个cell中。<code>Search</code>
时只需要查询query向量落在cell中的数据了，降低了距离计算次数。这个过程本质就是高维
KNN 聚类算法。<code>search</code> 阶段使用倒排索引来。</p>
<p>IndexIVFFlat 需要一个训练的阶段，其与另外一个索引 quantizer
有关，通过 quantizer 来判断属于哪个cell。IndexIVFFlat
在搜索阶段，引入了nlist(cell的数量)与nprob(执行搜索的cell数)参数。增大nprobe可以得到与brute-force更为接近的结果，nprobe就是速度与精度的调节器。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss</span><br><span class="line">nlist = <span class="hljs-number">100</span></span><br><span class="line">k = <span class="hljs-number">4</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 建立索引, 定义为dimension d = 128</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># 使用欧式距离 L2 建立索引。</span></span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">## xb: (100000,128)</span></span><br><span class="line">index.train(xb) </span><br><span class="line">index.add(xb)                </span><br><span class="line">index.nprobe = <span class="hljs-number">10</span>  <span class="hljs-comment"># 默认 nprobe 是 1 ,可以设置的大一些试试</span></span><br><span class="line">D, I = index.search(xq, k)</span><br><span class="line"><span class="hljs-built_in">print</span>(I[-<span class="hljs-number">5</span>:])   <span class="hljs-comment"># 最后五次查询的结果</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="indexivfpq"><strong>3. IndexIVFPQ</strong></h3>
<p>IndexFlatL2 和
IndexIVFFlat都要存储所有的向量数据。对于超大规模数据集来说，可能会不大现实。因此IndexIVFPQ
索引可以用来压缩向量，具体的压缩算法就是
Product-Quantization，注意，由于高维向量被压缩，因此 <code>search</code>
时候返回也是近似的结果。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line">nlist = <span class="hljs-number">100</span></span><br><span class="line"><span class="hljs-comment"># 每个向量分8段</span></span><br><span class="line">m = <span class="hljs-number">8</span> </span><br><span class="line"><span class="hljs-comment"># 求4-近邻</span></span><br><span class="line">k = <span class="hljs-number">4</span> </span><br><span class="line">quantizer = faiss.IndexFlatL2(d)    <span class="hljs-comment"># 内部的索引方式依然不变</span></span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="hljs-number">8</span>) <span class="hljs-comment"># 每个向量都被编码为8个字节大小</span></span><br><span class="line">index.train(xb)</span><br><span class="line">index.add(xb)</span><br><span class="line">index.nprobe = <span class="hljs-number">10</span>                </span><br><span class="line">D, I = index.search(xq, k)  <span class="hljs-comment"># 检索</span></span><br><span class="line"><span class="hljs-built_in">print</span>(I[-<span class="hljs-number">5</span>:])</span><br></pre></td></tr></tbody></table></figure>
<p>在本期中，我们仅使用基本的 IndexIVFFlat 和 IndexFlatIP 完成 bert
embedding 的索引和搜索，后续会有篇幅来解读 Product-Quantization
的论文原理和代码实践。</p>
<h2 id="ag_news-新闻数据集">ag_news 新闻数据集</h2>
<p>ag_news 新闻数据集 3.0 包含了英语新闻标题，training 部分包含
120000条数据， test 部分包含 7600条数据。</p>
<p>ag_news 可以通过 huggingface datasets API 自动下载</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_dataset</span>(<span class="hljs-params">part=<span class="hljs-string">'test'</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:</span></span><br><span class="line">    ds = datasets.load_dataset(<span class="hljs-string">"ag_news"</span>)</span><br><span class="line">    list_str = [r[<span class="hljs-string">'text'</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> ds[part]]</span><br><span class="line">    <span class="hljs-keyword">return</span> list_str</span><br><span class="line">    </span><br><span class="line">list_str = load_dataset(part=<span class="hljs-string">'train'</span>)</span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{<span class="hljs-built_in">len</span>(list_str)}</span>'</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> list_str[:<span class="hljs-number">3</span>]:</span><br><span class="line">    <span class="hljs-built_in">print</span>(s)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">'\n'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>显示前三条新闻标题为</p>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">120000</span><br><span class="line">Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\band of ultra-cynics, are seeing green again.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\which has a reputation for making well-timed and occasionally\controversial plays in the defense industry, has quietly placed\its bets on another part of the market.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\about the economy and the outlook for earnings are expected to\hang over the stock market next week during the depth of the\summer doldrums.</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="sentence-transformer">sentence-transformer</h2>
<p>和上一期一样，我们利用<code>sentence-transformer</code>
生成句子级别的embedding。其原理基于 Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks
（https://arxiv.org/abs/1908.10084）这篇论文。基本思想很直接，将句子中的每个词的
bert embedding
，输进入一个池化层(pooling)，例如选择最简单的平均池化层，将所有token
embedding 的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence
embedding。</p>
<p><img src="/zh/2022/docker-faiss-transformer/model.png"></p>
<h2 id="结果展示">结果展示</h2>
<p>数据集 train 部分由于包含的样本比较多，需要一段时间生成 bert
embedding，大家可以使用 <code>load_dataset(part='test')</code>
来快速体验。下面我们演示一个查询 how to make money 的最接近结果。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">index = load_index(<span class="hljs-string">'news_train.index'</span>)</span><br><span class="line">list_id = query(model, index, <span class="hljs-string">'how to make money'</span>)</span><br><span class="line"><span class="hljs-keyword">for</span> <span class="hljs-built_in">id</span> <span class="hljs-keyword">in</span> list_id:</span><br><span class="line">    <span class="hljs-built_in">print</span>(list_str[<span class="hljs-built_in">id</span>])</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Profit From That Traffic Ticket Got a traffic ticket? Can't beat 'em? Join 'em by investing in the company that processes those tickets.</span><br><span class="line"></span><br><span class="line">Answers in the Margins By just looking at operating margins, investors can find profitable industry leaders.</span><br><span class="line"></span><br><span class="line">Types of Investors: Which Are You? Learn a little about yourself, and it may improve your performance.</span><br><span class="line"></span><br><span class="line">Target Can Aim High Target can maintain its discount image while offering pricier services and merchandise.</span><br><span class="line"></span><br><span class="line">Finance moves Ford into the black US carmaker Ford Motor returns to profit, as the money it makes from lending to customers outweighs losses from selling vehicles.</span><br></pre></td></tr></tbody></table></figure>
<h2 id="核心代码">核心代码</h2>
<p>所有可运行代码和数据都已经包含在 docker
镜像中了，下面列出核心代码</p>
<h3 id="建立索引">建立索引</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_flat</span>(<span class="hljs-params">index_name, id_list, embedding_list, num_clusters</span>):</span></span><br><span class="line">    <span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">    <span class="hljs-keyword">import</span> faiss</span><br><span class="line"></span><br><span class="line">    dim = <span class="hljs-number">768</span></span><br><span class="line">    m = <span class="hljs-number">16</span></span><br><span class="line">    </span><br><span class="line">    embeddings = np.asarray(embedding_list)</span><br><span class="line">    </span><br><span class="line">    quantiser = faiss.IndexFlatIP(dim)</span><br><span class="line">    index = faiss.IndexIVFFlat(quantiser, dim, num_clusters, faiss.METRIC_INNER_PRODUCT)</span><br><span class="line">    index.train(embeddings)  <span class="hljs-comment">## clustering</span></span><br><span class="line">    </span><br><span class="line">    ids = np.arange(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(id_list))</span><br><span class="line">    ids = np.asarray(ids.astype(<span class="hljs-string">'int64'</span>))</span><br><span class="line">    </span><br><span class="line">    index.add_with_ids(embeddings, ids)</span><br><span class="line">    <span class="hljs-built_in">print</span>(index.is_trained) </span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Total Number of Embeddings in the index"</span>, index.ntotal)</span><br><span class="line">    faiss.write_index(index, index_name)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="查询结果">查询结果</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">query</span>(<span class="hljs-params">model, index, query_str: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:</span></span><br><span class="line">    topk = <span class="hljs-number">5</span></span><br><span class="line">    q_embed = model.encode([query_str])</span><br><span class="line">    D, I = index.search(q_embed, topk)</span><br><span class="line">    <span class="hljs-built_in">print</span>(D)</span><br><span class="line">    <span class="hljs-built_in">print</span>(I)</span><br><span class="line">    <span class="hljs-keyword">return</span> I[<span class="hljs-number">0</span>].tolist()</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-sentence-transformer-chinese/" itemprop="url">Bert 中文短句相似度计算 Docker CPU镜像</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-06-17T18:45:01.000Z" itemprop="datePublished">6月 18 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            11 分钟 读完 (约 1576 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们还是制作了一个集数据，模型，代码一体的 docker
环境，给大家开箱即用体验中文BERT句子embedding体验。具体地，我们基于
<code>BERT-wwm-ext</code>，<code>huggingface transformer</code> 和
<code>sentence-transformer</code> 把玩中文句子embedding
并寻找和查询短语相似度最接近的句子。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>本期 docker 镜像获取方式为，关注 <code>MyEncyclopedia</code>
公众号后回复 <code>docker-sentence-transformer</code>
即可获取镜像地址和启动命令。</p>
<h2 id="哈工大讯飞中文-bert">哈工大讯飞中文 Bert</h2>
<p>在中文预训练领域，哈工大讯飞联合实验室（HFL）发布的基于全词Mask的中文预训练模型
<code>BERT-wwm-ext</code> 是业界的标杆之一。<code>BERT-wwm-ext</code>
支持 <code>Tensorflow</code>, <code>Pytorch</code> （通过
<code>huggingface transformer</code> 接口）以及 <code>PaddleHub</code>
的接口或者类库，使用起来十分方便。下面的代码为官网中通过
<code>huggingface transformer</code> 接口直接下载并加载到
<code>Pytorch</code> 平台中。Github 地址为
https://github.com/ymcui/Chinese-BERT-wwm</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"></span><br><span class="line">model_name = <span class="hljs-string">'hfl/chinese-bert-wwm'</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertModel.from_pretrained(model_name)</span><br></pre></td></tr></tbody></table></figure>
<p>通过 <code>huggingface transformer</code> 的好处在于
<code>sentence-transformer</code> 也支持
<code>huggingface</code>，因此，通过
<code>huggingface</code>，我们无需手动串联 <code>BERT-wwm-ext</code> 和
<code>sentence-transformer</code>，少写了不少代码。</p>
<h2 id="sentence-transformer">sentence-transformer</h2>
<p><code>sentence-transformer</code> 顾名思义是利用
<code>transformer</code>
词向量的预训练模型来生成句子级别的embedding。原理基于这篇论文
Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
（https://arxiv.org/abs/1908.10084）。基本思想直接了当：将句子中的每个词经
bert embedding 后，输入池化层
(pooling)，例如选择最简单的平均池化层，再将所有token embedding
的均值作为输出，便得到跟输入句子长度无关的一个定长的 sentence
embedding。</p>
<p><img src="/zh/2022/docker-sentence-transformer-chinese/model.png"></p>
<p>下面的代码是其官网的一个基本例子，底层通过 <code>huggingface</code>
接口自动下载并加载 bert 词向量，并计算三句英语句子的 sentence
embedding。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="hljs-string">'paraphrase-MiniLM-L6-v2'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Our sentences we like to encode</span></span><br><span class="line">sentences = [<span class="hljs-string">'This framework generates embeddings for each input sentence'</span>,</span><br><span class="line">    <span class="hljs-string">'Sentences are passed as a list of string.'</span>,</span><br><span class="line">    <span class="hljs-string">'The quick brown fox jumps over the lazy dog.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Sentences are encoded by calling model.encode()</span></span><br><span class="line">embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Print the embeddings</span></span><br><span class="line"><span class="hljs-keyword">for</span> sentence, embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(sentences, embeddings):</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Sentence:"</span>, sentence)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Embedding:"</span>, embedding)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">""</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>当然，我们也可以绕过 <code>sentence-transformer</code> API，直接使用
<code>pytorch</code> API 和 <code>huggingface</code>
手动实现平均池化层，生成句子的 sentence embedding。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel</span><br><span class="line"><span class="hljs-keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Mean Pooling - Take attention mask into account for correct averaging</span></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mean_pooling</span>(<span class="hljs-params">model_output, attention_mask</span>):</span></span><br><span class="line">    token_embeddings = model_output[<span class="hljs-number">0</span>] <span class="hljs-comment">#First element of model_output contains all token embeddings</span></span><br><span class="line">    input_mask_expanded = attention_mask.unsqueeze(-<span class="hljs-number">1</span>).expand(token_embeddings.size()).<span class="hljs-built_in">float</span>()</span><br><span class="line">    sum_embeddings = torch.<span class="hljs-built_in">sum</span>(token_embeddings * input_mask_expanded, <span class="hljs-number">1</span>)</span><br><span class="line">    sum_mask = torch.clamp(input_mask_expanded.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>), <span class="hljs-built_in">min</span>=<span class="hljs-number">1e-9</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> sum_embeddings / sum_mask</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Sentences we want sentence embeddings for</span></span><br><span class="line">sentences = [<span class="hljs-string">'This framework generates embeddings for each input sentence'</span>,</span><br><span class="line">             <span class="hljs-string">'Sentences are passed as a list of string.'</span>,</span><br><span class="line">             <span class="hljs-string">'The quick brown fox jumps over the lazy dog.'</span>]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Load AutoModel from huggingface model repository</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="hljs-string">"sentence-transformers/all-MiniLM-L6-v2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Tokenize sentences</span></span><br><span class="line">encoded_input = tokenizer(sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">128</span>, return_tensors=<span class="hljs-string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Compute token embeddings</span></span><br><span class="line"><span class="hljs-keyword">with</span> torch.no_grad():</span><br><span class="line">    model_output = model(**encoded_input)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">#Perform pooling. In this case, mean pooling</span></span><br><span class="line">sentence_embeddings = mean_pooling(model_output, encoded_input[<span class="hljs-string">'attention_mask'</span>])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="中文最相近的句子">中文最相近的句子</h2>
<p>有了上面每个组件的使用方法，让我们生成下面中文句子的embedding</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    <span class="hljs-string">'今天晚上想吃牛排'</span>,</span><br><span class="line">    <span class="hljs-string">'MyEncyclopedia公众号全栈人工智能'</span>,</span><br><span class="line">    <span class="hljs-string">'人工智能需要懂很多数学么'</span>,</span><br><span class="line">    <span class="hljs-string">'上海疫情有完没完'</span>,</span><br><span class="line">    <span class="hljs-string">'教育部：连续7天社会面无疫情 高校可组织校园招聘'</span>,</span><br><span class="line">    <span class="hljs-string">'福建舰"下水！100秒看中国航母高光时刻'</span>,</span><br><span class="line">    <span class="hljs-string">'医保承担多少核酸检测费用？压力多大？'</span>,</span><br><span class="line">    <span class="hljs-string">'张家口过度防疫整改后又被曝光：要证明牛是阴性'</span>,</span><br><span class="line">    <span class="hljs-string">'上海多家银行天天排队爆满 有老人凌晨2点开始排队'</span>,</span><br><span class="line">    <span class="hljs-string">'A股不惧海外暴跌！走出独立行情沪指收复3300点'</span>,</span><br><span class="line">    <span class="hljs-string">'俄方称已准备好重启俄乌和谈'</span>,</span><br><span class="line">    <span class="hljs-string">'《自然》：奥密克戎感染后嗅觉丧失症状比原来少了'</span></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>接着我们给出如下三个短语的查询，找到和每个查询最匹配的三个句子
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q1 = <span class="hljs-string">'码农的春天来了么'</span></span><br><span class="line">q2 = <span class="hljs-string">'国际局势'</span></span><br><span class="line">q3 = <span class="hljs-string">'健康'</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>运行结果如下</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Query: 码农的春天来了么</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">人工智能需要懂很多数学么 (Cosine Score: 0.7606)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.7498)</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.7449)</span><br><span class="line"></span><br><span class="line">----------------------------------------------</span><br><span class="line">Query: 国际局势</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">俄方称已准备好重启俄乌和谈 (Cosine Score: 0.7041)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.6897)</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.6888)</span><br><span class="line"></span><br><span class="line">----------------------------------------------</span><br><span class="line">Query: 健康</span><br><span class="line"></span><br><span class="line">Top 3 most similar sentences <span class="hljs-keyword">in</span> corpus:</span><br><span class="line">上海疫情有完没完 (Cosine Score: 0.5882)</span><br><span class="line">MyEncyclopedia公众号全栈人工智能 (Cosine Score: 0.5870)</span><br><span class="line">今天晚上想吃牛排 (Cosine Score: 0.5815)</span><br></pre></td></tr></tbody></table></figure>
<p>结果发现 <code>上海疫情有完没完</code> 是一切问题的关键。。。</p>
<h2 id="完整代码">完整代码</h2>
<p>附上完整代码</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> sentence_transformers <span class="hljs-keyword">import</span> SentenceTransformer</span><br><span class="line"></span><br><span class="line">model_name = <span class="hljs-string">'hfl/chinese-bert-wwm'</span></span><br><span class="line">model = SentenceTransformer(model_name)</span><br><span class="line"></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="hljs-string">'今天晚上想吃牛排'</span>,</span><br><span class="line">    <span class="hljs-string">'MyEncyclopedia公众号全栈人工智能'</span>,</span><br><span class="line">    <span class="hljs-string">'人工智能需要懂很多数学么'</span>,</span><br><span class="line">    <span class="hljs-string">'上海疫情有完没完'</span>,</span><br><span class="line">    <span class="hljs-string">'教育部：连续7天社会面无疫情 高校可组织校园招聘'</span>,</span><br><span class="line">    <span class="hljs-string">'福建舰"下水！100秒看中国航母高光时刻'</span>,</span><br><span class="line">    <span class="hljs-string">'医保承担多少核酸检测费用？压力多大？'</span>,</span><br><span class="line">    <span class="hljs-string">'张家口过度防疫整改后又被曝光：要证明牛是阴性'</span>,</span><br><span class="line">    <span class="hljs-string">'上海多家银行天天排队爆满 有老人凌晨2点开始排队'</span>,</span><br><span class="line">    <span class="hljs-string">'A股不惧海外暴跌！走出独立行情沪指收复3300点'</span>,</span><br><span class="line">    <span class="hljs-string">'俄方称已准备好重启俄乌和谈'</span>,</span><br><span class="line">    <span class="hljs-string">'《自然》：奥密克戎感染后嗅觉丧失症状比原来少了'</span></span><br><span class="line">]</span><br><span class="line">sentence_embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line">q1 = <span class="hljs-string">'码农的春天来了么'</span></span><br><span class="line">q2 = <span class="hljs-string">'国际局势'</span></span><br><span class="line">q3 = <span class="hljs-string">'健康'</span></span><br><span class="line">queries = [q1, q2, q3]</span><br><span class="line">query_embeddings = model.encode(queries)</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">import</span> scipy</span><br><span class="line"></span><br><span class="line">number_top_matches = <span class="hljs-number">3</span></span><br><span class="line"><span class="hljs-keyword">for</span> query, query_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(queries, query_embeddings):</span><br><span class="line">    distances = scipy.spatial.distance.cdist([query_embedding], sentence_embeddings, <span class="hljs-string">"cosine"</span>)[<span class="hljs-number">0</span>]</span><br><span class="line">    results = <span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(distances)), distances)</span><br><span class="line">    results = <span class="hljs-built_in">sorted</span>(results, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nQuery:"</span>, query)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTop {} most similar sentences in corpus:"</span>.<span class="hljs-built_in">format</span>(number_top_matches))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> idx, distance <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>:number_top_matches]:</span><br><span class="line">        <span class="hljs-built_in">print</span>(sentences[idx].strip(), <span class="hljs-string">"(Cosine Score: %.4f)"</span> % (<span class="hljs-number">1</span>-distance))</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2022/docker-flair-transformer-zero-shot/" itemprop="url">玩转transformer+flair zero shot 短文本分类：无需翻墙或额外下载模型和数据集的CPU docker镜像</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2022-06-09T18:45:01.000Z" itemprop="datePublished">6月 10 2022</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            8 分钟 读完 (约 1160 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>在这一期中，我们来体验两个知名的 NLP 预训练类库 flair 和 transformer
的 zero-shot 短文本分类。所谓zero-shot
的意思是完全不需要数据集来训练，直接掉包解决问题。和以往一样，本期的
docker 镜像已经预装了 flair，transformer，pytorch，jupyter
notebook等<strong>包依赖</strong>，并且还预先下载了 flair 和 transformer
的两个<strong>预训练模型</strong>和 <strong>yahoo
短文本主题数据集</strong>，整个 docker
镜像达到12GB，为了就是让大家无需翻墙下载额外数据或者模型，并且使用CPU就能体验最新的NLP
zero shot 文本分类。</p>
<h2 id="docker-镜像获取方式">Docker 镜像获取方式</h2>
<p>关注 <code>MyEncyclopedia</code> 公众号后回复
<code>docker-transformer-zero-shot</code>
即可获取镜像地址和启动命令。</p>
<h2 id="flair-zero-shot">Flair zero shot</h2>
<p>先来看一个 flair 短文本 zero shot 短文本分类的例子。下面的代码将句子
<strong>Spain beat Swiss for first Nations League win</strong> 归类到
<strong>politics</strong>,
<strong>sports</strong>，<strong>health</strong> 之一。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> flair.models <span class="hljs-keyword">import</span> TARSClassifier</span><br><span class="line"><span class="hljs-keyword">from</span> flair.data <span class="hljs-keyword">import</span> Sentence</span><br><span class="line"><span class="hljs-keyword">import</span> flair, torch</span><br><span class="line">flair.device = torch.device(<span class="hljs-string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span></span><br><span class="line">tars = TARSClassifier.load(<span class="hljs-string">'tars-base'</span>)</span><br><span class="line">sentence = Sentence(text)</span><br><span class="line">classes = [<span class="hljs-string">'politics'</span>, <span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>]</span><br><span class="line">tars.predict_zero_shot(sentence, classes)</span><br><span class="line"></span><br><span class="line"><span class="hljs-built_in">print</span>(sentence)</span><br><span class="line"><span class="hljs-built_in">print</span>(sentence.to_dict())</span><br></pre></td></tr></tbody></table></figure>
<p>最后两行输出如下，<code>all labels</code> 字段显示概率最高的是
<code>sports</code>类别，达到 0.99+。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Sentence: <span class="hljs-string">"Spain beat Swiss for first Nations League win"</span> → sports (0.9952)</span><br><span class="line">{</span><br><span class="line">  <span class="hljs-string">'text'</span>: <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span>, </span><br><span class="line">  <span class="hljs-string">'all labels'</span>: [{<span class="hljs-string">'value'</span>: <span class="hljs-string">'sports'</span>, <span class="hljs-string">'confidence'</span>: 0.9952359795570374}]</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line">注意，在上面的代码中，`flair.device = torch.device(<span class="hljs-string">'cpu'</span>)` 强制使用了 cpu 资源，否则 flair 默认使用 gpu 会报错。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-comment">## Transformer zero shot</span></span><br><span class="line">再来看看大名鼎鼎的 transformer zero shot 的结果。这里使用了默认的 transformer zero shot 分类的模型 Transformer Bart，小伙伴们可以使用其他模型，但是有些不兼容 zero shot 分类。代码如下</span><br><span class="line"></span><br><span class="line">​```python</span><br><span class="line">from transformers import pipeline</span><br><span class="line"></span><br><span class="line">text = <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span></span><br><span class="line">classes = [<span class="hljs-string">'politics'</span>, <span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>]</span><br><span class="line">classifier = pipeline(<span class="hljs-string">"zero-shot-classification"</span>, device=-1)</span><br><span class="line">result = classifier(text, classes, multi_label=False)</span><br><span class="line"></span><br><span class="line"><span class="hljs-built_in">print</span>(result)</span><br><span class="line"><span class="hljs-built_in">print</span>(result[<span class="hljs-string">'labels'</span>][0])</span><br></pre></td></tr></tbody></table></figure>
<p>最后两行输出为</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">  <span class="hljs-string">'sequence'</span>: <span class="hljs-string">'Spain beat Swiss for first Nations League win'</span>, </span><br><span class="line">  <span class="hljs-string">'labels'</span>: [<span class="hljs-string">'sports'</span>, <span class="hljs-string">'health'</span>, <span class="hljs-string">'politics'</span>], </span><br><span class="line">  <span class="hljs-string">'scores'</span>: [0.9476209878921509, 0.03594793379306793, 0.016431059688329697]</span><br><span class="line">}</span><br><span class="line">sports</span><br></pre></td></tr></tbody></table></figure>
<p><code>result</code> 的
<code>labels</code>中会按照最大概率排序输出类别和对应的分数。对于这句句子，也分的相当准确，<code>sports</code>
为 0.94+。</p>
<p>也注意到 <code>pipeline("zero-shot-classification", device=-1)</code>
语句中 <strong>-1</strong> 表示强制使用 cpu。</p>
<h2 id="yahoo-短文本主题数据分类效果">Yahoo 短文本主题数据分类效果</h2>
<p>最后，来看一个真实数据集中这两者的实际效果，<code>yahoo_answers_topics</code>
是
<code>huggingface</code>的一个短文本分类数据集，可以通过以下命令下载并加载</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yahoo = load_dataset(<span class="hljs-string">'yahoo_answers_topics'</span>)</span><br></pre></td></tr></tbody></table></figure>
<p>它的具体类别为</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line"><span class="hljs-string">'Society &amp; Culture'</span>, </span><br><span class="line"><span class="hljs-string">'Science &amp; Mathematics'</span>, </span><br><span class="line"><span class="hljs-string">'Health'</span>, </span><br><span class="line"><span class="hljs-string">'Education &amp; Reference'</span>, </span><br><span class="line"><span class="hljs-string">'Computers &amp; Internet'</span>, </span><br><span class="line"><span class="hljs-string">'Sports'</span>, </span><br><span class="line"><span class="hljs-string">'Business &amp; Finance'</span>, </span><br><span class="line"><span class="hljs-string">'Entertainment &amp; Music'</span>, </span><br><span class="line"><span class="hljs-string">'Family &amp; Relationships'</span>, </span><br><span class="line"><span class="hljs-string">'Politics &amp; Government'</span></span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>由于数量比较大，这里只取随机的1000个来测试，一些数据点如下</p>
<table>
<colgroup>
<col style="width: 74%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Text</th>
<th>Topic</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A Permanent resident of Canada may stay out of Canada 3 years
without losing status.</td>
<td>Politics &amp; Government</td>
</tr>
<tr class="even">
<td>The official major league opening game occurred on April 10, 2006,
as the Cardinals defeated the Milwaukee Brewers 6-4. (Day Game)</td>
<td>Sports</td>
</tr>
<tr class="odd">
<td>Hold down the Command key while dragging and dropping files.</td>
<td>Computers &amp; Internet</td>
</tr>
</tbody>
</table>
<p>接着，对于每条短文本用 flair 和 transformer
来预测类别，最终统计准确率。</p>
<p>结果是 flair 准确率为 <strong>0.275</strong>，Transformer Bart 为
<strong>0.392</strong>，果然 transformer 显著胜出。其实，在
Yahoo数据集上取得 0.3 - 0.4
左右的效果已经不错了，毕竟有十个类别，全随机的准确率是
0.1。如果大家觉得这个效果一般的话，可以试试 tweet
情感分类数据集（具体在下面的链接中），Transformer 能达到惊人的
0.73。</p>
<p>下面附部分代码，完整代码可以从镜像中获得，或者感兴趣的小伙伴也可以访问</p>
<p>https://github.com/nlptown/nlp-notebooks/blob/master/Zero-Shot%20Text%20Classification.ipynb
获取所有五个数据集的代码，不过由于类库版本的关系，部分代码和模型或数据无法兼容，需要自行调试。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_flair</span>(<span class="hljs-params">dataset, default_name=<span class="hljs-string">'neutral'</span></span>):</span></span><br><span class="line">    classifier = TARSClassifier.load(<span class="hljs-string">'tars-base'</span>)</span><br><span class="line">    total, correct = <span class="hljs-number">0</span>, <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> item, gold_label_idx <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">zip</span>(dataset[<span class="hljs-string">"test_texts"</span>], dataset[<span class="hljs-string">"test_labels"</span>]),</span><br><span class="line">                                     total=<span class="hljs-built_in">len</span>(dataset[<span class="hljs-string">"test_texts"</span>])):</span><br><span class="line">        sentence = Sentence(item)</span><br><span class="line">        classifier.predict_zero_shot(sentence, dataset[<span class="hljs-string">"class_names"</span>])</span><br><span class="line">        sorted_labels = <span class="hljs-built_in">sorted</span>(sentence.to_dict()[<span class="hljs-string">'all labels'</span>], key=<span class="hljs-keyword">lambda</span> k: k[<span class="hljs-string">'confidence'</span>], reverse=<span class="hljs-literal">True</span>)</span><br><span class="line">        gold_label = dataset[<span class="hljs-string">"class_names"</span>][gold_label_idx]</span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(sorted_labels) &gt; <span class="hljs-number">0</span>:</span><br><span class="line">            predicted_label = sorted_labels[<span class="hljs-number">0</span>][<span class="hljs-string">'value'</span>]</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            predicted_label = default_name</span><br><span class="line">        <span class="hljs-keyword">if</span> predicted_label == gold_label:</span><br><span class="line">            correct += <span class="hljs-number">1</span></span><br><span class="line">        total += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> correct / total</span><br><span class="line">    </span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_huggingface</span>(<span class="hljs-params">dataset</span>):</span></span><br><span class="line">    classifier = pipeline(<span class="hljs-string">"zero-shot-classification"</span>, device=-<span class="hljs-number">1</span>)</span><br><span class="line">    correct = <span class="hljs-number">0</span></span><br><span class="line">    predictions, gold_labels = [], []</span><br><span class="line">    <span class="hljs-keyword">for</span> text, gold_label_idx <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">zip</span>(dataset[<span class="hljs-string">"test_texts"</span>], dataset[<span class="hljs-string">"test_labels"</span>]),</span><br><span class="line">                                     total=<span class="hljs-built_in">len</span>(dataset[<span class="hljs-string">"test_texts"</span>])):</span><br><span class="line"></span><br><span class="line">        result = classifier(text, dataset[<span class="hljs-string">"class_names"</span>], multi_label=<span class="hljs-literal">False</span>)</span><br><span class="line">        predicted_label = result[<span class="hljs-string">'labels'</span>][<span class="hljs-number">0</span>]</span><br><span class="line"></span><br><span class="line">        gold_label = dataset[<span class="hljs-string">"class_names"</span>][gold_label_idx]</span><br><span class="line"></span><br><span class="line">        predictions.append(predicted_label)</span><br><span class="line">        gold_labels.append(gold_label)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> predicted_label == gold_label:</span><br><span class="line">            correct += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    accuracy = correct / <span class="hljs-built_in">len</span>(predictions)</span><br><span class="line">    <span class="hljs-keyword">return</span> accuracy</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/tags/nlp/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/tags/nlp/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>