<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>标签: Reinforcement Learning - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/tags/Reinforcement-Learning/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/tags/Reinforcement-Learning/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#Reinforcement Learning</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2021/rl-ppo-1984/" itemprop="url">深度强化学习之：PPO训练红白机1942</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2021-05-08T18:45:01.000Z" itemprop="datePublished">5月 9 2021</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            19 分钟 读完 (约 2804 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>本篇是深度强化学习动手系列文章，自MyEncyclopedia公众号文章<a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457480980&amp;idx=1&amp;sn=ef919157726080caf9eebb3aaa8314f9&amp;chksm=87bc993ab0cb102c399687912b65d6a38994f2cb9ac87b531bf0ca2eaf119b8db0ea84e45916&amp;scene=21#wechat_redirect">深度强化学习之：DQN训练超级玛丽闯关</a>发布后收到不少关注和反馈，这一期，让我们实现目前主流深度强化学习算法PPO来打另一个红白机经典游戏1942。</p>
<p>相关文章链接如下：</p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457481574&amp;idx=1&amp;sn=712e92f15d5488dcb73470c9a6420a04&amp;chksm=87bc8748b0cb0e5eb839afe27712dd990ad3d4a2083b923593a499f8a1c4ada37e6dfa1db7e0&amp;scene=21#wechat_redirect">强化学习开源环境集</a></p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457481652&amp;idx=1&amp;sn=ddc083c1a7ce4930b4302384c507cbd5&amp;chksm=87bc879ab0cb0e8c6e68d5f7f5638e09b7167576edc7c8ed075531ec221630356eea7ff4ad56&amp;scene=21#wechat_redirect">视频论文解读：PPO算法</a></p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457481622&amp;idx=1&amp;sn=dbdce93433de31c68e99da08afb8699c&amp;chksm=87bc87b8b0cb0eaed487d3c553ad565513b07c887692bce33d83b1424926cd367d5d5f75ea5f&amp;scene=21#wechat_redirect">视频论文解读：组合优化的强化学习方法</a></p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457481507&amp;idx=1&amp;sn=0ae5e2c434973d35d45f3b794ffe52e3&amp;chksm=87bc870db0cb0e1ba11aa778283faee8fbba750e6208d56d663b689ee7ccdb31cb036365a41e&amp;scene=21#wechat_redirect">解读TRPO论文，深度强化学习结合传统优化方法</a></p>
<p><a target="_blank" rel="noopener" href="http://mp.weixin.qq.com/s?__biz=MzA4NzkxNzM3Nw==&amp;mid=2457481304&amp;idx=1&amp;sn=35eb9c72f2ed4feaff57db64e3dd0932&amp;chksm=87bc8676b0cb0f6013a71f5e16229e48e63a9fc30bd8695a761904a915f9482a224cb6e19217&amp;scene=21#wechat_redirect">解读深度强化学习基石论文：函数近似的策略梯度方法</a></p>
<h2 id="nes-1942-环境安装">NES 1942 环境安装</h2>
<p>红白机游戏环境可以由OpenAI Retro来模拟，OpenAI Retro还在 Gym
集成了其他的经典游戏环境，包括Atari 2600，GBA，SNES等。</p>
<p>不过，受到版权原因，除了一些基本的rom，大部分游戏需要自行获取rom。</p>
<p>环境准备部分相关代码如下</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym-retro</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m retro.import /path/to/your/ROMs/directory/</span><br></pre></td></tr></tbody></table></figure>
<h2 id="openai-gym-输入动作类型">OpenAI Gym 输入动作类型</h2>
<p>在创建 retro
环境时，可以在retro.make中通过参数use_restricted_actions指定 action
space，即按键的配置。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env = retro.make(game=<span class="hljs-string">'1942-Nes'</span>, use_restricted_actions=retro.Actions.FILTERED)</span><br></pre></td></tr></tbody></table></figure>
<p>可选参数如下，FILTERED，DISCRETE和MULTI_DISCRETE
都可以指定过滤的动作，过滤动作需要通过配置文件加载。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actions</span>(<span class="hljs-params">Enum</span>):</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Different settings for the action space of the environment</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    ALL = <span class="hljs-number">0</span>  <span class="hljs-comment">#: MultiBinary action space with no filtered actions</span></span><br><span class="line">    FILTERED = <span class="hljs-number">1</span>  <span class="hljs-comment">#: MultiBinary action space with invalid or not allowed actions filtered out</span></span><br><span class="line">    DISCRETE = <span class="hljs-number">2</span>  <span class="hljs-comment">#: Discrete action space for filtered actions</span></span><br><span class="line">    MULTI_DISCRETE = <span class="hljs-number">3</span>  <span class="hljs-comment">#: MultiDiscete action space for filtered actions</span></span><br></pre></td></tr></tbody></table></figure>
<p>DISCRETE和MULTI_DISCRETE 是 Gym 里的
Action概念，它们的基类都是gym.spaces.Space，可以通过
sample()方法采样，下面具体一一介绍。</p>
<ul>
<li>Discrete：对应一维离散空间，例如，Discrete(n=4) 表示 [0, 3]
范围的整数。</li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> Discrete</span><br><span class="line">space = Discrete(<span class="hljs-number">4</span>)</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure>
<p>输出是</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>Box：对应多维连续空间，每一维的范围可以用 [low，high] 指定。
举例，Box(low=-1.0, high=2, shape=(3, 4,), dtype=np.float32) 表示 shape
是 [3, 4]，每个范围在 [-1, 2] 的float32型 tensor。</li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> Box</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">space = Box(low=-<span class="hljs-number">1.0</span>, high=<span class="hljs-number">2.0</span>, shape=(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>), dtype=np.float32)</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure>
<p>输出是 </p><figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[-0.7538084   0.96901214  0.38641307 -0.05045208]</span><br><span class="line"> [-0.85486996  1.3516271   0.3222616   1.2540635 ]</span><br><span class="line"> [-0.29908678 -0.8970335   1.4869047   0.7007356 ]]</span><br></pre></td></tr></tbody></table></figure><p></p>
<ul>
<li>MultiBinary: 0或1的多维离散空间。例如，MultiBinary([3,2]) 表示 shape
是3x2的0或1的tensor。 <figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> MultiBinary</span><br><span class="line">space = MultiBinary([<span class="hljs-number">3</span>,<span class="hljs-number">2</span>])</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<p>输出是</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[1 0]</span><br><span class="line"> [1 1]</span><br><span class="line"> [0 0]]</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>MultiDiscrete：多维整型离散空间。例如，MultiDiscrete([5,2,2])
表示三维Discrete空间，第一维范围在 [0-4]，第二，三维范围在[0-1]。</li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> MultiDiscrete</span><br><span class="line">space = MultiDiscrete([<span class="hljs-number">5</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>])</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure>
<p>输出是</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2 1 0]</span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>Tuple：组合成 tuple 复合空间。举例来说，可以将
Box，Discrete，Discrete组成tuple 空间：Tuple(spaces=(Box(low=-1.0,
high=1.0, shape=(3,), dtype=np.float32), Discrete(n=3),
Discrete(n=2)))</li>
</ul>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> *</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line">space = <span class="hljs-type">Tuple</span>(spaces=(Box(low=-<span class="hljs-number">1.0</span>, high=<span class="hljs-number">1.0</span>, shape=(<span class="hljs-number">3</span>,), dtype=np.float32), Discrete(n=<span class="hljs-number">3</span>), Discrete(n=<span class="hljs-number">2</span>)))</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure>
<p>输出是</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([ 0.22640526,  0.75286865, -0.6309239 ], dtype=float32), 0, 1)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>Dict：组合成有名字的复合空间。例如，Dict({'position':Discrete(2),
'velocity':Discrete(3)}) <figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.spaces <span class="hljs-keyword">import</span> *</span><br><span class="line">space = <span class="hljs-type">Dict</span>({<span class="hljs-string">'position'</span>:Discrete(<span class="hljs-number">2</span>), <span class="hljs-string">'velocity'</span>:Discrete(<span class="hljs-number">3</span>)})</span><br><span class="line"><span class="hljs-built_in">print</span>(space.sample())</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<p>输出是</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([(<span class="hljs-string">'position'</span>, 1), (<span class="hljs-string">'velocity'</span>, 1)])</span><br></pre></td></tr></tbody></table></figure>
<h2 id="nes-1942-动作空间配置">NES 1942 动作空间配置</h2>
<p>了解了 gym/retro 的动作空间，我们来看看1942的默认动作空间
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">env = retro.make(game=<span class="hljs-string">'1942-Nes'</span>)</span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">"The size of action is: "</span>, env.action_space.shape)</span><br></pre></td></tr></tbody></table></figure><p></p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The size of action is:  (9,)</span><br></pre></td></tr></tbody></table></figure>
<p>表示有9个 Discrete 动作，包括 start, select这些控制键。</p>
<p>从训练1942角度来说，我们希望指定最少的有效动作取得最好的成绩。根据经验，我们知道这个游戏最重要的键是4个方向加上
fire
键。限定游戏动作空间，官方的做法是在创建游戏环境时，指定预先生成的动作输入配置文件。但是这个方式相对麻烦，我们采用了直接指定按键的二进制表示来达到同样的目的，此时，需要设置
use_restricted_actions=retro.Actions.FILTERED。</p>
<p>下面的代码限制了6种按键，并随机play。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">action_list = [</span><br><span class="line">    <span class="hljs-comment"># No Operation</span></span><br><span class="line">    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">    <span class="hljs-comment"># Left</span></span><br><span class="line">    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">    <span class="hljs-comment"># Right</span></span><br><span class="line">    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">    <span class="hljs-comment"># Down</span></span><br><span class="line">    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">    <span class="hljs-comment"># Up</span></span><br><span class="line">    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">    <span class="hljs-comment"># B</span></span><br><span class="line">    [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_play</span>(<span class="hljs-params">env, action_list, sleep_seconds=<span class="hljs-number">0.01</span></span>):</span></span><br><span class="line">    env.viewer = <span class="hljs-literal">None</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    score = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        time.sleep(sleep_seconds)</span><br><span class="line">        action = np.random.randint(<span class="hljs-built_in">len</span>(action_list))</span><br><span class="line"></span><br><span class="line">        next_state, reward, done, _ = env.step(action_list[action])</span><br><span class="line">        state = next_state</span><br><span class="line">        score += reward</span><br><span class="line">        <span class="hljs-keyword">if</span> done:</span><br><span class="line">            <span class="hljs-built_in">print</span>(<span class="hljs-string">"Episode Score: "</span>, score)</span><br><span class="line">            env.reset()</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">            </span><br><span class="line">env = retro.make(game=<span class="hljs-string">'1942-Nes'</span>, use_restricted_actions=retro.Actions.FILTERED)</span><br><span class="line">random_play(env, action_list)</span><br></pre></td></tr></tbody></table></figure>
<p>来看看其游戏效果，全随机死的还是比较快。</p>
<figure>
<img src="/zh/2021/rl-ppo-1984/random.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="图像输入处理">图像输入处理</h2>
<p>一般对于通过屏幕像素作为输入的RL
end-to-end训练来说，对图像做预处理很关键。因为原始图像较大，一方面我们希望能尽量压缩图像到比较小的tensor，另一方面又要保证关键信息不丢失，比如子弹的图像不能因为图片缩小而消失。另外的一个通用技巧是将多个连续的frame合并起来组成立体的frame，这样可以有效表示连贯动作。</p>
<p>下面的代码通过 pipeline 将游戏每帧原始图像从shape (224, 240, 3)
转换成 (4, 84, 84)，也就是原始的 width=224，height=240，rgb=3转换成
width=84，height=240，stack_size=4的黑白图像。具体 pipeline为</p>
<ol type="1">
<li><p>MaxAndSkipEnv：每两帧过滤一帧图像，减少数据量。</p></li>
<li><p>FrameDownSample：down sample 图像到指定小分辨率
84x84，并从彩色降到黑白。</p></li>
<li><p>FrameBuffer：合并连续的4帧，形成 (4, 84, 84) 的图像输入</p></li>
</ol>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_env</span>():</span></span><br><span class="line">    env = retro.make(game=<span class="hljs-string">'1942-Nes'</span>, use_restricted_actions=retro.Actions.FILTERED)</span><br><span class="line">    env = MaxAndSkipEnv(env, skip=<span class="hljs-number">2</span>)</span><br><span class="line">    env = FrameDownSample(env, (<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))</span><br><span class="line">    env = FrameBuffer(env, <span class="hljs-number">4</span>)</span><br><span class="line">    env.seed(<span class="hljs-number">0</span>)</span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<p>观察图像维度变换</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">env = retro.make(game=<span class="hljs-string">'1942-Nes'</span>, use_restricted_actions=retro.Actions.FILTERED)</span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Initial shape: "</span>, env.observation_space.shape)</span><br><span class="line"></span><br><span class="line">env = build_env(env)</span><br><span class="line"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Processed shape: "</span>, env.observation_space.shape)</span><br></pre></td></tr></tbody></table></figure>
<p>确保shape 从 (224, 240, 3) 转换成 (4, 84, 84)</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Initial shape:  (224, 240, 3)</span><br><span class="line">Processed shape:  (4, 84, 84)</span><br></pre></td></tr></tbody></table></figure>
<p>FrameDownSample实现如下，我们使用了 cv2
类库来完成黑白化和图像缩放</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FrameDownSample</span>(<span class="hljs-params">ObservationWrapper</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env, exclude, width=<span class="hljs-number">84</span>, height=<span class="hljs-number">84</span></span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(FrameDownSample, self).__init__(env)</span><br><span class="line">        self.exclude = exclude</span><br><span class="line">        self.observation_space = Box(low=<span class="hljs-number">0</span>,</span><br><span class="line">                                     high=<span class="hljs-number">255</span>,</span><br><span class="line">                                     shape=(width, height, <span class="hljs-number">1</span>),</span><br><span class="line">                                     dtype=np.uint8)</span><br><span class="line">        self._width = width</span><br><span class="line">        self._height = height</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">observation</span>(<span class="hljs-params">self, observation</span>):</span></span><br><span class="line">        <span class="hljs-comment"># convert image to gray scale</span></span><br><span class="line">        screen = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># crop screen [up: down, left: right]</span></span><br><span class="line">        screen = screen[self.exclude[<span class="hljs-number">0</span>]:self.exclude[<span class="hljs-number">2</span>], self.exclude[<span class="hljs-number">3</span>]:self.exclude[<span class="hljs-number">1</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># to float, and normalized</span></span><br><span class="line">        screen = np.ascontiguousarray(screen, dtype=np.float32) / <span class="hljs-number">255</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># resize image</span></span><br><span class="line">        screen = cv2.resize(screen, (self._width, self._height), interpolation=cv2.INTER_AREA)</span><br><span class="line">        <span class="hljs-keyword">return</span> screen</span><br></pre></td></tr></tbody></table></figure>
<p>MaxAndSkipEnv，每两帧过滤一帧</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MaxAndSkipEnv</span>(<span class="hljs-params">Wrapper</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env=<span class="hljs-literal">None</span>, skip=<span class="hljs-number">4</span></span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(MaxAndSkipEnv, self).__init__(env)</span><br><span class="line">        self._obs_buffer = deque(maxlen=<span class="hljs-number">2</span>)</span><br><span class="line">        self._skip = skip</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action</span>):</span></span><br><span class="line">        total_reward = <span class="hljs-number">0.0</span></span><br><span class="line">        done = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self._skip):</span><br><span class="line">            obs, reward, done, info = self.env.step(action)</span><br><span class="line">            self._obs_buffer.append(obs)</span><br><span class="line">            total_reward += reward</span><br><span class="line">            <span class="hljs-keyword">if</span> done:</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br><span class="line">        max_frame = np.<span class="hljs-built_in">max</span>(np.stack(self._obs_buffer), axis=<span class="hljs-number">0</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> max_frame, total_reward, done, info</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        self._obs_buffer.clear()</span><br><span class="line">        obs = self.env.reset()</span><br><span class="line">        self._obs_buffer.append(obs)</span><br><span class="line">        <span class="hljs-keyword">return</span> obs</span><br></pre></td></tr></tbody></table></figure>
<p>FrameBuffer，将最近的4帧合并起来</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">FrameBuffer</span>(<span class="hljs-params">ObservationWrapper</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env, num_steps, dtype=np.float32</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(FrameBuffer, self).__init__(env)</span><br><span class="line">        obs_space = env.observation_space</span><br><span class="line">        self._dtype = dtype</span><br><span class="line">        self.observation_space = Box(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">255</span>, shape=(num_steps, obs_space.shape[<span class="hljs-number">0</span>], obs_space.shape[<span class="hljs-number">1</span>]), dtype=self._dtype)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        frame = self.env.reset()</span><br><span class="line">        self.buffer = np.stack(arrays=[frame, frame, frame, frame])</span><br><span class="line">        <span class="hljs-keyword">return</span> self.buffer</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">observation</span>(<span class="hljs-params">self, observation</span>):</span></span><br><span class="line">        self.buffer[:-<span class="hljs-number">1</span>] = self.buffer[<span class="hljs-number">1</span>:]</span><br><span class="line">        self.buffer[-<span class="hljs-number">1</span>] = observation</span><br><span class="line">        <span class="hljs-keyword">return</span> self.buffer</span><br></pre></td></tr></tbody></table></figure>
<p>最后，visualize
处理后的图像，同样还是在随机play中，确保关键信息不丢失</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_play_preprocessed</span>(<span class="hljs-params">env, action_list, sleep_seconds=<span class="hljs-number">0.01</span></span>):</span></span><br><span class="line">    <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">    env.viewer = <span class="hljs-literal">None</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    score = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>):</span><br><span class="line">        time.sleep(sleep_seconds)</span><br><span class="line">        action = np.random.randint(<span class="hljs-built_in">len</span>(action_list))</span><br><span class="line"></span><br><span class="line">        plt.imshow(state[-<span class="hljs-number">1</span>], cmap=<span class="hljs-string">"gray"</span>)</span><br><span class="line">        plt.title(<span class="hljs-string">'Pre Processed image'</span>)</span><br><span class="line">        plt.pause(sleep_seconds)</span><br><span class="line"></span><br><span class="line">        next_state, reward, done, _ = env.step(action_list[action])</span><br><span class="line">        state = next_state</span><br><span class="line">        score += reward</span><br><span class="line">        <span class="hljs-keyword">if</span> done:</span><br><span class="line">            <span class="hljs-built_in">print</span>(<span class="hljs-string">"Episode Score: "</span>, score)</span><br><span class="line">            env.reset()</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br></pre></td></tr></tbody></table></figure>
<p>matplotlib 动画输出</p>
<figure>
<img src="/zh/2021/rl-ppo-1984/preprocess.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="cnn-actor-critic">CNN Actor &amp; Critic</h2>
<p>Actor 和 Critic 模型相同，输入是 (4, 84, 84) 的图像，输出是 [0, 5]
的action index。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Actor</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_shape, num_actions</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        self.input_shape = input_shape</span><br><span class="line">        self.num_actions = num_actions</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(self.feature_size(), <span class="hljs-number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="hljs-number">512</span>, self.num_actions),</span><br><span class="line">            nn.Softmax(dim=<span class="hljs-number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        dist = Categorical(x)</span><br><span class="line">        <span class="hljs-keyword">return</span> dist</span><br></pre></td></tr></tbody></table></figure>
<h2 id="ppo核心代码">PPO核心代码</h2>
<p>先计算 <span class="math inline">\(r_t(\theta)\)</span>，这里采用了一个技巧，对 <span class="math inline">\(\pi_\theta\)</span> 取 log，相减再取
exp，这样可以增强数值稳定性。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dist = self.actor_net(state)</span><br><span class="line">new_log_probs = dist.log_prob(action)</span><br><span class="line">ratio = (new_log_probs - old_log_probs).exp()</span><br><span class="line">surr1 = ratio * advantage</span><br></pre></td></tr></tbody></table></figure>
<p>surr1 对应PPO论文中的 <span class="math inline">\(L^{CPI}\)</span></p>
<figure>
<img src="/zh/2021/rl-ppo-1984/L_CPI.PNG">
<figcaption>
</figcaption>
</figure>
<p>然后计算 surr2，对应 <span class="math inline">\(L^{CLIP}\)</span>
中的 clip 部分，clip可以由 torch.clamp 函数实现。<span class="math inline">\(L^{CLIP}\)</span> 则对应 actor_loss。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">surr2 = torch.clamp(ratio, <span class="hljs-number">1.0</span> - self.clip_param, <span class="hljs-number">1.0</span> + self.clip_param) * advantage</span><br><span class="line">actor_loss = - torch.<span class="hljs-built_in">min</span>(surr1, surr2).mean()</span><br></pre></td></tr></tbody></table></figure>
<figure>
<img src="/zh/2021/rl-ppo-1984/L_CLIP.PNG">
<figcaption>
</figcaption>
</figure>
<p>最后，计算总的 loss <span class="math inline">\(L_t^{CLIP+VF+S}\)</span>，包括
actor_loss，critic_loss 和 policy的 entropy。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">entropy = dist.entropy().mean()</span><br><span class="line"></span><br><span class="line">critic_loss = (return_ - value).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean()</span><br><span class="line">loss = actor_loss + <span class="hljs-number">0.5</span> * critic_loss - <span class="hljs-number">0.001</span> * entropy</span><br></pre></td></tr></tbody></table></figure>
<figure>
<img src="/zh/2021/rl-ppo-1984/loss.PNG">
<figcaption>
</figcaption>
</figure>
<p>上述完整代码如下</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.ppo_epoch):</span><br><span class="line">    <span class="hljs-keyword">for</span> state, action, old_log_probs, return_, advantage <span class="hljs-keyword">in</span> sample_batch():</span><br><span class="line">        dist = self.actor_net(state)</span><br><span class="line">        value = self.critic_net(state)</span><br><span class="line"></span><br><span class="line">        entropy = dist.entropy().mean()</span><br><span class="line">        new_log_probs = dist.log_prob(action)</span><br><span class="line"></span><br><span class="line">        ratio = (new_log_probs - old_log_probs).exp()</span><br><span class="line">        surr1 = ratio * advantage</span><br><span class="line">        surr2 = torch.clamp(ratio, <span class="hljs-number">1.0</span> - self.clip_param, <span class="hljs-number">1.0</span> + self.clip_param) * advantage</span><br><span class="line"></span><br><span class="line">        actor_loss = - torch.<span class="hljs-built_in">min</span>(surr1, surr2).mean()</span><br><span class="line">        critic_loss = (return_ - value).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean()</span><br><span class="line"></span><br><span class="line">        loss = actor_loss + <span class="hljs-number">0.5</span> * critic_loss - <span class="hljs-number">0.001</span> * entropy</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># Minimize the loss</span></span><br><span class="line">        self.actor_optimizer.zero_grad()</span><br><span class="line">        self.critic_optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.actor_optimizer.step()</span><br><span class="line">        self.critic_optimizer.step()</span><br></pre></td></tr></tbody></table></figure>
<p>补充一下 GAE 的计算，advantage 根据公式</p>
<figure>
<img src="/zh/2021/rl-ppo-1984/gae.PNG">
<figcaption>
</figcaption>
</figure>
<p>可以转换成如下代码</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_gae</span>(<span class="hljs-params">self, next_value</span>):</span></span><br><span class="line">    gae = <span class="hljs-number">0</span></span><br><span class="line">    returns = []</span><br><span class="line">    values = self.values + [next_value]</span><br><span class="line">    <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.rewards))):</span><br><span class="line">        delta = self.rewards[step] + self.gamma * values[step + <span class="hljs-number">1</span>] * self.masks[step] - values[step]</span><br><span class="line">        gae = delta + self.gamma * self.tau * self.masks[step] * gae</span><br><span class="line">        returns.insert(<span class="hljs-number">0</span>, gae + values[step])</span><br><span class="line">    <span class="hljs-keyword">return</span> returns</span><br></pre></td></tr></tbody></table></figure>
<h2 id="外层-training-代码">外层 Training 代码</h2>
<p>外层调用代码基于随机 play 的逻辑，agent.act()封装了采样和 forward
prop，agent.step() 则封装了 backprop 和参数学习迭代的逻辑。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> i_episode <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_epoch + <span class="hljs-number">1</span>, n_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    score = <span class="hljs-number">0</span></span><br><span class="line">    timestamp = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">while</span> timestamp &lt; <span class="hljs-number">10000</span>:</span><br><span class="line">        action, log_prob, value = agent.act(state)</span><br><span class="line">        next_state, reward, done, info = env.step(action_list[action])</span><br><span class="line">        score += reward</span><br><span class="line">        timestamp += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        agent.step(state, action, value, log_prob, reward, done, next_state)</span><br><span class="line">        <span class="hljs-keyword">if</span> done:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            state = next_state</span><br></pre></td></tr></tbody></table></figure>
<h2 id="训练结果">训练结果</h2>
<p>让我们来看看学习的效果吧，注意我们的飞机学到了一些关键的技巧，躲避子弹；飞到角落尽快击毙敌机；一定程度预测敌机出现的位置并预先走到位置。</p>
<div class="bili_video"><iframe src="https://player.bilibili.com/player.html?aid=NaN&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" width="544" height="452" allowfullscreen="true"> </iframe></div>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2021/vlog-paper-neural-combinatorial-optimization/" itemprop="url">视频论文解读：组合优化的强化学习方法</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2021-02-14T18:45:01.000Z" itemprop="datePublished">2月 15 2021</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            几秒 读完 (约 7 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><h2 id="youtube">YouTube</h2>
<style>.embed-container {
    position: relative;
    padding-bottom: 56.25%;
    height: 0;
    overflow: hidden;
    max-width: 100%;
  }
  .embed-container iframe, .embed-container object, .embed-container embed {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }
  </style>

<div class="embed-container"><iframe src="https://www.youtube.com/embed/XRZESq5njZU" allowfullscreen="" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"></iframe></div>
<h2 id="bilibili">BiliBili</h2>
<iframe src="//player.bilibili.com/player.html?aid=459236231&amp;bvid=BV1n5411E7MG&amp;cid=297866423&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true">
</iframe>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/paper-rl-trpo-2017/" itemprop="url">解读TRPO论文，一种深度强化学习和传统优化方法结合的方法</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-12-24T18:45:01.000Z" itemprop="datePublished">12月 25 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            24 分钟 读完 (约 3616 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><figure>
<img src="/zh/2020/paper-rl-trpo-2017/0-paper-title.png">
</figure>
<p>导读：本论文由Berkeley 的几位大神于2015年发表于 JMLR（Journal of
Machine Learning Research）。深度强化学习算法例如DQN或者PG（Policy
Gradient）都无法避免训练不稳定的问题：在训练过程中效果容易退化并且很难恢复。针对这个通病，TRPO采用了传统优化算法中的trust
region方法，以保证每一步迭代能够获得效果提升，直至收敛到局部最优点。</p>
<p>本篇论文涉及到的知识点比较多，不仅建立在强化学习领域经典论文的结论：Kakade
&amp; Langford 于2002 年发表的 Approximately Optimal Approximate
Reinforcement Learning
关于优化目标的近似目标和重要性采样，也涉及到传统优化方法 trust region
的建模和其具体的矩阵近似数值算法。读懂本论文，对于深度强化学习及其优化方法可以有比较深入的理解。本论文附录的证明部分由于更为深奥和冗长，在本文中不做具体讲解，但是也建议大家能够仔细研读。</p>
<p>阅读本论文需要注意的是，这里解读的版本是arxiv的版本，这个版本带有附录，不同于
JMLR的版本的是，arxiv版本中用reward函数而后者用cost函数，优化方向相反。</p>
<p>arxiv 下载链接为 https://arxiv.org/pdf/1502.05477.pdf</p>
<h2 id="论文框架">0. 论文框架</h2>
<p>本论文解决的目标是希望每次迭代参数能保证提升效果，具体想法是利用优化领域的
trust
region方法（中文可以翻译成置信域方法或信赖域方法），通过参数在trust
region范围中去找到一定能提升的下一次迭代。</p>
<p>本论文框架如下</p>
<ol type="1">
<li><p>首先，引入Kakade &amp; Langford 论文 Approximately Optimal
Approximate Reinforcement Learning
中关于近似优化目标的结论。（论文第二部分）</p></li>
<li><p>基于 Kakade 论文中使用mixture
policy保证每一步效果提升的方法，扩展到一般随机策略，引入策略分布的total
variation divergence作为约束。（论文第三部分）</p></li>
<li><p>将total variation divergence约束替换成平均 KL divergence
约束，便于使用蒙特卡洛方法通过采样来生成每一步的具体优化问题。（论文第四，五部分）</p></li>
<li><p>给出解决优化问题的具体算法，将优化目标用first
order来近似，约束项用second order 来近似，由于second
order涉及到构造Hessian matrix，计算量巨大，论文给出了 conjugate gradient
+ Fisher information matrix的近似快速实现方案。（论文第六部分）</p></li>
<li><p>从理论角度指出，Kakade 在2002年提出的方法natrual policy gradient
和经典的policy gradient 都是TRPO的特别形式。（论文第七部分）</p></li>
<li><p>评价TRPO在两种强化学习模式下的最终效果，一种是MuJoCo模拟器中能得到真实状态的模式，一种是Atari游戏环境，即观察到的屏幕像素可以信息完全地表达潜在真实状态的模式。（论文第八部分）</p></li>
</ol>
<p>本文下面的小结序号和论文小结序号相同，便于对照查阅。</p>
<h2 id="介绍">1. 介绍</h2>
<p>TRPO 第一次证明了最小化某种 surrogate
目标函数且采用non-trivial的步长，一定可以保证策略提升。进一步将此
surrogate 目标函数转换成trust
region约束下的优化问题。TRPO是一种on-policy
的算法，因为每一步迭代，需要在新的策略下通过采样数据来构建具体优化问题。</p>
<h2 id="已有理论基础">2. 已有理论基础</h2>
<p>第二部分主要回顾了 Kakade &amp; Langford 于2002 年的论文
Approximately Optimal Approximate Reinforcement Learning
中的一系列结论。</p>
<p>先来定义几个重要概念的数学定义</p>
<p><span class="math inline">\(\eta(\pi)\)</span> 是策略 <span class="math inline">\(\pi\)</span> 的目标，即discounted reward
和的期望。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-1.png">
</figure>
<ol start="2" type="1">
<li>然后是策略的Q值和V值</li>
</ol>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-2.png">
</figure>
<ol start="3" type="1">
<li>最后是策略的advantage函数</li>
</ol>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-3.png">
</figure>
<p>接着，开始引入 Kakade &amp; Langford 论文结论，即下式（公式1）。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-kakade.png">
</figure>
<p>公式1表明，下一次迭代策略的目标可以分解成现有策略的目标 <span class="math inline">\(\eta(\pi)\)</span> 和现有advantage
函数在新策略trajectory分布下的期望。</p>
<p>公式1可以很容易从trajectory分布转换成新策略在状态的访问频率，即公式2</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-kakade-form2.png">
</figure>
<p>状态的访问频率或稳定状态分布定义成</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-kakade-rho.png">
<figcaption>
</figcaption>
</figure>
<p>注意到公式2中状态的期望依然依赖于新策略 <span class="math inline">\(\rho_{\widetilde\pi}\)</span>
的稳定状态分布，不方便实现。原因如下，期望形式有利于采样来解决问题，但是由于采样数据源于
on-policy <span class="math inline">\(\pi\)</span> 而非 <span class="math inline">\({\widetilde\pi}\)</span>
，因此无法直接采样未知的策略 <span class="math inline">\({\widetilde\pi}\)</span>。</p>
<p>幸好，Kakade 论文中证明了，可以用 <span class="math inline">\(\rho_{\pi}\)</span> 的代替 <span class="math inline">\(\rho_{\widetilde\pi}\)</span>
并且证明了这种代替下的近似目标函数 <span class="math inline">\(L_{\pi}\)</span> 是原来函数的一阶近似</p>
<p><span class="math display">\[
L_{\pi}(\widetilde\pi) \approx \eta(\widetilde\pi)
\]</span></p>
<p>即满足</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-local-first-deriv.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(L_{\pi}\)</span> 具体定义表达式为</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-local.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(L_{\pi}(\widetilde\pi)\)</span>
是一阶近似意味着在小范围区域中一定是可以得到提升的，但是范围是多大，是否能保证
<span class="math inline">\(\eta\)</span>
的提升？Kakade的论文中不仅给出了通过mix新老策略的提升方式，还给出了这个方式对原目标
<span class="math inline">\(\eta\)</span> 较 <span class="math inline">\(L_{\pi}(\widetilde\pi)\)</span> 的提升下届。</p>
<p>策略更新规则如下</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-mix-policy-update.png">
<figcaption>
</figcaption>
</figure>
<p>公式6为具体提升下届为</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/2-mix-policy-bound.png">
<figcaption>
</figcaption>
</figure>
<h2 id="扩展到随机策略">3. 扩展到随机策略</h2>
<p>论文的这一部分将Kakade的mix policy update
扩展到一般的随机策略，同时依然保证每次迭代能得到目标提升。</p>
<p>首先，每次策略迭代必须不能和现有策略变化太大，因此，引入分布间常见的TV
divergence，即 total variation divergence。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-tv.png">
<figcaption>
</figcaption>
</figure>
<p>有了两个分布距离的定义，就可以定义两个策略的距离。离散状态下，一个策略是状态到动作分布的
map 或者 dict，因此，可以定义两个策略的距离为所有状态中最大的动作分布的
<span class="math inline">\(D_{TV}\)</span>，即</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-tv-max.png">
<figcaption>
</figcaption>
</figure>
至此，可以引出定理一：在一般随机策略下，Kakade
的surrogate函数较原目标的提升下届依然成立，即公式8在新的<span class="math inline">\(\alpha\)</span>定义下可以从公示6推导而来。
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-theorem1.png">
<figcaption>
</figcaption>
</figure>
<p>进一步将 TV divergence 转换成 KL divergence，转换成KL divergence
的目的是为了后续使用传统且成熟的 trust region 蒙特卡洛方法和 conjugate
gradient 的优化近似解法。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-Dtv-Dkl.png">
<figcaption>
</figcaption>
</figure>
<p>由于上面两种距离的大小关系，可以推导出用KL divergence表示的 <span class="math inline">\(\eta\)</span> 较 <span class="math inline">\(L_{\pi}(\widetilde\pi)\)</span> 的提升下届</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-theorem1-Dkl.png">
<figcaption>
</figcaption>
</figure>
<p>根据公式9，就可以形成初步的概念上的算法一，通过每一步形成无约束优化问题，同时保证每次迭代的
<span class="math inline">\(\pi_i\)</span> 对应的 <span class="math inline">\(\eta\)</span> 是递增的。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/3-alg1.png">
<figcaption>
</figcaption>
</figure>
<h2 id="trust-region-policy-optimization">4. Trust Region Policy
Optimization</h2>
<p>看到这里已经不容易了，尽管算法一给出了一个解决方案，但是本论文的主角TRPO
还未登场。TRPO算法的作用依然是近似！</p>
<p>算法一对于下面的目标函数做优化，即每次找到下一个 <span class="math inline">\(\theta_i\)</span> 最大化下式，<span class="math inline">\(\eta\)</span> 每一步一定能得到提升。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/4-true-max.png">
<figcaption>
</figcaption>
</figure>
问题是在实践中，惩罚系数 <span class="math inline">\(C\)</span>
会导致步长非常小，一种稳定的使用较大步长的方法是将惩罚项变成约束项，即：
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/4-trust-region-constraint.png">
<figcaption>
</figcaption>
</figure>
<p>将 <span class="math inline">\(D^{max}_{KL}\)</span>
放入约束项中符合trust region 这种传统优化解法。</p>
<p>关于 <span class="math inline">\(D^{max}_{KL}\)</span>
约束，再补充两点</p>
<ol type="1">
<li><p>其定义是两个策略中所有状态中最大的动作分布的 <span class="math inline">\(D_{TV}\)</span>
，因此它约束了所有状态下新老策略动作分布的KL散度，也就意味着有和状态数目相同数量的约束项，海量的约束项导致算法很难应用到实际中。</p></li>
<li><p>约束项的 trust region 不是参数 <span class="math inline">\(\theta\)</span>
的空间，而是其KL散度的空间。</p></li>
</ol>
<p>基于第一点，再次使用近似法，在约束项中用KL期望来代替各个状态下的KL散度，权重为on-policy
策略的分布 <span class="math inline">\(\rho(\theta_{old})\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/4-kl-exp.png">
<figcaption>
</figcaption>
</figure>
<p>最终，得到TRPO在实际中的优化目标（12式）：</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/4-final-optimization.png">
<figcaption>
</figcaption>
</figure>
<h2 id="用采样方法来trust-region约束优化">5. 用采样方法来Trust
Region约束优化</h2>
<p>论文第五部分，将TRPO优化目标12式改写成期望形式，引入两种蒙特卡洛方法
single path 和 vine 来采样。</p>
<p>具体来说，<span class="math inline">\(L_{\theta_{old}}\)</span>
由两项组成 <span class="math display">\[
L_{\theta_{old}} = \eta(\theta_{old}) + \sum_s
\rho_{\theta_{old}}(s)\sum_a {\pi_{\theta}}(a |s) A_{\theta_{old}}(s,a)
\]</span></p>
<p>第一项是常量，只需优化第二项，即优化问题等价为13式</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/5-13.png">
<figcaption>
</figcaption>
</figure>
<p>随后，为了可以适用非 on-policy <span class="math inline">\(\pi_{\theta_{old}}\)</span>
的动作分布来任意采样，引入采样的动作分布 <span class="math inline">\(q(a|s)\)</span>，将13式中的 <span class="math inline">\(\sum_a\)</span>
部分通过重要性采样改成以下形式：</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/5-IS.png">
<figcaption>
</figcaption>
</figure>
<p>再将13式中的 <span class="math inline">\(\sum_s \rho(s)\)</span>
改成期望形式 <span class="math inline">\(\mathbb{E}_{s \sim
\rho}\)</span> ，并将 <span class="math inline">\(A\)</span> 改成 <span class="math inline">\(Q\)</span> 值，得14式。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/5-14.png">
<figcaption>
</figcaption>
</figure>
<p>至此，我们得到trust
region优化的期望形式：优化目标中期望的状态空间是基于 on-policy <span class="math inline">\(\pi_{\theta_{old}}\)</span>，动作空间是基于任意采样分布
<span class="math inline">\(q(a|s)\)</span>，优化约束中的期望是基于
on-policy <span class="math inline">\(\pi_{\theta_{old}}\)</span>。</p>
<h3 id="single-path采样">5.1 Single path采样</h3>
<p>根据14式，single path
是最基本的的蒙特卡洛采样方法，和REINFORCE算法一样， 通过on-policy <span class="math inline">\(\pi_{\theta_{old}}\)</span>生成采样的
trajectory数据： <span class="math inline">\(s_0, a_0, s_1, a_1, ...,
a_{T-1}, s_{T}\)</span>，然后代入14式。注意，此时 <span class="math inline">\(q(a|s) =
\pi_{\theta_{old}}(a|s)\)</span>，即用现有策略的动作分布直接代替采样分布。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/5-single-path.png">
<figcaption>
</figcaption>
</figure>
<h3 id="vine-采样">5.2 Vine 采样</h3>
<p>虽然single path方法简单明了，但是有着online monte
carlo方法固有的缺陷，即variance较大。Vine方法通过在一个状态多次采样来改善此缺陷。Vine的翻译是藤，寓意从一个状态多次出发来采样，如下图，<span class="math inline">\(s_n\)</span>
状态下采样多个rollouts，很像植物的藤长出多分叉。当然，vine方法要求环境能restart
到某一状态，比如游戏环境通过save load返回先前的状态。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/5-vine.png">
<figcaption>
</figcaption>
</figure>
<p>具体来说，vine 方法首先通过生成多个on-policy
的trajectories来确定一个状态集合 <span class="math inline">\(s_1, s_2,
..., s_N\)</span>。对于状态集合的每一个状态 <span class="math inline">\(s_n\)</span> 采样K个动作，服从 $ a_{n, k} q(s_{n})
$ 。接着，对于每一个 <span class="math inline">\((s_n, a_{n,k})\)</span>
再去生成一次 rollout 来估计 <span class="math inline">\(\hat{Q}_{\theta_{i}}\left(s_{n}, a_{n,
k}\right)\)</span> 。试验证明，在连续动作空间问题中，<span class="math inline">\(q\left(\cdot \mid s_{n}\right)\)</span> 直接使用
on-policy
可以取得不错效果，在离散空间问题中，使用uniform分布效果更好。</p>
<h2 id="转换成具体优化问题">6. 转换成具体优化问题</h2>
<p>再回顾一下现在的进度，12式定义了优化目标，约束项是KL
divergence空间的trust region
形式。14式改写成了等价的期望形式，通过两种蒙特卡洛方法生成 state-action
数据集，可以代入14式得到每一步的具体数值的优化问题。论文这一部分简单叙述了如何高效但近似的解此类问题，详细的一些步骤在附录中阐述。我们把相关解读都放在下一节。</p>
<h2 id="和已有理论的联系">7. 和已有理论的联系</h2>
<h3 id="简化成-natural-policy-gradient">7.1 简化成 Natural Policy
Gradient</h3>
<p>再回到12式，即约束项是KL divergence空间的trust region 形式</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/4-final-optimization.png">
<figcaption>
</figcaption>
</figure>
<p>对于这种形式的优化问题，一般的做法是通过对优化目标做一阶函数近似，即
<span class="math display">\[
L_{\theta_{old}}(\theta) \approx
L_{\theta_{old}}\left(\theta_{old}\right)+g^{T}\left(\theta-\theta_{old}\right)
\]</span></p>
<p><span class="math display">\[
\left.g \doteq \nabla_{\theta}
L_{\theta_{old}}(\theta)\right|_{\theta_{old}}
\]</span></p>
<p>并对约束函数做二阶函数近似，因为约束函数在 <span class="math inline">\(\theta_{old}\)</span> 点取到极值，因此一阶导为0。
<span class="math display">\[
\bar{D}_{K L}\left(\theta \| \theta_{old}\right) \approx
\frac{1}{2}\left(\theta-\theta_{old}\right)^{T}
H\left(\theta-\theta_{old}\right)
\]</span></p>
<p><span class="math display">\[
\left.H \doteq \nabla_{\theta}^{2} \bar{D}_{K L}\left(\theta \|
\theta_{old}\right)\right|_{\theta_{old}}
\]</span></p>
<p>12式的优化目标可以转换成17式</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/7-17.png">
<figcaption>
</figcaption>
</figure>
<p>对应参数迭代更新公式如下</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/7-17-update.png">
<figcaption>
</figcaption>
</figure>
<p>这个方法便是Kakade在2002年发表的 natrual policy gradient 论文。</p>
<h3 id="简化成-policy-gradient">7.2 简化成 Policy Gradient</h3>
<p>注意，<span class="math inline">\(L_{\theta_{old}}\)</span>的一阶近似的梯度 <span class="math display">\[
\left.\nabla_{\theta} L_{\theta_{\text {old
}}}(\theta)\right|_{\theta=\theta_{\text {old }}}
\cdot\left(\theta-\theta_{\text {old }}\right)
\]</span></p>
<p>即PG定理 <span class="math display">\[
\frac{\partial \rho}{\partial \theta}=\sum_{s} d^{\pi}(s) \sum_{a}
\frac{\partial \pi(s, a)}{\partial \theta} Q^{\pi}(s, a)
\]</span></p>
因此，PG定理等价于<span class="math inline">\(L_{\theta_{old}}\)</span>的一阶近似的梯度在<span class="math inline">\(\theta\)</span> 空间 <span class="math inline">\(l_2\)</span> 约束下的优化问题，即18式
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/7-18.png">
<figcaption>
</figcaption>
</figure>
<h3 id="近似数值解法">7.3 近似数值解法</h3>
<p>这里简单描述关于17式及其参数更新规则中的大矩阵数值计算近似方式。</p>
<p>$ {D}_{}^{} $ 二阶近似中的 <span class="math inline">\(A\)</span> 是
Hessian 方形矩阵，维度为 <span class="math inline">\(\theta\)</span>
个数的平方。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/7-A-matrix.png">
<figcaption>
</figcaption>
</figure>
<p>直接构建 <span class="math inline">\(A\)</span> 矩阵或者其逆矩阵
<span class="math inline">\(A^{-1}\)</span>都是计算量巨大的， 注<span class="math inline">\(A^{-1}\)</span>出现在natural policy update <span class="math inline">\(\theta\)</span> 更新公式中，<span class="math inline">\(A^{-1} \nabla_{\theta} L(\theta)\)</span> 。</p>
<p>一种方法是通过构建Fisher Information Matrix，引入期望形式便于采样
<span class="math display">\[
\mathbf{A}=E_{\pi_{\theta}}\left[\nabla_{\theta} \log
\pi_{\theta}(\mathbf{a} \mid \mathbf{s}) \nabla_{\theta} \log
\pi_{\theta}(\mathbf{a} \mid \mathbf{s})^{T}\right]
\]</span> 另一种方式是使用conjugate gradient
方法，通过矩阵乘以向量快速计算法迭代逼近 <span class="math inline">\(A^{-1} \nabla_{\theta} L(\theta)\)</span>。</p>
<h2 id="试验结果">8. 试验结果</h2>
在两种强化学习模式下，比较TRPO和其他模型的效果。模式一是在MuJoCo模拟器中，这种环境下能得到真实状态的情况。
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/8-mujoco.png">
<figcaption>
</figcaption>
</figure>
<p>另一种模式是完全信息下的Atari游戏环境，这种环境下观察到的屏幕像素可以信息完全地表达潜在真实状态。</p>
<figure>
<img src="/zh/2020/paper-rl-trpo-2017/8-atari.png">
<figcaption>
</figcaption>
</figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/paper-rl-pg-sutton-1999/" itemprop="url">解读深度强化学习基石论文：函数近似的策略梯度方法</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-12-11T18:45:01.000Z" itemprop="datePublished">12月 12 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            12 分钟 读完 (约 1769 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/0-paper-title.png">
<figcaption>
</figcaption>
</figure>
<p>导读：这篇式1999 年Richard Sutton
在强化学习领域中的经典论文，论文证明了策略梯度定理和在用函数近似 Q
值时策略梯度定理依然成立，本文奠定了后续以深度强化学习策略梯度方法的基石。理解熟悉本论文对
Policy Gradient，Actor Critic 方法有很好的指导意义。</p>
<p>论文分成四部分。第一部分指出策略梯度在两种期望回报定义下都成立（定理一）。第二部分提出，如果
<span class="math inline">\(Q^{\pi}\)</span> 被函数 <span class="math inline">\(f_w\)</span>
近似时且满足兼容（compatible）条件，以 <span class="math inline">\(f_w\)</span> 替换策略梯度中的 <span class="math inline">\(Q^{\pi}\)</span>公式也成立（定理二）。第三部分举Gibbs分布的策略为例，如何应用
<span class="math inline">\(Q^{\pi}\)</span>近似函数来实现策略梯度算法。第四部分证明了近似函数的策略梯度迭代法一定能收敛到局部最优解。附录部分证明了两种定义下的策略梯度定理。</p>
<h2 id="策略梯度定理">1. 策略梯度定理</h2>
<p>对于Agent和环境而言，可以分成episode和non-episode，后者的时间步骤可以趋近于无穷大，但一般都可以适用两种期望回报定义。一种是单步平均reward
，另一种是指定唯一开始状态并对trajectory求 <span class="math inline">\(\gamma\)</span>-discounted
之和，称为开始状态定义。两种定义都考虑到了reward的sum会趋近于无穷大，通过不同的方式降低了此问题的概率。</p>
<h3 id="a.-平均reward定义">A. 平均reward定义</h3>
<p>目标函数 <span class="math inline">\(\rho(\pi)\)</span>
定义成单步的平均reward，这种情况下等价于稳定状态分布下期望值。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-average-reward-rou.png">
<figcaption>
</figcaption>
</figure>
<p>稳定状态分布定义成无限次数后状态的分布。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-average-reward-d.png">
<figcaption>
</figcaption>
</figure>
<p>此时，<span class="math inline">\(Q^{\pi}\)</span>
定义为无限步的reward sum 减去累积的单步平均 reward <span class="math inline">\(\rho(\pi)\)</span>，这里减去<span class="math inline">\(\rho(\pi)\)</span>是为了一定程度防止 <span class="math inline">\(Q^{\pi}\)</span>没有上界。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-average-reward-q.png">
<figcaption>
</figcaption>
</figure>
<h3 id="b.-开始状态定义">B. 开始状态定义</h3>
<p>在开始状态定义方式中，某指定状态<span class="math inline">\(s_0\)</span>作为起始状态，<span class="math inline">\(\rho(\pi)\)</span> 的定义为 trajectory
的期望回报，注意由于时间步骤 t 趋近于无穷大，必须要乘以discount 系数
<span class="math inline">\(\gamma &lt; 1\)</span>
保证期望不会趋近无穷大。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-start-state-rou.png">
<figcaption>
</figcaption>
</figure>
<span class="math inline">\(Q^{\pi}\)</span> 也直接定义成 trajectory
的期望回报。
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-start-state-q.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(d^{\pi}\)</span>
依然为无限次数后状态的稳定分布。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-start-state-d.png">
<figcaption>
</figcaption>
</figure>
<h3 id="策略梯度定理-1">策略梯度定理</h3>
<p>论文指出上述两种定义都满足策略梯度定理，即目标 <span class="math inline">\(\rho\)</span> 对于参数 <span class="math inline">\(\theta\)</span> 的偏导不依赖于 <span class="math inline">\(d^{\pi}\)</span> 对于 <span class="math inline">\(\theta\)</span> 偏导，仅取决</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-pg-theorem.png">
<figcaption>
</figcaption>
</figure>
<p>关于策略梯度定理的一些综述，可以参考。</p>
<p>论文中还提到策略梯度定理公式和经典的William
REINFORCE算法之间的联系。REINFORCE算法即策略梯度的蒙特卡洛实现。</p>
<p>联系如下：</p>
<p>首先，根据策略梯度定理，如果状态 s 是通过 <span class="math inline">\(\pi\)</span> 采样得到，则下式是$$
的无偏估计。注意，这里action的summation和 <span class="math inline">\(\pi\)</span> 是无关的。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-sum-a.png">
<figcaption>
</figcaption>
</figure>
在William REINFORCE算法中，采用<span class="math inline">\(R_t\)</span>
作为 <span class="math inline">\(Q^{\pi}(s_t, a_t)\)</span>的近似，但是
<span class="math inline">\(R_t\)</span> 取决于 on-policy <span class="math inline">\(\pi\)</span> 的动作分布，因此必须除掉 <span class="math inline">\(\pi(s_t, a_t)\)</span>项，去除引入<span class="math inline">\(R_t\)</span> 后导致oversample动作空间。
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/1-rt.png">
<figcaption>
</figcaption>
</figure>
<h2 id="函数近似的策略梯度">2. 函数近似的策略梯度</h2>
<p>论文第二部分，进一步引入 <span class="math inline">\(Q_{\pi}\)</span>
的近似函数 <span class="math inline">\(f_w\)</span>: $ $。</p>
<p>如果我们有<span class="math inline">\(Q_{\pi}(s_t,
a_t)\)</span>的无偏估计，例如 <span class="math inline">\(R_t\)</span>，很自然，可以让 <span class="math inline">\(\partial f_w \over \partial w\)</span> 通过最小化
<span class="math inline">\(R_t\)</span> 和 <span class="math inline">\(f_w\)</span>之间的差距来计算。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/2-dw.png">
<figcaption>
</figcaption>
</figure>
<p>当拟合过程收敛到局部最优时，策略梯度定理中右边项对于 <span class="math inline">\(w\)</span> 求导为0，可得(3)式。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/2-(3).png">
<figcaption>
</figcaption>
</figure>
<p>至此，引出策略梯度定理的延续，即定理2：当 <span class="math inline">\(f_w\)</span>
满足(3)式同时满足(4)式（称为compatible条件时），可以用 <span class="math inline">\(f_w(s, a)\)</span>替换原策略梯度中的 <span class="math inline">\(Q_{\pi}(s,a)\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/2-pg-func-approx-theorem.png">
<figcaption>
</figcaption>
</figure>
<h2 id="一个应用示例">3. 一个应用示例</h2>
<p>假设一个策略用features的线性组合后的 Gibbs分布来生成，即：</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/3-pi.png">
<figcaption>
</figcaption>
</figure>
注意，<span class="math inline">\(\phi_{sa}\)</span> 和 <span class="math inline">\(\theta\)</span> 都是 <span class="math inline">\(l\)</span> 维的。 当 <span class="math inline">\(f_w\)</span> 满足compatible
条件，由公式（4）可得<span class="math inline">\(\partial f_w \over
\partial w\)</span>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/3-deriv.png">
<figcaption>
</figcaption>
</figure>
注意，<span class="math inline">\(\partial f_w \over \partial w\)</span>
也是 <span class="math inline">\(l\)</span>维。<span class="math inline">\(f_w\)</span> 可以很自然的参数化为
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/3-fw.png">
<figcaption>
</figcaption>
</figure>
即 <span class="math inline">\(f_w\)</span> 和 策略 <span class="math inline">\(\pi\)</span> 一样是features的线性关系。当然 <span class="math inline">\(f_w\)</span> 还满足对于所有状态，在 <span class="math inline">\(\pi\)</span> 动作分布下均值为0。
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/3-fw-mean.png">
<figcaption>
</figcaption>
</figure>
<p>上式和advantage 函数 <span class="math inline">\(A^{\pi}(s,
a)\)</span> 定义一致，因此可以认为 <span class="math inline">\(f_w\)</span> 的意义是 <span class="math inline">\(A^{\pi}\)</span> 的近似。</p>
<p><span class="math inline">\(A^{\pi}\)</span>具体定义如下</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/3-a-pi.png">
<figcaption>
</figcaption>
</figure>
<h2 id="函数近似的策略梯度收敛性证明">4.
函数近似的策略梯度收敛性证明</h2>
<p>这一部分证明了在满足一定条件后，<span class="math inline">\(\theta\)</span> 可以收敛到局部最优点。</p>
<p>条件为</p>
<ol type="1">
<li>Compatible 条件，公式（4）</li>
<li>任意两个 <span class="math inline">\(\partial \pi \over \partial
\theta\)</span> 偏导是有限的，即</li>
</ol>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/4-theorem3-bound.png">
<figcaption>
</figcaption>
</figure>
<ol start="3" type="1">
<li>步长数列满足如下条件</li>
</ol>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/4-theorem3-alpha.png">
<figcaption>
</figcaption>
</figure>
<ol start="4" type="1">
<li><p>环境的 reward 是有限的</p>
<p>此时，当 <span class="math inline">\(w_k\)</span> 和 <span class="math inline">\(\theta_k\)</span>
按如下方式迭代一定能收敛到局部最优。</p></li>
</ol>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/4-theorem3-wk.png">
<figcaption>
</figcaption>
</figure>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/4-theorem3-theta.png">
<figcaption>
</figcaption>
</figure>
收敛到局部最优，即
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/4-theorem3-converge.png">
<figcaption>
</figcaption>
</figure>
<h2 id="策略梯度定理的两种情况下的证明">5.
策略梯度定理的两种情况下的证明</h2>
<p>下面简单分解策略梯度的证明步骤。</p>
<h3 id="a.-平均reward-定义下的证明">A. 平均reward 定义下的证明</h3>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-1.png">
<figcaption>
</figcaption>
</figure>
<p>根据定义，将 <span class="math inline">\(\theta\)</span>
导数放入求和号中，并分别对乘积中的每项求导。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-2.png">
<figcaption>
</figcaption>
</figure>
<p>将<span class="math inline">\(Q_{\pi}\)</span>的定义代入第二项 <span class="math inline">\(Q^{\pi}\)</span> 对 <span class="math inline">\(\theta\)</span> 求偏导中，引入环境reward 随机变量
<span class="math inline">\(R^a_s\)</span>，环境dynamics <span class="math inline">\(P\)</span> 和 <span class="math inline">\(\rho(\pi)\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-3.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(\theta\)</span> 偏导进一步移入，<span class="math inline">\(R^a_s\)</span>， <span class="math inline">\(P\)</span> 不依赖于<span class="math inline">\(\theta\)</span>。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-4.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(\rho(\pi)\)</span> 对于 <span class="math inline">\(\theta\)</span> 偏导整理到等式左边</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-5.png">
<figcaption>
</figcaption>
</figure>
<p>两边同时乘以 <span class="math inline">\(\sum d^{\pi}\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-6.png">
<figcaption>
</figcaption>
</figure>
<p>由于 <span class="math inline">\(d^{\pi}\)</span> 是状态在 <span class="math inline">\(\pi\)</span> 下的平稳分布，<span class="math inline">\(\sum \pi \sum P\)</span> 项表示 agent 主观 <span class="math inline">\(\pi\)</span> 和环境客观 <span class="math inline">\(P\)</span>
对于状态分布的影响，因此可以直接去除。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-avg-7.png">
<figcaption>
</figcaption>
</figure>
<p>整理证得。</p>
<h3 id="b.-start-state-定义下的证明">B. Start-state 定义下的证明</h3>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-1.png">
<figcaption>
</figcaption>
</figure>
<p>根据定义，将 <span class="math inline">\(\theta\)</span>
导数放入求和号中，并分别对乘积中的每项求导。</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-2.png">
<figcaption>
</figcaption>
</figure>
<p>将<span class="math inline">\(Q_{\pi}\)</span>的定义代入第二项 <span class="math inline">\(Q^{\pi}\)</span> 对 <span class="math inline">\(\theta\)</span> 求偏导中，引入环境reward 随机变量
<span class="math inline">\(R^a_s\)</span>，环境dynamics <span class="math inline">\(P\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-3.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(\theta\)</span> 偏导进一步移入，<span class="math inline">\(R^a_s\)</span>， <span class="math inline">\(P\)</span> 不依赖于<span class="math inline">\(\theta\)</span>。注意，此式表示从状态 <span class="math inline">\(s\)</span> 出发一步之后的能到达的所有 <span class="math inline">\(s^{\prime}\)</span> ，将次式反复unroll <span class="math inline">\(V^{\pi}\)</span> 成 <span class="math inline">\(Q^{\pi}\)</span> 之后得到</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-4.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(\operatorname{Pr}(s \rightarrow x, k,
\pi)\)</span> 表示 k 步后 状态 s 能到达的所有状态 x</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-5.png">
<figcaption>
</figcaption>
</figure>
<p>根据定义，<span class="math inline">\(\rho =
V^{\pi}(s_0)\)</span></p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-6.png">
<figcaption>
</figcaption>
</figure>
<p>将 <span class="math inline">\(V^{\pi}(s_0)\)</span> 替换成unroll 成
<span class="math inline">\(Q^{\pi}\)</span> 的表达式</p>
<figure>
<img src="/zh/2020/paper-rl-pg-sutton-1999/5-start-state-7.png">
<figcaption>
</figcaption>
</figure>
<p><span class="math inline">\(\operatorname{Pr}(s \rightarrow x, k,
\pi)\)</span> 即 <span class="math inline">\(d^{\pi}\)</span></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-policy-gradient/" itemprop="url">深度强化学习之：Policy Gradient Theorem 一些理解</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-12-11T18:45:01.000Z" itemprop="datePublished">12月 12 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            17 分钟 读完 (约 2538 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>Policy gradient
定理作为现代深度强化学习的基石，同时也是actor-critic的基础，重要性不言而喻。但是它的推导和理解不是那么浅显，不同的资料中又有着众多形式，不禁令人困惑。本篇文章MyEncyclopedia试图总结众多资料背后的一些相通的地方，并写下自己的一些学习理解心得。</p>
<h2 id="引入-policy-gradient">引入 Policy Gradient</h2>
Policy gradient 引入的目的是若我们将策略 <span class="math inline">\(\pi_{\theta}\)</span> 的参数 <span class="math inline">\(\theta\)</span> 直接和一个标量 <span class="math inline">\(J\)</span>
直接联系在一起的话，就能够利用目前最流行的深度学习自动求导的方法，迭代地去找到
<span class="math inline">\(\theta^*\)</span> 来最大化 <span class="math inline">\(J\)</span>：
<div>
<p><span class="math display">\[
\theta^{\star}=\arg \max _{\theta} J(\theta)
\]</span></p>
</div>
<div>
<p><span class="math display">\[
{\theta}_{t+1} \doteq {\theta}_{t}+\alpha \nabla J(\theta)
\]</span></p>
</div>
此时，训练神经网络成功地收敛到 <span class="math inline">\(\theta^{*}\)</span> 时可以直接给出任意一个状态 s
的动作分布。
<figure>
<img src="/zh/2020/rl-policy-gradient/policy_net.png">
<figcaption>
</figcaption>
</figure>
<p>那么问题来了，首先一个如何定义 <span class="math inline">\(J(\theta)\)</span>，其次，如何求出或者估计 $
J()$。</p>
<p>第一个问题比较直白，用value function或者广义的expected
return都可以。</p>
<p>这里列举一些常见的定义。对于episodic 并且初始都是 <span class="math inline">\(s_0\)</span>状态的情况，直接定义成v值，即Sutton教程中的episodic情况下的定义</p>
<div>
<p><span class="math display">\[
J(\boldsymbol{\theta}) \doteq
v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)  \quad \quad
\text{(1.1)}
\]</span></p>
</div>
进一步，上式等价于 <span class="math inline">\(V(s)\)</span>
在状态平稳分布下的均值。
<div>
<p><span class="math display">\[
\begin{aligned}
J(\theta) &amp;= \sum_{s \in \mathcal{S}} d^{\pi}(s) V^{\pi}(s) \\
&amp;=\sum_{s \in \mathcal{S}} d^{\pi}(s) \sum_{a \in \mathcal{A}}
\pi_{\theta}(a \mid s) Q^{\pi}(s, a)
\end{aligned} \quad \quad \text{(1.2)}
\]</span></p>
</div>
<p>其中，状态平稳分布 <span class="math inline">\(d^{\pi}(s)\)</span>
定义为</p>
<div>
<p><span class="math display">\[
d^{\pi}(s)=\lim _{t \rightarrow \infty} P\left(s_{t}=s \mid s_{0},
\pi_{\theta}\right)
\]</span></p>
</div>
另一种定义从trajectory角度出发，公式如下：
<div>
<p><span class="math display">\[
J(\boldsymbol{\theta}) \doteq E_{\tau \sim
p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t},
\mathbf{a}_{t}\right)\right] \quad \quad \text{(1.3)}
\]</span></p>
</div>
<p>即$ $ 是一次trajectory，服从以 <span class="math inline">\(\theta\)</span> 作为参数的随机变量</p>
<div>
<p><span class="math display">\[
\tau \sim p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots,
\mathbf{s}_{T}, \mathbf{a}_{T}\right)
\]</span></p>
</div>
<p><span class="math inline">\(J(\theta)\)</span> 对于所有的可能的 <span class="math inline">\(\tau\)</span> 求 expected
return。这种视角下对于finite 和 infinite horizon来说也有变形。</p>
Infinite horizon 情况下，通过 <span class="math inline">\((s,
a)\)</span> 的marginal distribution来计算
<div>
<p><span class="math display">\[
J(\boldsymbol{\theta}) \doteq E_{(\mathbf{s}, \mathbf{a}) \sim
p_{\theta}(\mathbf{s}, \mathbf{a})}[r(\mathbf{s}, \mathbf{a})] \quad
\quad \text{(1.4)}
\]</span></p>
</div>
Finite horizon 情况下，通过每一时刻下 <span class="math inline">\((s_t,
a_t)\)</span> 的marginal distribution来计算
<div>
<p><span class="math display">\[
J(\boldsymbol{\theta}) \doteq \sum_{t=1}^{T} E_{\left(\mathbf{s}_{t},
\mathbf{a}_{t}\right) \sim p_{\theta}\left(\mathbf{s}_{t},
\mathbf{a}_{t}\right)} \quad \quad \text{(1.5)}
\]</span></p>
</div>
关于第二个问题，如何求出或者估计 $ J()$ 就是 policy gradient theorem
的主题了。仔细想想确实会有一些问题。一是 reward 随机变量 <span class="math inline">\(R(s, a)\)</span> 是离散情况下 $ J()$
还是否存在，再是 <span class="math inline">\(J(\theta)\)</span>
不仅取决于agent 主观的 <span class="math inline">\(\pi_{\theta}\)</span>，还取决于环境客观的dynamics
model
<div>
<p><span class="math display">\[
p\left(s^{\prime}, r \mid s, a\right) =
\operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \mid S_{t-1}=s,
A_{t-1}=a\right\}
\]</span></p>
</div>
<p>当环境dynamics未知时，如何再去求 $ J()$
呢。还有就是如果涉及到状态的分布也是取决于环境dynamics的，计算 $ J()$
也面临同样的问题。</p>
<p>幸好，policy
gradient定理完美的解答了上述问题。我们先来看看它的表述内容。</p>
<h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2>
策略梯度定理证明了，无论定义何种 <span class="math inline">\(J(\theta)\)</span> ，策略梯度等比于下式，其中
<span class="math inline">\(\mu(s)\)</span> 为 <span class="math inline">\(\pi_{\theta}\)</span>
下的状态分布。等比系数在episodic情况下为episode的平均长度，在infinite
horizon情况下为1。
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a}
q_{\pi}(s, a) \nabla \pi(a \mid s, \boldsymbol{\theta}) \quad \quad
\text{(2.1)}
\]</span></p>
</div>
考虑到系数可以包含在步长 <span class="math inline">\(\alpha\)</span>
中， <span class="math inline">\(\mu(s)\)</span> 是on policy <span class="math inline">\(\pi_{\theta}\)</span> 的权重，<span class="math inline">\(\nabla J(\theta)\)</span>
也可以写成期望形式的等式，注意，下式中 <span class="math inline">\(S_t\)</span> 从具体 <span class="math inline">\(s\)</span> 变成了随机变量，随机概率部分移到了
<span class="math inline">\(\mathbb{E}_{\pi}\)</span>中了。
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta}) =\mathbb{E}_{\pi}\left[\sum_{a}
q_{\pi}\left(S_{t}, a\right) \nabla \pi\left(a \mid S_{t},
\boldsymbol{\theta}\right)\right]  \quad \quad \text{(2.2)}
\]</span></p>
</div>
<p>Policy Gradient 定理的伟大之处在于等式右边并没有 <span class="math inline">\(d^{\pi}(s)\)</span>，或者环境transition model
<span class="math inline">\(p\left(s^{\prime}, r \mid s,
a\right)\)</span>！同时，等式右边变换成了最利于统计采样的期望形式，因为期望可以通过样本的平均来估算。</p>
<p>但是，这里必须注意的是action space的期望并不是基于 $(a S_{t}, ) $
的权重的，因此，继续改变形式，引入 action space的 on policy 权重 $(a
S_{t}, ) $ ，得到 2.3式。</p>
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta})=\mathbb{E}_{\pi}\left[\sum_{a} \pi\left(a
\mid S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t}, a\right)
\frac{\nabla \pi\left(a \mid S_{t},
\boldsymbol{\theta}\right)}{\pi\left(a \mid S_{t},
\boldsymbol{\theta}\right)}\right] \quad \quad \text{(2.3)}
\]</span></p>
</div>
将 <span class="math inline">\(a\)</span> 替换成 $A_{t} $，得到2.4式
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta})==\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t},
A_{t}\right) \frac{\nabla \pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}\right)}\right]  \quad \quad \text{(2.4)}
\]</span></p>
</div>
<p>将 <span class="math inline">\(q_{\pi}\)</span>替换成 <span class="math inline">\(G_t\)</span>，由于</p>
<div>
<p><span class="math display">\[
\mathbb{E}_{\pi}[G_{t} \mid S_{t}, A_{t}]= q_{\pi}\left(S_{t},
A_{t}\right)
\]</span></p>
</div>
<p>得到2.5式</p>
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta})==\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla
\pi\left(A_{t} \mid S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t}
\mid S_{t}, \boldsymbol{\theta}\right)}\right]  \quad \quad \text{(2.5)}
\]</span></p>
</div>
至此，action 和 state space的权重都源自 <span class="math inline">\(\pi_{\theta}\)</span>，期望内的随机变量可以通过
<span class="math inline">\(\pi_{\theta}\)</span> 在每一时间 t
采样来无偏估计，这便是大名鼎鼎的 REINFORCE 算法，即Monte Carlo Policy
Gradient。
<div>
<p><span class="math display">\[
\nabla J(\boldsymbol{\theta}) \approx G_{t} \frac{\nabla \pi\left(A_{t}
\mid S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}\right)} \quad \quad \text{(2.6)}
\]</span></p>
</div>
此时，<span class="math inline">\(\theta\)</span> 迭代更新公式为
<div>
<p><span class="math display">\[
\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t}
\frac{\nabla \pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}  \quad \quad \text{(2.7)}
\]</span></p>
</div>
下面是REINFORCE算法完整流程
<figure>
<img src="/zh/2020/rl-policy-gradient/reinforce.png">
<figcaption>
</figcaption>
</figure>
<h2 id="policy-gradient-theorem---trajectory-form">Policy Gradient
Theorem - Trajectory Form</h2>
Trajectory 形式的策略梯度定理也很常见，这里也总结一下，回顾 1.3 式 <span class="math inline">\(J(\theta)\)</span>的定义
<div>
<p><span class="math display">\[
J(\boldsymbol{\theta}) \doteq E_{\tau \sim
p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t},
\mathbf{a}_{t}\right)\right] \quad \quad \text{(1.3)}
\]</span></p>
</div>
最后可以证明出
<div>
<p><span class="math display">\[
\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{\tau \sim
\pi_{\theta}}{\mathrm{E}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log
\pi_{\theta}\left(a_{t} \mid s_{t}\right) R(\tau)\right] \quad \quad
\text{(3.1)}
\]</span></p>
</div>
3.1式中每一时刻 t 中依赖全时刻的 <span class="math inline">\(R(\tau)\)</span> ，进一步优化可以证明，时刻 t
只依赖于后续reward sum，即 reward-to-go， $ _{t}$
<div>
<p><span class="math display">\[
\hat{R}_{t} \doteq \sum_{t^{\prime}=t}^{T} R\left(s_{t^{\prime}},
a_{t^{\prime}}, s_{t^{\prime}+1}\right)
\]</span></p>
</div>
最终的策略梯度定理的形式为：
<div>
<p><span class="math display">\[
\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{\tau \sim
\pi_{\theta}}{\mathrm{E}}\left[\sum_{t=0}^{T} \nabla_{\theta} \log
\pi_{\theta}\left(a_{t} \mid s_{t}\right) \hat{R}_{t} \right] \quad
\quad \text{(3.2)}
\]</span></p>
</div>
由于 log-derivative trick的存在，3.2式和2.5式（Sutton 教程中的policy
gradient）等价。
<div>
<p><span class="math display">\[
\nabla_{\theta} \log \pi_{\theta}(a)=\frac{\nabla_{\theta}
\pi_{\theta}}{\pi_{\theta}} \quad \quad \text{(3.3)}
\]</span></p>
</div>
<h2 id="和监督学习的联系">和监督学习的联系</h2>
<p>Policy Gradient中的 <span class="math inline">\(\nabla_{\theta} \log
\pi\)</span> 广泛存在在机器学习范畴中，被称为 score function gradient
estimator。RL 在supervised learning settings 中有 imitation
learning，即通过专家的较优stochastic policy <span class="math inline">\(\pi_{\theta}(a|s)\)</span> 收集数据集</p>
<div>
<p><span class="math display">\[
\{(s_1, a^{*}_1), (s_2, a^{*}_2), ...\}
\]</span></p>
</div>
算法有监督的学习去找到max log likelyhook 的 <span class="math inline">\(\theta^{*}\)</span>
<div>
<p><span class="math display">\[
\theta^{*}=\operatorname{argmax}_{\theta} \sum_{n} \log
\pi_{\theta}\left(a_{n}^{*} \mid s_{n}\right) \quad \quad \text{(4.1)}
\]</span></p>
</div>
此时，参数迭代公式为
<div>
<p><span class="math display">\[
\theta_{n+1} \leftarrow \theta_{n}+\alpha_{n} \nabla_{\theta} \log
\pi_{\theta}\left(a_{n}^{*} \mid s_{n}\right) \quad \quad \text{(4.2)}
\]</span></p>
</div>
<p>对照Policy Graident RL，on-policy <span class="math inline">\(\pi_{\theta}(a|s)\)</span> 产生数据集</p>
<div>
<p><span class="math display">\[
\{(s_1, a_1, r_1), (s_2, a_2, r_2), ...\}
\]</span></p>
</div>
<p>目标是最大化on-policy <span class="math inline">\(\pi_{\theta}\)</span> 分布下的expected return</p>
<div>
<p><span class="math display">\[
\theta^{*}=\operatorname{argmax}_{\theta} \sum_{n} R(\tau_{n})
\]</span></p>
</div>
对照2.7式 <span class="math inline">\(\theta\)</span>
的更新公式，2.7式可以写成如下4.3式
<div>
<p><span class="math display">\[
\theta_{n+1} \leftarrow \theta_{n}+\alpha_{n}  G_{n} \nabla_{\theta}
\log \pi_{\theta}\left(a_{n} \mid s_{n}\right) \quad \quad \text{(4.3)}
\]</span></p>
</div>
<p>对比 4.3 和 4.2，发现此时4.3中只多了一个权重系数 <span class="math inline">\(G_n\)</span>。</p>
<p>关于 $G_{n} <em>{} </em>{}(a_{n} s_{n}) $ 或者 <span class="math inline">\(G_{t} \frac{\nabla \pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}\)</span> 有一些深入的理解。</p>
首先policy gradient RL 不像supervised imitation learning直接有label
作为signal，PG
RL必须通过采样不同的action获得reward或者return作为signal，即1.4式中的
<div>
<p><span class="math display">\[
E_{(\mathbf{s}, \mathbf{a}) \sim p_{\theta}(\mathbf{s},
\mathbf{a})}[r(\mathbf{s}, \mathbf{a})] \quad \quad \text{(5.1)}
\]</span></p>
</div>
广义的score function gradient estimator
对于形式为5.2的函数期望求gradient。对比上式，PG RL ， <span class="math inline">\(f(x)\)</span>视为reward 随机变量，期望是under
on-policy <span class="math inline">\(\pi_{\theta}\)</span>。
<div>
<p><span class="math display">\[
E_{x \sim p(x \mid \theta)}[f(x)] \quad \quad \text{(5.2)}
\]</span></p>
</div>
以下是score function gradient
estimator的推导，这里不做赘述，主要利用了3.3式的 log-derivative trick。
<div>
<p><span class="math display">\[
\begin{aligned} \nabla_{\theta} E_{x}[f(x)] &amp;=\nabla_{\theta}
\sum_{x} p(x) f(x) \\ &amp;=\sum_{x} \nabla_{\theta} p(x) f(x) \\
&amp;=\sum_{x} p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) \\
&amp;=\sum_{x} p(x) \nabla_{\theta} \log p(x) f(x) \\
&amp;=E_{x}\left[f(x) \nabla_{\theta} \log p(x)\right] \end{aligned}
\quad \quad \text{(5.3)}
\]</span></p>
</div>
<p>Policy Gradient 工作的机制大致如下</p>
<p>首先，根据现有的 on-policy <span class="math inline">\(\pi_{\theta}\)</span> 采样出一些动作 action
产生trajectories，这些trajectories最终得到反馈 <span class="math inline">\(R(\tau)\)</span></p>
<figure>
<img src="/zh/2020/rl-policy-gradient/pg_sample.png">
<figcaption>
</figcaption>
</figure>
用采样到的数据通过R加权来代替imitation learning的labeled loss
<div>
<p><span class="math display">\[
R(s,a) \nabla \pi_{\theta_{t}}(a \mid s) \approx \nabla
\pi_{\theta_{t}}(a^{*} \mid s)
\]</span></p>
</div>
<p>最后，由于采样到的action分布服从于<span class="math inline">\(a \sim
\pi_{\theta}(a)\)</span> ，除掉 <span class="math inline">\(\pi_{\theta}\)</span> ：</p>
<p><span class="math inline">\(G_{t} \frac{\nabla \pi\left(A_{t} \mid
S_{t}, \boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}\)</span></p>
<p>此时，采样的均值可以去无偏估计2.2式中的Expectation。</p>
<div>
<p><span class="math display">\[
\sum_N G_{t} \frac{\nabla \pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} \mid S_{t},
\boldsymbol{\theta}_{t}\right)}
\]</span></p>
</div>
<div>
<p><span class="math display">\[
=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla
\pi\left(a \mid S_{t}, \boldsymbol{\theta}\right)\right]
\]</span></p>
</div>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-dqn-mario/" itemprop="url">深度强化学习之：DQN训练超级玛丽闯关</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-12-04T18:45:01.000Z" itemprop="datePublished">12月 5 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            12 分钟 读完 (约 1743 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>上一期 MyEncyclopedia公众号文章 <a href="/zh/2020/rl-dqn-mario/!--swig￼9--">从Q-Learning
演化到
DQN</a>，我们从原理上讲解了DQN算法，这一期，让我们通过代码来实现任天堂游戏机中经典的超级玛丽的自动通关吧。本文所有代码在
https://github.com/MyEncyclopedia/reinforcement-learning-2nd/tree/master/super_mario。</p>
<h2 id="dqn-算法回顾">DQN 算法回顾</h2>
<p>上期详细讲解了DQN中的两个重要的技术：Target Network 和 Experience
Replay，正是有了它们才使得 Deep Q
Network在实战中容易收敛，以下是Deepmind 发表在Nature 的 Human-level
control through deep reinforcement learning 的完整算法流程。</p>
<figure>
<img src="/zh/2020/rl-dqn-mario/dqn_alg_nature.png">
<figcaption>
</figcaption>
</figure>
<h2 id="超级玛丽-nes-openai-环境">超级玛丽 NES OpenAI 环境</h2>
<p>安装基于OpenAI gym的超级玛丽环境执行下面的 pip 命令即可。</p>
<figure class="highlight bash hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym-super-mario-bros</span><br></pre></td></tr></tbody></table></figure>
<p>我们先来看一下游戏环境的输入和输出。下面代码采用随机的action来和游戏交互。有了
<a href="/zh/2020/rl-dqn-mario/!--swig￼10--">组合游戏系列3: 井字棋、五子棋的OpenAI Gym
GUI环境</a> 对于OpenAI Gym
接口的介绍，现在对于其基本的交互步骤已经不陌生了。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> gym_super_mario_bros</span><br><span class="line"><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> random, randrange</span><br><span class="line"><span class="hljs-keyword">from</span> gym_super_mario_bros.actions <span class="hljs-keyword">import</span> RIGHT_ONLY</span><br><span class="line"><span class="hljs-keyword">from</span> nes_py.wrappers <span class="hljs-keyword">import</span> JoypadSpace</span><br><span class="line"><span class="hljs-keyword">from</span> gym <span class="hljs-keyword">import</span> wrappers</span><br><span class="line"></span><br><span class="line">env = gym_super_mario_bros.make(<span class="hljs-string">'SuperMarioBros-v0'</span>)</span><br><span class="line">env = JoypadSpace(env, RIGHT_ONLY)</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># Play randomly</span></span><br><span class="line">done = <span class="hljs-literal">False</span></span><br><span class="line">env.reset()</span><br><span class="line"></span><br><span class="line">step = <span class="hljs-number">0</span></span><br><span class="line"><span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">    action = randrange(<span class="hljs-built_in">len</span>(RIGHT_ONLY))</span><br><span class="line">    state, reward, done, info = env.step(action)</span><br><span class="line">    <span class="hljs-built_in">print</span>(done, step, info)</span><br><span class="line">    env.render()</span><br><span class="line">    step += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">env.close()</span><br></pre></td></tr></tbody></table></figure>
<p>游戏render效果如下</p>
<p>。。。</p>
<p>注意我们在游戏环境初始化的时候用了参数
RIGHT_ONLY，它定义成五种动作的list，表示仅使用右键的一些组合，适用于快速训练来完成Mario第一关。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">RIGHT_ONLY = [</span><br><span class="line">    [<span class="hljs-string">'NOOP'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'A'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'B'</span>],</span><br><span class="line">    [<span class="hljs-string">'right'</span>, <span class="hljs-string">'A'</span>, <span class="hljs-string">'B'</span>],</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure>
<p>观察一些 info 输出内容，coins表示金币获得数量，flag_get
表示是否取得最后的旗子，time 剩余时间，以及 Mario 大小状态和所在的
x，y位置。 </p><figure class="highlight plaintext hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">{</span><br><span class="line">   "coins":0,</span><br><span class="line">   "flag_get":False,</span><br><span class="line">   "life":2,</span><br><span class="line">   "score":0,</span><br><span class="line">   "stage":1,</span><br><span class="line">   "status":"small",</span><br><span class="line">   "time":381,</span><br><span class="line">   "world":1,</span><br><span class="line">   "x_pos":594,</span><br><span class="line">   "y_pos":89</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="游戏图像处理">游戏图像处理</h2>
<p>Deep Reinforcement Learning 一般是 end-to-end learning，意味着游戏的
screen image
作为observation直接视为真实状态，喂给神经网络训练。于此相反的另一种做法是，通过游戏环境拿到内部状态，例如所有相关物品的位置和属性作为模型输入。这两种方式的区别有两点。第一点，用观察到的屏幕像素代替真正的状态
s，在partially observable 的环境时可能因为 non-stationarity
导致无法很好的工作，而拿内部状态利用了额外的作弊信息，在partially
observable环境中也可以工作。第二点，第一种方式屏幕像素维度比较高，输入数据量大，需要神经网络的大量训练拟合，第二种方式，内部真实状态往往维度低得多，训练起来很快，但缺点是因为除了内部状态往往还需要游戏相关规则作为输入，因此generalization能力不如前者强。</p>
<figure>
<img src="/zh/2020/rl-dqn-mario/dqn.jpg">
<figcaption>
</figcaption>
</figure>
<p>这里，我们当然采样屏幕像素的 end-to-end
方式了，自然首要任务是将游戏帧图像有效处理。超级玛丽游戏环境的屏幕输出是
(240, 256, 3) shape的 numpy
array，通过下面一系列的转换，尽可能的在不影响训练效果的情况下减小采样到的数据量。</p>
<ol type="1">
<li><p>MaxAndSkipFrameWrapper：每4个frame连在一起，采取同样的动作，降低frame数量。</p></li>
<li><p>FrameDownsampleWrapper：将原始的 (240, 256, 3) down sample 到
(84, 84, 1)</p></li>
<li><p>ImageToPyTorchWrapper：转换成适合 pytorch 的 (1, 84, 84)
shape</p></li>
<li><p>FrameBufferWrapper：保存最后4次屏幕采样</p></li>
<li><p>NormalizeFloats：Normalize 成 [0., 1.0] 的浮点值</p></li>
</ol>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">wrap_environment</span>(<span class="hljs-params">env_name: <span class="hljs-built_in">str</span>, action_space: <span class="hljs-built_in">list</span></span>) -&gt; Wrapper:</span></span><br><span class="line">    env = make(env_name)</span><br><span class="line">    env = JoypadSpace(env, action_space)</span><br><span class="line">    env = MaxAndSkipFrameWrapper(env)</span><br><span class="line">    env = FrameDownsampleWrapper(env)</span><br><span class="line">    env = ImageToPyTorchWrapper(env)</span><br><span class="line">    env = FrameBufferWrapper(env, <span class="hljs-number">4</span>)</span><br><span class="line">    env = NormalizeFloats(env)</span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<h2 id="cnn-模型">CNN 模型</h2>
<p>模型比较简单，三个卷积层后做
softmax输出，输出维度数为离散动作数。act() 采用了epsilon-greedy
模式，即在epsilon小概率时采取随机动作来
explore，大于epsilon时采取估计的最可能动作来 exploit。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQNModel</span>(<span class="hljs-params">nn.Module</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, input_shape, num_actions</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>(DQNModel, self).__init__()</span><br><span class="line">        self._input_shape = input_shape</span><br><span class="line">        self._num_actions = num_actions</span><br><span class="line"></span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(input_shape[<span class="hljs-number">0</span>], <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">8</span>, stride=<span class="hljs-number">4</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">4</span>, stride=<span class="hljs-number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(self.feature_size, <span class="hljs-number">512</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="hljs-number">512</span>, num_actions)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">self, x</span>):</span></span><br><span class="line">        x = self.features(x).view(x.size()[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>)</span><br><span class="line">        <span class="hljs-keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, state, epsilon, device</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> random() &gt; epsilon:</span><br><span class="line">            state = torch.FloatTensor(np.float32(state)).unsqueeze(<span class="hljs-number">0</span>).to(device)</span><br><span class="line">            q_value = self.forward(state)</span><br><span class="line">            action = q_value.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].item()</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            action = randrange(self._num_actions)</span><br><span class="line">        <span class="hljs-keyword">return</span> action</span><br></pre></td></tr></tbody></table></figure>
<h2 id="experience-replay-缓存">Experience Replay 缓存</h2>
<p>实现采用了 Pytorch CartPole DQN 的官方代码，本质是一个最大为 capacity
的 list 保存采样的 (s, a, r, s', is_done) 五元组。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="hljs-string">'Transition'</span>, (<span class="hljs-string">'state'</span>, <span class="hljs-string">'action'</span>, <span class="hljs-string">'reward'</span>, <span class="hljs-string">'next_state'</span>, <span class="hljs-string">'done'</span>))</span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ReplayMemory</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, capacity</span>):</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        self.memory = []</span><br><span class="line">        self.position = <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">push</span>(<span class="hljs-params">self, *args</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.memory) &lt; self.capacity:</span><br><span class="line">            self.memory.append(<span class="hljs-literal">None</span>)</span><br><span class="line">        self.memory[self.position] = Transition(*args)</span><br><span class="line">        self.position = (self.position + <span class="hljs-number">1</span>) % self.capacity</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sample</span>(<span class="hljs-params">self, batch_size</span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> random.sample(self.memory, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.memory)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="dqnagent">DQNAgent</h2>
<p>我们将 DQN 的逻辑封装在 DQNAgent 类中。DQNAgent 成员变量包括两个
DQNModel，一个ReplayMemory。</p>
<p>train() 方法中会每隔一定时间将 Target Network
的参数同步成现行Network的参数。在td_loss_backprop()方法中采样
ReplayMemory 中的五元组，通过minimize TD error方式来改进现行 Network
参数 <span class="math inline">\(\theta\)</span>。Loss函数为：</p>
<p><span class="math display">\[
L\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right)
\sim \mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}}
Q_{target}\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s,
a ; \theta_{i}\right)\right)^{2}\right]
\]</span></p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DQNAgent</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, state, episode_idx</span>):</span></span><br><span class="line">        self.update_epsilon(episode_idx)</span><br><span class="line">        action = self.model.act(state, self.epsilon, self.device)</span><br><span class="line">        <span class="hljs-keyword">return</span> action</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process</span>(<span class="hljs-params">self, episode_idx, state, action, reward, next_state, done</span>):</span></span><br><span class="line">        self.replay_mem.push(state, action, reward, next_state, done)</span><br><span class="line">        self.train(episode_idx)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, episode_idx</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.replay_mem) &gt; self.initial_learning:</span><br><span class="line">            <span class="hljs-keyword">if</span> episode_idx % self.target_update_frequency == <span class="hljs-number">0</span>:</span><br><span class="line">                self.target_model.load_state_dict(self.model.state_dict())</span><br><span class="line">            self.optimizer.zero_grad()</span><br><span class="line">            self.td_loss_backprop()</span><br><span class="line">            self.optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">td_loss_backprop</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        transitions = self.replay_mem.sample(self.batch_size)</span><br><span class="line">        batch = Transition(*<span class="hljs-built_in">zip</span>(*transitions))</span><br><span class="line"></span><br><span class="line">        state = Variable(FloatTensor(np.float32(batch.state))).to(self.device)</span><br><span class="line">        action = Variable(LongTensor(batch.action)).to(self.device)</span><br><span class="line">        reward = Variable(FloatTensor(batch.reward)).to(self.device)</span><br><span class="line">        next_state = Variable(FloatTensor(np.float32(batch.next_state))).to(self.device)</span><br><span class="line">        done = Variable(FloatTensor(batch.done)).to(self.device)</span><br><span class="line"></span><br><span class="line">        q_values = self.model(state)</span><br><span class="line">        next_q_values = self.target_net(next_state)</span><br><span class="line"></span><br><span class="line">        q_value = q_values.gather(<span class="hljs-number">1</span>, action.unsqueeze(-<span class="hljs-number">1</span>)).squeeze(-<span class="hljs-number">1</span>)</span><br><span class="line">        next_q_value = next_q_values.<span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]</span><br><span class="line">        expected_q_value = reward + self.gamma * next_q_value * (<span class="hljs-number">1</span> - done)</span><br><span class="line"></span><br><span class="line">        loss = (q_value - expected_q_value.detach()).<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>)</span><br><span class="line">        loss = loss.mean()</span><br><span class="line">        loss.backward()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="外层-training-代码">外层 Training 代码</h2>
<p>最后是外层调用代码，基本和以前文章一样。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">env, args, agent</span>):</span></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args.num_episodes):</span><br><span class="line">        episode_reward = <span class="hljs-number">0.0</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">            action = agent.act(state, episode_idx)</span><br><span class="line">            <span class="hljs-keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            next_state, reward, done, stats = env.step(action)</span><br><span class="line">            agent.process(episode_idx, state, action, reward, next_state, done)</span><br><span class="line">            state = next_state</span><br><span class="line">            episode_reward += reward</span><br><span class="line">            <span class="hljs-keyword">if</span> done:</span><br><span class="line">                <span class="hljs-built_in">print</span>(<span class="hljs-string">f'<span class="hljs-subst">{episode_idx}</span>: <span class="hljs-subst">{episode_reward}</span>'</span>)</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/share-rl-bootcamp-berkeley-2017/" itemprop="url">分享课程 Berkeley Deep Reinforcement Learning Bootcamp 2017</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-11-06T18:45:01.000Z" itemprop="datePublished">11月 7 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            4 分钟 读完 (约 576 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>Berkeley 2017年联合了DeepMind 以及 OpenAI
举办了一个大咖云集的深度强化学习训练营，是难得的前沿深度强化学习佳品，本公众号
MyEncyclopedia 用代码实现了权威教材 Sutton &amp; Barto
第二版强化学习的基础部分之后，会大致沿着这个训练营的思路，从原理到代码逐步揭示强化深度学习面纱，并结合各种有意思的游戏环境来演示。</p>
<p><strong>如果没有耐心的同学可以直接跳到文末的百度云盘下载链接，内容涵盖所有视频和slide</strong>。</p>
<p>此次训练营主讲的强化学习领域专家包括</p>
<ul>
<li><p>Pieter Abbeel，前Berkeley
机器人学习实验室主任，伯克利人工智能研究(BAIR)实验室联合主任</p></li>
<li><p>Andrej Karpathy，前 OpenAI研究科学家、现特斯拉AI总监</p></li>
<li><p>Vlad Mnih，Deepmind 研究科学家</p></li>
<li><p>John Schulman，Deepmind 研究科学家，OpenAI共同创建人</p></li>
<li><p>Sergey Levine，Berkeley 计算机副教授</p></li>
</ul>
<figure>
<img src="/zh/2020/share-rl-bootcamp-berkeley-2017/deep-rl-bootcamp.png">
<figcaption>
</figcaption>
</figure>
<h1 id="课程列表">课程列表</h1>
<ul>
<li>Core Lecture 1 Intro to MDPs and Exact Solution Methods -- Pieter
Abbeel</li>
<li>Core Lecture 2 Sample-based Approximations and Fitted Learning --
Rocky Duan</li>
<li>Core Lecture 3 DQN + Variants -- Vlad Mnih</li>
<li>Core Lecture 4a Policy Gradients and Actor Critic -- Pieter
Abbeel</li>
<li>Core Lecture 4b Pong from Pixels -- Andrej Karpathy</li>
<li>Core Lecture 5 Natural Policy Gradients, TRPO, and PPO -- John
Schulman</li>
<li>Core Lecture 6 Nuts and Bolts of Deep RL Experimentation -- John
Schulman</li>
<li>Core Lecture 7 SVG, DDPG, and Stochastic Computation Graphs -- John
Schulman</li>
<li>Core Lecture 8 Derivative-free Methods -- Peter Chen</li>
<li>Core Lecture 9 Model-based RL -- Chelsea Finn</li>
<li>Core Lecture 10a Utilities -- Pieter Abbeel</li>
<li>Core Lecture 10b Inverse RL -- Chelsea Finn</li>
<li>Frontiers Lecture I: Recent Advances, Frontiers and Future of Deep
RL -- Vlad Mnih</li>
<li>Frontiers Lecture II: Recent Advances, Frontiers and Future of Deep
RL -- Sergey Levine</li>
<li>TAs Research Overviews</li>
</ul>
<p>前两讲总结了强化学习基础理论方面，包括用动态规划求精确解，采样与环境交互的传统基本方法。第三四讲覆盖了主流的深度强化学习的几种模式：DQN，PG和AC。第五到七讲深入了深度强化学习的各种前沿方法。值得一提的是第六讲，很好的从实践中总结了各种调试诊断方法。余下的若干讲涉及到了非主流的剩余强化学习领域。</p>
<figure>
<img src="/zh/2020/share-rl-bootcamp-berkeley-2017/landscape.png">
<figcaption>
</figcaption>
</figure>
<figure>
<img src="/zh/2020/share-rl-bootcamp-berkeley-2017/rl-hierarchy.png">
<figcaption>
</figcaption>
</figure>
<h2 id="下载方法">下载方法</h2>
<p>关注 MyEncyclopedia 公众号，输入
<strong>rl-bootcamp-ucb-2017</strong> 即可获得百度云盘链接</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-qlearning-to-dqn/" itemprop="url">通过代码学Sutton强化学习：从Q-Learning 演化到 DQN</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-10-29T18:45:01.000Z" itemprop="datePublished">10月 30 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 分钟 读完 (约 2717 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>上一期 MyEncyclopedia公众号文章 <a href="/zh/2020/rl-qlearning-to-dqn/!--swig￼1--">SARSA、Q-Learning和Expected
SARSA时序差分算法训练CartPole</a>中，我们通过CartPole的OpenAI
Gym环境实现了Q-learning算法，这一期，我们将会分析Q-learning算法面临的maximization
bias 问题和提出double learning算法来改进。接着，我们将tabular
Q-learning算法扩展到用带参函数来近似 Q(s, a)，这就是Deepmind
在2015年Nature上发表的Deep Q Network
（DQN）思想：用神经网络结合Q-learning算法实现超越人类玩家打Atari游戏的水平。</p>
<h2 id="q-learning-回顾">Q-Learning 回顾</h2>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Q-learning (off-policy TD Control) for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy
derived from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
\max_{a}Q(S^{\prime}, a) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<p>在<a href="/zh/2020/rl-qlearning-to-dqn/!--swig￼2--">SARSA、Q-Learning和Expected
SARSA时序差分算法训练CartPole</a>&nbsp;中，我们实现了同样基于 <span class="math inline">\(\epsilon\)</span>-greedy
策略的Q-learning算法和SARSA算法，两者代码上的区别确实不大，但本质上Q-learning是属于
off-policy 范畴而 SARSA却属于 on-policy
范畴。一种理解方式是，Q-learning相比于SARSA少了第二次从 <span class="math inline">\(\epsilon\)</span>-greedy
策略采样出下一个action，即S, A, R', S', A'
五元组中最后一个A'，而直接通过max操作去逼近 <span class="math inline">\(q^{*}\)</span>。如此，Q-learning并没有像SARSA完成一次完整的GPI（Generalized
Policy Iteration），缺乏on-policy的策略迭代的特点，故而 Q-learning
属于off-policy方法。我们也可以从另一个角度来分析两者的区别。注意到这两个算法不是一定非要使用
<span class="math inline">\(\epsilon\)</span>-greedy
策略的。对于Q-learning来说，完全可以使用随机策略，理论上已经证明，只要保证每个action以后依然有几率会被探索下去，Q-learning
最终会收敛到最优策略。Q-learning使用 <span class="math inline">\(\epsilon\)</span>-greedy
是为了能快速收敛。对于SARSA算法来说，则无法使用随机策略，因为随机策略无法形成策略提升。而
<span class="math inline">\(\epsilon\)</span>-greedy
策略却可以形成策略迭代，完成策略提升，当然，<span class="math inline">\(\epsilon\)</span>-greedy 策略在 SARSA
算法中也可以保证快速收敛。因此，尽管两者都使用 <span class="math inline">\(\epsilon\)</span>-greedy
策略再借由环境产生reward和state，它们的作用并非完全一样。至此，我们可以体会到on-policy和off-policy本质的区别。</p>
<h3 id="收敛条件">收敛条件</h3>
Tabular Q-Learning 收敛到最佳Q函数的条件如下[2]:
<div>
<p><span class="math display">\[
\Sigma^{\infty}_{n=0} \alpha_{n} = {\infty} \quad \text{  AND  } \quad
\Sigma^{\infty}_{n=0} \alpha^2_{n} \lt {\infty}
\]</span></p>
</div>
<p>一种方式是将 <span class="math inline">\(\alpha\)</span>设置成 (s,
a)访问次数的倒数：<span class="math inline">\(\alpha_{n}(s,a) = 1/ n(s,a
)\)</span></p>
<p>则整体更新公式为</p>
<p><span class="math display">\[
Q(s,a) \leftarrow Q(s,a) + \alpha_n(s, a)[R+\gamma
\max_{a^{\prime}}Q(s^{\prime}, a^{\prime}) - Q(s, a)]
\]</span></p>
<h3 id="q-learning-最大化偏差问题">Q-Learning 最大化偏差问题</h3>
<p>Q-Learning 会产生最大化偏差问题（Maximization Bias，在Sutton
教材6.7节），它的原因是用估计值中取最大值去估计真实值中最大是有偏的。这个可以做如下试验来模拟，若有5个
[-3, 3] 的离散均匀分布 <span class="math inline">\(d_i\)</span>，<span class="math inline">\(\max(\mathbb{E}[d_i]) =
0\)</span>，但是若我们用单批采样 <span class="math inline">\(x_i \sim
d_i\)</span>来估算 <span class="math inline">\(\mathbb{E}[d_i]\)</span>在取max的话，<span class="math inline">\(\mathbb{E}[{\max(x_i)]}\)</span>
是有bias的。但是如果我们将这个过程分解成选择最大action和评估其值两步，每一步用独立的采样集合就可以做到无偏，这个改进方法称为double
learning。具体过程为第一步在<span class="math inline">\(Q_1\)</span>集合中找到最大的action，第二步在<span class="math inline">\(Q_2\)</span>中返回此action值，即：</p>
<div>
<p><span class="math display">\[
\begin{align*}
A^{\star} = \operatorname{argmax}_{a}Q_1(a) \\
Q_2(A^{\star}) = Q_2(\operatorname{argmax}_{a}Q_1(a))
\end{align*}
\]</span></p>
</div>
<p>则无限模拟后结果是无偏的：<span class="math inline">\(\mathbb{E}[Q_2(A^{\star})] = q(A^{\star})\)</span>
下面是简单模拟试验两种方法的均值比较</p>
<figure>
<img src="/zh/2020/rl-qlearning-to-dqn/double_sampling.png">
<figcaption>
Maximization Bias
</figcaption>
</figure>
<p>试验完整代码如下</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> random</span><br><span class="line"><span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> floor</span><br><span class="line"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np</span><br><span class="line"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd</span><br><span class="line"><span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">uniform</span>(<span class="hljs-params">a: <span class="hljs-built_in">int</span>, b: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">    u = random.random()</span><br><span class="line">    <span class="hljs-keyword">return</span> a + floor((b - a + <span class="hljs-number">1</span>) * u)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span><br><span class="line">    total_max_bias = <span class="hljs-number">0</span></span><br><span class="line">    avgs_max_bias = []</span><br><span class="line">    total_double_sampling = <span class="hljs-number">0</span></span><br><span class="line">    avgs_double_sampling = []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">100</span>):</span><br><span class="line">        samples = np.array([uniform(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])</span><br><span class="line">        max_sample = <span class="hljs-built_in">max</span>(samples)</span><br><span class="line">        total_max_bias += max_sample</span><br><span class="line">        avgs_max_bias.append(total_max_bias / e)</span><br><span class="line"></span><br><span class="line">        samples2 = np.array([uniform(-<span class="hljs-number">3</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)])</span><br><span class="line">        total_double_sampling += samples2[np.argmax(samples)]</span><br><span class="line">        avgs_double_sampling.append(total_double_sampling / e)</span><br><span class="line"></span><br><span class="line">    df = pd.DataFrame({<span class="hljs-string">'Max of Samples'</span>: avgs_max_bias, <span class="hljs-string">'Double Samples'</span>: avgs_double_sampling})</span><br><span class="line">    <span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt</span><br><span class="line">    sns.lineplot(data=df)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></tbody></table></figure>
<p>回到Q-learning 中使用的 <span class="math inline">\(\epsilon\)</span>-greedy策略，Q-learning可以保证随着<span class="math inline">\(\epsilon\)</span> 的减小，最大化偏差会
asymptotically 趋近于真实值，但是double learning
可以更快地趋近于真实值。</p>
<figure>
<img src="/zh/2020/rl-qlearning-to-dqn/double_learning_vs_max_bias.png">
<figcaption>
Maximization Bias vs Double learning
</figcaption>
</figure>
<p>下面是Sutton 强化学习第二版6.7节中完整的Double Q-learning算法。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Double Q-learning, for estimating } Q_1 \approx  Q_2
\approx q_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q_1(s,a), \text{ and } Q_2(s,a) \text{, for all
} s \in \mathcal{S}^{+}, a \in \mathcal{A}(s) \text{, such that }
Q(terminal, \cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy }
\epsilon\text{-greedy in } Q_1 + Q_2 \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad \text{With 0.5 probability:} \\
&amp; \quad \quad \quad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha \left (
R+\gamma Q_2(S^{\prime}, \operatorname{argmax}_{a}Q_1(S^{\prime}, a)) -
Q_1(S,A) \right )\\
&amp; \quad \quad \text{else:} \\
&amp; \quad \quad \quad Q_1(S,A) \leftarrow Q_1(S,A) + \alpha \left (
R+\gamma Q_2(S^{\prime}, \operatorname{argmax}_{a}Q_1(S^{\prime}, a)) -
Q_1(S,A) \right )\\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<p>更详细内容，可以参考 Hado V. Hasselt 的 Double Q-learning paper
[3]。</p>
<h2 id="gradient-q-learning">Gradient Q-Learning</h2>
<p>Tabular
Q-learning由于受制于维度爆炸，无法扩展到高维状态空间，一般近似解决方案是用
approximating function来逼近Q函数。即我们将状态抽象出一组特征 <span class="math inline">\(s = \vec x= [x_1, x_2, ..., x_n]^T\)</span>，Q
用一个 x 的函数来近似表达 <span class="math inline">\(Q(s, a) \approx
g(\vec x;
\theta)\)</span>，如此，就联系起了深度神经网络。有了函数表达，深度学习还必须的元素是损失函数，这个很自然的可以用
TD
error。至此，问题转换成深度学习的几个要素均已具备，Q-learning算法改造成了深度学习中的有监督问题。</p>
<p>估计值：<span class="math inline">\(Q\left(s, a ;
\theta\right)\)</span></p>
<p>目标值：<span class="math inline">\(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta\right)\)</span></p>
<p>损失函数：</p>
<p><span class="math display">\[
L\left(\theta\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim
\mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime},
a^{\prime} ; \theta\right)-Q\left(s, a ; \theta\right)\right)^{2}\right]
\]</span></p>
<h2 id="收敛性分析">收敛性分析</h2>
<p>首先明确一点，至此 gradient q-learning 和 tabular Q-learning
一样，都是没有记忆的，即对于一个新的环境产生的 sample 去做 stochastic
online update。</p>
<p>若Q函数是状态特征的线性函数，即 <span class="math inline">\(Q(s, a;
\theta) = \Sigma_i w_i x_i\)</span> ，那么线性Gradient
Q-learning的收敛条件和Tabular Q-learning 一样，也为</p>
<div>
<p><span class="math display">\[
\Sigma^{\infty}_{n=0} \alpha_{n} = {\infty} \quad \text{  AND  } \quad
\Sigma^{\infty}_{n=0} \alpha^2_{n} \lt {\infty}
\]</span></p>
</div>
<p>若Q函数是非线性函数，即使符合上述条件，也无法保证收敛，本质上源于改变
<span class="math inline">\(\theta\)</span> 使得 Q 值在 (s, a)
点上减小误差会影响 (s, a) 周边点的误差。</p>
<h2 id="dqn减少不收敛的两个技巧">DQN减少不收敛的两个技巧</h2>
<ol type="1">
<li><span class="math inline">\(\theta_{i-1} \rightarrow
\theta_{i}\)</span> 改变导致max中的估计值和目标值中的Q同时变化，面临着
chasing its own tail
的问题。解决的方法是使用不同的参数来parameterize两个Q，并且目标值的Q网络参数固定一段时间产生一批固定策略下的环境采样。这个技巧称为
Target Network。引入这个 trick 后深度学习的要素变成</li>
</ol>
<p>估计值：<span class="math inline">\(Q\left(s, a ;
\theta_{i}\right)\)</span></p>
<p>目标值：<span class="math inline">\(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta_i^{-}\right)\)</span></p>
<p>损失函数，DQN在Nature上的loss函数： <span class="math display">\[
L\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right)
\sim \mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}}
Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ;
\theta_{i}\right)\right)^{2}\right]
\]</span></p>
<ol start="2" type="1">
<li>尽管目标值的 <span class="math inline">\(Q(;\theta^{-})\)</span>固定了，但是<span class="math inline">\(\theta_{i-1} \rightarrow \theta_{i}\)</span>
还会使得估计值的 <span class="math inline">\(Q(s, a;\theta_i)\)</span>
在变化的同时影响其他的 <span class="math inline">\(Q(s_k,
a_j;\theta_i)\)</span>，让之前训练过的 (s,
a)的点的损失值发生变化，解决的办法是将 online stochastic 改成 batch
gradient，也就是将最近的一系列采样值保存下来，这个方法称为 experience
replay。</li>
</ol>
<p>有了这两个优化，Deep Q
Network投入实战效果就容易收敛了，以下是Deepmind 发表在Nature 的
Human-level control through deep reinforcement learning [1]
的完整算法流程。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Deep Q-learning with experience replay}\\
&amp; \text{Initialize replay memory } D\text{ to capacity } N \\
&amp; \text{Initialize action-value function } Q \text{ with random
weights } \theta \\
&amp; \text{Initialize target action-value function } \hat{Q} \text{
with weights } \theta^{-} = \theta \\
&amp; \textbf{For} \text{ episode = 1, } M \textbf{ do} \\
&amp; \text{Initialize sequences } s_1 = \{x_1\} \text{ and preprocessed
sequence } \phi_1 = \phi(s_1)\\
&amp; \quad \textbf{For } t=\text{ 1, T }\textbf{ do} \\
&amp; \quad \quad \text{With probability }\epsilon \text{ select a
random action } a_t \\
&amp; \quad \quad \text{otherwise select } a_t =
\operatorname{argmax}_{a}Q(\phi(s_t), a; \theta)\\
&amp; \quad \quad \text{Execute action } a_t \text{ in emulator and
observe reward } r_t \text{ and image  }x_{t+1}\\
&amp; \quad \quad \text{Set } s_{t+1} = s_t, a_t, x_{t+1} \text{ and
preprocess } \phi_{t+1} = \phi(s_{t+1})\\
&amp; \quad \quad \text{Store transition } (\phi_t, a_t, r_t,
\phi_{t+1}) \text{ in } D\\
&amp; \quad \quad \text{Sample random minibatch of transitions }
(\phi_j, a_j, r_j, \phi_{j+1}) \text{ from } D\\
&amp; \quad \quad \text{Set } y_j=
    \begin{cases}
      r_j \quad \quad\quad\quad\text{if episode terminates at step
j+1}\\
      r_j + \gamma \max_{a^{\prime}}\hat Q(\phi_{j+1}, a^{\prime};
\theta^{-}) \quad \text { otherwise}\\
    \end{cases}       \\
&amp; \quad \quad \text{Perform a gradient descent step on } (y_j -
Q(\phi_j, a_j; \theta))^2 \text{ with respect to the network parameters
} \theta\\
&amp; \quad \quad \text{Every C steps reset } \hat Q = Q\\
&amp; \quad \textbf{End For} \\
&amp; \textbf{End For}
\end{align*}
\]</span></p>
</div>
<h3 id="dqn-with-double-q-learning">DQN with Double Q-Learning</h3>
<p>DQN 算法和 Double Q-Learning 能不能结合起来呢？Hado van Hasselt 在
Deep Reinforcement Learning with Double Q-learning [4] 中提出参考 Double
Q-learning 将 DQN
的目标值改成如下函数，可以进一步提升最初DQN的效果。</p>
<p>目标值：<span class="math inline">\(r+\gamma Q(s^{\prime}, \max
_{a^{\prime}} Q\left(s^{\prime}, a^{\prime}; \theta_t\right);
\theta_t^{-})\)</span></p>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><p><strong>Human-level control through deep reinforcement
learning</strong> Volodymyr Mnih, Koray Kavukcuoglu, David Silver
(2015)</p></li>
<li><p>CS885 Reinforcement Learning Lecture 4b: May 11, 2018</p></li>
<li><p><strong>Double Q-learning</strong> Hado V. Hasselt
(2010)</p></li>
<li><p><strong>Deep Reinforcement Learning with Double
Q-learning</strong> Hado van Hasselt, Arthur Guez, David Silver
(2015)</p></li>
</ol>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/" itemprop="url">通过代码学Sutton强化学习：SARSA、Q-Learning和Expected SARSA时序差分算法训练CartPole</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-10-16T18:45:01.000Z" itemprop="datePublished">10月 17 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 分钟 读完 (约 2450 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>这一期我们进入第六章：时序差分学习（Temporal-Difference
Learning）。TD
Learning本质上是加了bootstrapping的蒙特卡洛（MC），也是model-free的方法，但实践中往往比蒙特卡洛收敛更快。我们选取OpenAI
Gym中经典的CartPole环境来讲解TD。更多相关内容，欢迎关注 <strong>本公众号
MyEncyclopedia</strong>。</p>
<h2 id="cartpole-openai-环境">CartPole OpenAI 环境</h2>
<p>如图所示，小车上放了一根杆，杆会根据物理系统定理因重力而倒下，我们可以控制小车往左或者往右，目的是尽可能地让杆保持树立状态。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_intro.gif">
<figcaption>
CartPole OpenAI Gym
</figcaption>
</figure>
<p>CartPole
观察到的状态是四维的float值，分别是车位置，车速度，杆角度和杆角速度。下表为四个维度的值范围。给到小车的动作，即action
space，只有两种：0，表示往左推；1，表示往右推。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Min</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cart Position</td>
<td>-4.8</td>
<td>4.8</td>
</tr>
<tr class="even">
<td>Cart Velocity</td>
<td>-Inf</td>
<td>Inf</td>
</tr>
<tr class="odd">
<td>Pole Angle</td>
<td>-0.418 rad (-24 deg)</td>
<td>0.418 rad (24 deg)</td>
</tr>
<tr class="even">
<td>Pole Angular Velocity</td>
<td>-Inf</td>
<td>Inf</td>
</tr>
</tbody>
</table>
<h3 id="离散化连续状态">离散化连续状态</h3>
<p>从上所知，CartPole step()
函数返回了4维ndarray，类型为float32的连续状态空间。对于传统的tabular方法来说第一步必须离散化状态，目的是可以作为Q
table的主键来查找。下面定义的State类型是离散化后的具体类型，另外 Action
类型已经是0和1，不需要做离散化处理。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]</span><br><span class="line">Action = <span class="hljs-built_in">int</span></span><br></pre></td></tr></tbody></table></figure>
<p>离散化处理时需要考虑的一个问题是如何设置每个维度的分桶策略。分桶策略会决定性地影响训练的效果。原则上必须将和action以及reward强相关的维度做细粒度分桶，弱相关或者无关的维度做粗粒度分桶。举个例子，小车位置本身并不能影响Agent采取的下一动作，当给定其他三维状态的前提下，因此我们对小车位置这一维度仅设置一个桶（bucket
size=1）。而杆的角度和角速度是决定下一动作的关键因素，因此我们分别设置成6个和12个。</p>
<p>以下是离散化相关代码，四个维度的 buckets=(1, 2, 6,
12)。self.q是action value的查找表，具体类型是shape 为 (1, 2, 6, 12, 2)
的ndarray。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CartPoleAbstractAgent</span>(<span class="hljs-params">metaclass=abc.ABCMeta</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, buckets=(<span class="hljs-params"><span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span></span>), discount=<span class="hljs-number">0.98</span>, lr_min=<span class="hljs-number">0.1</span>, epsilon_min=<span class="hljs-number">0.1</span></span>):</span></span><br><span class="line">        self.env = gym.make(<span class="hljs-string">'CartPole-v0'</span>)</span><br><span class="line"></span><br><span class="line">        env = self.env</span><br><span class="line">        <span class="hljs-comment"># [position, velocity, angle, angular velocity]</span></span><br><span class="line">        self.dims_config = [(env.observation_space.low[<span class="hljs-number">0</span>], env.observation_space.high[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>),</span><br><span class="line">                            (-<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>),</span><br><span class="line">                            (env.observation_space.low[<span class="hljs-number">2</span>], env.observation_space.high[<span class="hljs-number">2</span>], <span class="hljs-number">6</span>),</span><br><span class="line">                            (-math.radians(<span class="hljs-number">50</span>) / <span class="hljs-number">1.</span>, math.radians(<span class="hljs-number">50</span>) / <span class="hljs-number">1.</span>, <span class="hljs-number">12</span>)]</span><br><span class="line">        self.q = np.zeros(buckets + (self.env.action_space.n,))</span><br><span class="line">        self.pi = np.zeros_like(self.q)</span><br><span class="line">        self.pi[:] = <span class="hljs-number">1.0</span> / env.action_space.n</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_bin_idx</span>(<span class="hljs-params">self, val: <span class="hljs-built_in">float</span>, lower: <span class="hljs-built_in">float</span>, upper: <span class="hljs-built_in">float</span>, bucket_num: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">        percent = (val + <span class="hljs-built_in">abs</span>(lower)) / (upper - lower)</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-built_in">min</span>(bucket_num - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>((bucket_num - <span class="hljs-number">1</span>) * percent))))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discretize</span>(<span class="hljs-params">self, obs: np.ndarray</span>) -&gt; State:</span></span><br><span class="line">        discrete_states = <span class="hljs-built_in">tuple</span>([self.to_bin_idx(obs[d], *self.dims_config[d]) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(obs))])</span><br><span class="line">        <span class="hljs-keyword">return</span> discrete_states</span><br></pre></td></tr></tbody></table></figure>
<p>train() 方法串联起来 agent 和 env 交互的流程，包括从 env
得到连续状态转换成离散状态，更新 Agent 的 Q table 甚至
Agent的执行policy，choose_action会根据执行 policy 选取action。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, num_episodes=<span class="hljs-number">2000</span></span>):</span></span><br><span class="line">    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_episodes):</span><br><span class="line">        <span class="hljs-built_in">print</span>(e)</span><br><span class="line">        s: State = self.discretize(self.env.reset())</span><br><span class="line"></span><br><span class="line">        self.adjust_learning_rate(e)</span><br><span class="line">        self.adjust_epsilon(e)</span><br><span class="line">        done = <span class="hljs-literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">            action: Action = self.choose_action(s)</span><br><span class="line">            obs, reward, done, _ = self.env.step(action)</span><br><span class="line">            s_next: State = self.discretize(obs)</span><br><span class="line">            a_next = self.choose_action(s_next)</span><br><span class="line">            self.update_q(s, action, reward, s_next, a_next)</span><br><span class="line">            s = s_next</span><br></pre></td></tr></tbody></table></figure>
<p>choose_action 的默认实现为基于现有 Q table 的 <span class="math inline">\(\epsilon\)</span>-greedy 策略。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span>(<span class="hljs-params">self, state</span>) -&gt; Action:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> np.random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="hljs-keyword">return</span> self.env.action_space.sample()</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> np.argmax(self.q[state])</span><br></pre></td></tr></tbody></table></figure>
<p>抽象出公共的基类代码 CartPoleAbstractAgent
之后，SARSA、Q-Learning和Expected SARSA只需要复写 update_q
抽象方法即可。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CartPoleAbstractAgent</span>(<span class="hljs-params">metaclass=abc.ABCMeta</span>):</span></span><br><span class="line"><span class="hljs-meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="td-learning的精髓">TD Learning的精髓</h2>
<p>在上一期，本公众号 MyEncyclopedia 的<a href="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/!--swig￼8--">21点游戏的蒙特卡洛On-Policy控制</a>介绍了Monte
Carlo方法，知道MC需要在环境中模拟直至最终结局。若记<span class="math inline">\(G_t\)</span>为t步以后的最终return，则 MC online
update 版本更新为：</p>
<p><span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha[G_{t} - V(S_t)]
\]</span></p>
<p>可以认为 <span class="math inline">\(V(S_t)\)</span> 向着目标为 <span class="math inline">\(G_t\)</span> 更新了一小步。</p>
<p>而TD方法可以只模拟下一步，得到 <span class="math inline">\(R_{t+1}\)</span>，而余下步骤的return，<span class="math inline">\(G_t - R_{t+1}\)</span> 用已有的 <span class="math inline">\(V(S_{t+1})\)</span>
来估计，或者统计上称作bootstrapping。这样 TD 的更新目标值变成 <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})\)</span>，整体online
update 公式则为： <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1})- V(S_t)]
\]</span></p>
<p>概念上，如果只使用下一步 <span class="math inline">\(R_{t+1}\)</span>
值然后bootstrap称为
TD(0)，用于区分使用多步后的reward的TD方法。另外，变化的数值 <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})- V(S_t)\)</span>
称为TD error。</p>
<p>另外一个和Monte Carlo的区别在于一般TD方法保存更精细的Q值，<span class="math inline">\(Q(S_t,
A_t)\)</span>，并用Q值来boostrap，而MC一般用V值也可用Q值。</p>
<h2 id="sarsa-on-policy-td-控制">SARSA: On-policy TD 控制</h2>
<p>SARSA的命名源于一次迭代产生了五元组 <span class="math inline">\(S_t，A_t，R_{t+1}，S_{t+1}，A_{t+1}\)</span>。SARSA利用五个值做
action-value的 online update：</p>
<p><span class="math display">\[
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma Q(S_{t+1},
A_{t+1}) - Q(S_t,A_t)]
\]</span></p>
<p>对应的Q table更新实现为： </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SarsaAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] += self.lr * (r + self.discount * (self.q[s_next][a_next]) - self.q[s][a])</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>SARSA 在执行policy
后的Q值更新是对于针对于同一个policy的，完成了一次策略迭代（policy
iteration），这个特点区分于后面的Q-learning算法，这也是SARSA 被称为
On-policy 的原因。下面是完整算法伪代码。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Sarsa (on-policy TD Control) for estimating } Q \approx
q_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Choose } A \text{ from } S \text{ using policy derived
from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad \text{Choose  }A^{\prime} \text { from  } S^{\prime}
\text{ using policy derived from } Q \text{ (e.g., }
\epsilon\text{-greedy)} \\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
Q(S^{\prime}, A^{\prime}) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}; A \leftarrow A^{\prime} \\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<h3 id="sarsa-训练分析">SARSA 训练分析</h3>
<p>SARSA收敛较慢，1000次episode后还无法持久稳定，后面的Q-learning 和
Expected Sarsa 都可以在1000次episode学习长时间保持不倒的状态。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_sarsa_1000.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="q-learning-off-policy-td-控制">Q-Learning: Off-policy TD
控制</h2>
<p>Q-Learning 是深度学习时代前强化学习领域中的著名算法，它的 online
update 公式为： <span class="math display">\[
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma
\max_{a}Q(S_{t+1}, a) - Q(S_t,A_t)]
\]</span></p>
<p>对应的 update_q() 方法具体实现 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QLearningAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] += self.lr * (r + self.discount * np.<span class="hljs-built_in">max</span>(self.q[s_next]) - self.q[s][a])</span><br></pre></td></tr></tbody></table></figure><p></p>
本质上用现有的Q table中最好的action来bootrap 对应的最佳Q值，推导如下：
<div>
<p><span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}}
q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\
&amp;=\mathbb{E}[R \mid S_{t}=s, A_{t}=a] + \gamma\sum_{s^{\prime}}
p\left(s^{\prime}\mid s, a\right)\max _{a^{\prime}}
q_{*}\left(s^{\prime}, a^{\prime}\right) \\
&amp;\approx r + \gamma \max _{a^{\prime}} q_{*}\left(s^{\prime},
a^{\prime}\right)
\end{aligned}
\]</span></p>
</div>
<p>Q-Learning 被称为 off-policy 的原因是它并没有完成一次policy
iteration，而是直接用已有的 Q 来不断近似 <span class="math inline">\(Q_{*}\)</span>。</p>
<p>对比下面的Q-Learning 伪代码和之前的 SARSA
版本可以发现，Q-Learning少了一次模拟后的 <span class="math inline">\(A_{t+1}\)</span>，这也是Q-Learning
中执行policy和预估Q值（即off-policy）分离的一个特征。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Q-learning (off-policy TD Control) for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy
derived from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
\max_{a}Q(S^{\prime}, a) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<h3 id="q-learning-训练分析">Q-Learning 训练分析</h3>
<p>Q-Learning 1000次episode就可以持久稳定住。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_exp_sarsa_1000.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="sarsa-改进版-expected-sarsa">SARSA 改进版 Expected SARSA</h2>
<p>Expected SARSA 改进了 SARSA
的地方在于考虑到了在某一状态下的现有策略动作分布，以此来减少variance，加快收敛，具体更新规则为：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
Q(S_t,A_t) &amp;\leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma
\mathbb{E}_{\pi}[Q(S_{t+1}, A_{t+1} \mid S_{t+1})] - Q(S_t,A_t)] \\
&amp;\leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma \sum_{a}
\pi\left(a\mid S_{t+1}\right) Q(S_{t+1}, a) - Q(S_t,A_t)] \\
\end{aligned}
\]</span></p>
</div>
<p>注意在实现中，update_q() 不仅更新了Q table，还显示更新了执行policy
<span class="math inline">\(\pi\)</span>。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ExpectedSarsaAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] = self.q[s][a] + self.lr * (r + self.discount * np.dot(self.pi[s_next], self.q[s_next]) - self.q[s][a])</span><br><span class="line">        <span class="hljs-comment"># update pi[s]</span></span><br><span class="line">        best_a = np.random.choice(np.where(self.q[s] == <span class="hljs-built_in">max</span>(self.q[s]))[<span class="hljs-number">0</span>])</span><br><span class="line">        n_actions = self.env.action_space.n</span><br><span class="line">        self.pi[s][:] = self.epsilon / n_actions</span><br><span class="line">        self.pi[s][best_a] = <span class="hljs-number">1</span> - (n_actions - <span class="hljs-number">1</span>) * (self.epsilon / n_actions)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>同样的，Expected SARSA 1000次迭代也能比较好的学到最佳policy。</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-blackjack-2/" itemprop="url">通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-29T18:45:01.000Z" itemprop="datePublished">9月 30 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            15 分钟 读完 (约 2204 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼5--">21点游戏的策略蒙特卡洛值预测</a>学习了如何用Monte
Carlo来预估给定策略 <span class="math inline">\(\pi\)</span> 的 <span class="math inline">\(V_{\pi}\)</span> 值之后，这一期我们用Monte
Carlo方法来解得21点游戏最佳策略 <span class="math inline">\(\pi_{*}\)</span>。</p>
<h2 id="蒙特卡洛策略提升">蒙特卡洛策略提升</h2>
<p>回顾一下，在<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼6--">Grid World
策略迭代和值迭代</a>中由于存在Policy Improvement
Theorem，我们可以利用环境dynamics信息计算出策略v值，再选取最greedy
action的方式改进策略，形成策略提示最终能够不断逼近最佳策略。 <span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} v_{*}
\]</span> Monte Carlo Control方法搜寻最佳策略 <span class="math inline">\(\pi{*}\)</span>，是否也能沿用同样的思路呢？答案是可行的。不过，不同于第四章中我们已知环境MDP就知道状态的前后依赖关系，进而从v值中能推断出策略
<span class="math inline">\(\pi\)</span>，在Monte
Carlo方法中，环境MDP是未知的，因而我们只能从action-value中下手，通过海量Monte
Carlo试验来近似 <span class="math inline">\(q_{\pi}\)</span>。有了策略 Q
值，再和MDP策略迭代方法一样，选取最greedy
action的策略，这种策略提示方式理论上被证明了最终能够不断逼近最佳策略。
<span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} q_{*}
\]</span></p>
<p>但是此方法有一个前提要满足，由于数据是依据策略 <span class="math inline">\(\pi_{i}\)</span>
生成的，理论上需要保证在无限次的模拟过程中，每个状态都必须被无限次访问到，才能保证最终每个状态的Q估值收敛到真实的
<span class="math inline">\(q_{*}\)</span>。满足这个前提的一个简单实现是强制随机环境初始状态，保证每个状态都能有一定概率被生成。这个思路就是
Monte Carlo Control with Exploring Starts算法，伪代码如下：</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Monte Carlo ES (Exploring Starts), for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Initialize:} \\
&amp; \quad \pi(s) \in \mathcal A(s) \text{ arbitrarily for all }s \in
\mathcal{S} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Loop forever (for episode):}\\
&amp; \quad \text{Choose } S_0\in \mathcal{S}, A_0 \in \mathcal A(S_0)
\text{ randomly such that all pairs have probability &gt; 0} \\
&amp; \quad \text{Generate an episode from } S_0, A_0 \text{, following
} \pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad \pi(S_t) \leftarrow
\operatorname{argmax}_a Q(S_t, a)\\
\end{align*}
\]</span></p>
</div>
<p>下面我们实现21点游戏的Monte Carlo ES
算法。21点游戏只有200个有效的状态，可以满足算法要求的生成episode前先随机选择某一状态的前提条件。</p>
<p>相对于上一篇，我们增加 ActionValue和Policy的类型定义，ActionValue表示
<span class="math inline">\(q(s, a)\)</span>
，是一个State到动作分布的Dict，Policy
类型也一样。Actions为一维ndarray，维数是离散动作数量。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>]</span><br><span class="line">Action = <span class="hljs-built_in">bool</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">Actions = np.ndarray</span><br><span class="line">ActionValue = <span class="hljs-type">Dict</span>[State, Actions]</span><br><span class="line">Policy = <span class="hljs-type">Dict</span>[State, Actions]</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>下面代码示例如何给定
Policy后，依据指定状态state的动作分布采样，决定下一动作。
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">policy: Policy</span><br><span class="line">A: ActionValue = policy[state]</span><br><span class="line">action = np.random.choice([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], p=A/<span class="hljs-built_in">sum</span>(A))</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>整个算法的 python 代码实现如下：</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_exploring_starts</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        s0 = random.choice(states)</span><br><span class="line">        reset_env_with_s0(env, s0)</span><br><span class="line">        episode_history = gen_custom_s0_stochastic_episode(policy, env, s0)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                best_a = np.argmax(Q[s])</span><br><span class="line">                policy[s][best_a] = <span class="hljs-number">1.0</span></span><br><span class="line">                policy[s][<span class="hljs-number">1</span>-best_a] = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<p>在MC Exploring Starts
算法中，我们需要指定环境初始状态，一种做法是env.reset()时接受初始状态，但是考虑到不去修改第三方实现的
BlackjackEnv类，采用一个取巧的办法，在调用reset()后直接改写env
的私有变量，这个逻辑封装在 reset_env_with_s0 方法中。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_env_with_s0</span>(<span class="hljs-params">env: BlackjackEnv, s0: State</span>) -&gt; BlackjackEnv:</span></span><br><span class="line">    env.reset()</span><br><span class="line">    player_sum = s0[<span class="hljs-number">0</span>]</span><br><span class="line">    oppo_sum = s0[<span class="hljs-number">1</span>]</span><br><span class="line">    has_usable = s0[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    env.dealer[<span class="hljs-number">0</span>] = oppo_sum</span><br><span class="line">    <span class="hljs-keyword">if</span> has_usable:</span><br><span class="line">        env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">11</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> player_sum &gt; <span class="hljs-number">11</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">10</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">10</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">2</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">2</span></span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<h2 id="算法结果的可视化和理论对比">算法结果的可视化和理论对比</h2>
下图是有Usable Ace情况下的理论最优策略。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_usable.png">
<figcaption>
理论最佳策略（有Ace）
</figcaption>
</figure>
Monte
Carlo方法策略提示的收敛是比较慢的，下图是运行10,000,000次episode后有Usable
Ace时的策略 <span class="math inline">\(\pi_{*}^{\prime}\)</span>。对比理论最优策略，MC
ES在不少的状态下还未收敛到理论最优解。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（有Ace）
</figcaption>
</figure>
同样的，下两张图是无Usable Ace情况下的理论最优策略和试验结果的对比。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_no_usable.png">
<figcaption>
理论最佳策略（无Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_no_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（无Ace）
</figcaption>
</figure>
下面的两张图画出了运行代码10,000,000次episode后 <span class="math inline">\(\pi{*}\)</span>的V值图。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_usable.png">
<figcaption>
MC ES 10M的最佳V值（有Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_no_usable.png">
<figcaption>
MC ES 10M的最佳V值（无Ace）
</figcaption>
</figure>
<h2 id="exploring-starts-蒙特卡洛控制改进">Exploring Starts
蒙特卡洛控制改进</h2>
<p>为了避免Monte Carlo ES
Control在初始时必须访问到任意状态的限制，教材中介绍了一种改进算法，On-policy
first-visit MC control for <span class="math inline">\(\epsilon
\text{-soft policies}\)</span> ，它同样基于Monte Carlo 预估Q值，但用
<span class="math inline">\(\epsilon \text{-soft}\)</span>
策略来代替最有可能的action策略作为下一次迭代策略，<span class="math inline">\(\epsilon \text{-soft}\)</span>
本质上来说就是对于任意动作都保留 <span class="math inline">\(\epsilon\)</span>
小概率的访问可能，权衡了exploration和exploitation，由于每个动作都可能被无限次访问到，Explorting
Starts中的强制随机初始状态就可以去除了。Monte Carlo ES Control 和
On-policy first-visit MC control for <span class="math inline">\(\epsilon \text{-soft policies}\)</span>
都属于on-policy算法，其区别于off-policy的本质在于预估 <span class="math inline">\(q_{\pi}(s,a)\)</span>时是否从同策略<span class="math inline">\(\pi\)</span>生成的数据来计算。一个比较subtle的例子是著名的Q-Learning，因为根据这个定义，Q-Learning属于off-policy。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{On-policy first-visit MC control (for }\epsilon
\textbf{-soft policies), estimating } \pi \approx \pi_{*} \\
&amp; \text{Algorithm parameter: small } \epsilon &gt; 0 \\
&amp; \text{Initialize:} \\
&amp; \quad \pi \leftarrow \text{ an arbitrary } \epsilon \text{-soft
policy} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Repeat forever (for episode):}\\
&amp; \quad \text{Generate an episode following } \pi : S_0, A_0, R_1,
S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad A^{*} \leftarrow \operatorname{argmax}_a
Q(S_t, a)\\
&amp; \quad \quad \quad \quad \text{For all } a \in \mathcal A(S_t):\\
&amp; \quad \quad \quad \quad \quad \pi(a|S_t) \leftarrow
    \begin{cases}
      1 - \epsilon + \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a =
A^{*}\\
      \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a \neq A^{*}\\
    \end{cases}       \\
\end{align*}
\]</span></p>
</div>
<p>伪代码对应的 Python 实现如下。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_epsilon_greedy</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span>, epsilon=<span class="hljs-number">0.1</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_epsilon_greedy_policy</span>(<span class="hljs-params">policy: Policy, Q: ActionValue, s: State</span>):</span></span><br><span class="line">        policy[s] = np.ones(env.action_space.n, dtype=<span class="hljs-built_in">float</span>) * epsilon / env.action_space.n</span><br><span class="line">        best_action = np.argmax(Q[s])</span><br><span class="line">        policy[s][best_action] += (<span class="hljs-number">1.0</span> - epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        episode_history = gen_stochastic_episode(policy, env)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                update_epsilon_greedy_policy(policy, Q, s)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
        
<nav class="pagination is-centered is-rounded" role="navigation" aria-label="pagination">
    <div class="pagination-previous is-invisible is-hidden-mobile">
        <a href="/tags/Reinforcement-Learning/page/0/">上一页</a>
    </div>
    <div class="pagination-next">
        <a href="/tags/Reinforcement-Learning/page/2/">下一页</a>
    </div>
    <ul class="pagination-list is-hidden-mobile">
        
        <li><a class="pagination-link is-current" href="/tags/Reinforcement-Learning/">1</a></li>
        
        <li><a class="pagination-link" href="/tags/Reinforcement-Learning/page/2/">2</a></li>
        
    </ul>
</nav>
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/tags/Reinforcement-Learning/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/tags/Reinforcement-Learning/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>