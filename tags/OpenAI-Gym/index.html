<!DOCTYPE html>
<html class="has-navbar-fixed-top">
<head>
    <meta charset="utf-8">
<title>标签: OpenAI Gym - MyEncyclopedia</title>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.css">


<link href="/en/tags/OpenAI-Gym/" rel="alternate" hreflang="en" />
    


<meta name="description" content="">





    <meta property="og:type" content="website">
<meta property="og:title" content="MyEncyclopedia">
<meta property="og:url" content="https://myencyclopedia.github.io/tags/OpenAI-Gym/">
<meta property="og:site_name" content="MyEncyclopedia">
<meta property="og:locale">
<meta property="article:author" content="MyEncyclopedia">
<meta name="twitter:card" content="summary">





<link rel="icon" href="/favicon.png">


<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Ovo|Source+Code+Pro">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/bulma/0.6.2/css/bulma.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/css/justifiedGallery.min.css">


<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">


<link rel="stylesheet" href="/css/style.css">


<script defer src="//use.fontawesome.com/releases/v5.0.8/js/all.js"></script>


    
    
    
    
    
    
    
    
    
    

    


<meta name="generator" content="Hexo 5.4.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>
    
<nav class="navbar is-transparent is-fixed-top navbar-main" role="navigation" aria-label="main navigation">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item navbar-logo" href="/">
                
                <img src="/images/logo.png" alt="" height="28">
                
            </a>
            <div class="navbar-burger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>
        
        <div class="navbar-menu navbar-start">
            
            <a class="navbar-item "
               href="/archives">Archives</a>
            
            <a class="navbar-item "
               href="/about">About</a>
            
        </div>
        
        <div class="navbar-menu navbar-end">
            
            <a class="navbar-item search" title="搜索" href="javascript:;">
                <i class="fas fa-search"></i>
            </a>
            
            
            
            <a class="navbar-item" title="GitHub" href="https://myencyclopedia.github.io">
                
                <i class="fab fa-github"></i>
                
            </a>
               
            
        </div>
    </div>
</nav>

    <section class="section section-heading">
    <div class="container">
        <div class="content">
            <h5>#OpenAI Gym</h5>
        </div>
    </div>
</section>
<section class="section">
    <div class="container">
    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/" itemprop="url">通过代码学Sutton强化学习：SARSA、Q-Learning和Expected SARSA时序差分算法训练CartPole</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-10-16T18:45:01.000Z" itemprop="datePublished">10月 17 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            16 分钟 读完 (约 2450 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>这一期我们进入第六章：时序差分学习（Temporal-Difference
Learning）。TD
Learning本质上是加了bootstrapping的蒙特卡洛（MC），也是model-free的方法，但实践中往往比蒙特卡洛收敛更快。我们选取OpenAI
Gym中经典的CartPole环境来讲解TD。更多相关内容，欢迎关注 <strong>本公众号
MyEncyclopedia</strong>。</p>
<h2 id="cartpole-openai-环境">CartPole OpenAI 环境</h2>
<p>如图所示，小车上放了一根杆，杆会根据物理系统定理因重力而倒下，我们可以控制小车往左或者往右，目的是尽可能地让杆保持树立状态。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_intro.gif">
<figcaption>
CartPole OpenAI Gym
</figcaption>
</figure>
<p>CartPole
观察到的状态是四维的float值，分别是车位置，车速度，杆角度和杆角速度。下表为四个维度的值范围。给到小车的动作，即action
space，只有两种：0，表示往左推；1，表示往右推。</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Min</th>
<th>Max</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cart Position</td>
<td>-4.8</td>
<td>4.8</td>
</tr>
<tr class="even">
<td>Cart Velocity</td>
<td>-Inf</td>
<td>Inf</td>
</tr>
<tr class="odd">
<td>Pole Angle</td>
<td>-0.418 rad (-24 deg)</td>
<td>0.418 rad (24 deg)</td>
</tr>
<tr class="even">
<td>Pole Angular Velocity</td>
<td>-Inf</td>
<td>Inf</td>
</tr>
</tbody>
</table>
<h3 id="离散化连续状态">离散化连续状态</h3>
<p>从上所知，CartPole step()
函数返回了4维ndarray，类型为float32的连续状态空间。对于传统的tabular方法来说第一步必须离散化状态，目的是可以作为Q
table的主键来查找。下面定义的State类型是离散化后的具体类型，另外 Action
类型已经是0和1，不需要做离散化处理。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]</span><br><span class="line">Action = <span class="hljs-built_in">int</span></span><br></pre></td></tr></tbody></table></figure>
<p>离散化处理时需要考虑的一个问题是如何设置每个维度的分桶策略。分桶策略会决定性地影响训练的效果。原则上必须将和action以及reward强相关的维度做细粒度分桶，弱相关或者无关的维度做粗粒度分桶。举个例子，小车位置本身并不能影响Agent采取的下一动作，当给定其他三维状态的前提下，因此我们对小车位置这一维度仅设置一个桶（bucket
size=1）。而杆的角度和角速度是决定下一动作的关键因素，因此我们分别设置成6个和12个。</p>
<p>以下是离散化相关代码，四个维度的 buckets=(1, 2, 6,
12)。self.q是action value的查找表，具体类型是shape 为 (1, 2, 6, 12, 2)
的ndarray。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CartPoleAbstractAgent</span>(<span class="hljs-params">metaclass=abc.ABCMeta</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, buckets=(<span class="hljs-params"><span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span></span>), discount=<span class="hljs-number">0.98</span>, lr_min=<span class="hljs-number">0.1</span>, epsilon_min=<span class="hljs-number">0.1</span></span>):</span></span><br><span class="line">        self.env = gym.make(<span class="hljs-string">'CartPole-v0'</span>)</span><br><span class="line"></span><br><span class="line">        env = self.env</span><br><span class="line">        <span class="hljs-comment"># [position, velocity, angle, angular velocity]</span></span><br><span class="line">        self.dims_config = [(env.observation_space.low[<span class="hljs-number">0</span>], env.observation_space.high[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>),</span><br><span class="line">                            (-<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">1</span>),</span><br><span class="line">                            (env.observation_space.low[<span class="hljs-number">2</span>], env.observation_space.high[<span class="hljs-number">2</span>], <span class="hljs-number">6</span>),</span><br><span class="line">                            (-math.radians(<span class="hljs-number">50</span>) / <span class="hljs-number">1.</span>, math.radians(<span class="hljs-number">50</span>) / <span class="hljs-number">1.</span>, <span class="hljs-number">12</span>)]</span><br><span class="line">        self.q = np.zeros(buckets + (self.env.action_space.n,))</span><br><span class="line">        self.pi = np.zeros_like(self.q)</span><br><span class="line">        self.pi[:] = <span class="hljs-number">1.0</span> / env.action_space.n</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">to_bin_idx</span>(<span class="hljs-params">self, val: <span class="hljs-built_in">float</span>, lower: <span class="hljs-built_in">float</span>, upper: <span class="hljs-built_in">float</span>, bucket_num: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">        percent = (val + <span class="hljs-built_in">abs</span>(lower)) / (upper - lower)</span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-built_in">min</span>(bucket_num - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">int</span>(<span class="hljs-built_in">round</span>((bucket_num - <span class="hljs-number">1</span>) * percent))))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">discretize</span>(<span class="hljs-params">self, obs: np.ndarray</span>) -&gt; State:</span></span><br><span class="line">        discrete_states = <span class="hljs-built_in">tuple</span>([self.to_bin_idx(obs[d], *self.dims_config[d]) <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(obs))])</span><br><span class="line">        <span class="hljs-keyword">return</span> discrete_states</span><br></pre></td></tr></tbody></table></figure>
<p>train() 方法串联起来 agent 和 env 交互的流程，包括从 env
得到连续状态转换成离散状态，更新 Agent 的 Q table 甚至
Agent的执行policy，choose_action会根据执行 policy 选取action。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self, num_episodes=<span class="hljs-number">2000</span></span>):</span></span><br><span class="line">    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_episodes):</span><br><span class="line">        <span class="hljs-built_in">print</span>(e)</span><br><span class="line">        s: State = self.discretize(self.env.reset())</span><br><span class="line"></span><br><span class="line">        self.adjust_learning_rate(e)</span><br><span class="line">        self.adjust_epsilon(e)</span><br><span class="line">        done = <span class="hljs-literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">            action: Action = self.choose_action(s)</span><br><span class="line">            obs, reward, done, _ = self.env.step(action)</span><br><span class="line">            s_next: State = self.discretize(obs)</span><br><span class="line">            a_next = self.choose_action(s_next)</span><br><span class="line">            self.update_q(s, action, reward, s_next, a_next)</span><br><span class="line">            s = s_next</span><br></pre></td></tr></tbody></table></figure>
<p>choose_action 的默认实现为基于现有 Q table 的 <span class="math inline">\(\epsilon\)</span>-greedy 策略。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span>(<span class="hljs-params">self, state</span>) -&gt; Action:</span></span><br><span class="line">    <span class="hljs-keyword">if</span> np.random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="hljs-keyword">return</span> self.env.action_space.sample()</span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">return</span> np.argmax(self.q[state])</span><br></pre></td></tr></tbody></table></figure>
<p>抽象出公共的基类代码 CartPoleAbstractAgent
之后，SARSA、Q-Learning和Expected SARSA只需要复写 update_q
抽象方法即可。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CartPoleAbstractAgent</span>(<span class="hljs-params">metaclass=abc.ABCMeta</span>):</span></span><br><span class="line"><span class="hljs-meta">    @abc.abstractmethod</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="td-learning的精髓">TD Learning的精髓</h2>
<p>在上一期，本公众号 MyEncyclopedia 的<a href="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/!--swig￼8--">21点游戏的蒙特卡洛On-Policy控制</a>介绍了Monte
Carlo方法，知道MC需要在环境中模拟直至最终结局。若记<span class="math inline">\(G_t\)</span>为t步以后的最终return，则 MC online
update 版本更新为：</p>
<p><span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha[G_{t} - V(S_t)]
\]</span></p>
<p>可以认为 <span class="math inline">\(V(S_t)\)</span> 向着目标为 <span class="math inline">\(G_t\)</span> 更新了一小步。</p>
<p>而TD方法可以只模拟下一步，得到 <span class="math inline">\(R_{t+1}\)</span>，而余下步骤的return，<span class="math inline">\(G_t - R_{t+1}\)</span> 用已有的 <span class="math inline">\(V(S_{t+1})\)</span>
来估计，或者统计上称作bootstrapping。这样 TD 的更新目标值变成 <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})\)</span>，整体online
update 公式则为： <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1} + \gamma V(S_{t+1})- V(S_t)]
\]</span></p>
<p>概念上，如果只使用下一步 <span class="math inline">\(R_{t+1}\)</span>
值然后bootstrap称为
TD(0)，用于区分使用多步后的reward的TD方法。另外，变化的数值 <span class="math inline">\(R_{t+1} + \gamma V(S_{t+1})- V(S_t)\)</span>
称为TD error。</p>
<p>另外一个和Monte Carlo的区别在于一般TD方法保存更精细的Q值，<span class="math inline">\(Q(S_t,
A_t)\)</span>，并用Q值来boostrap，而MC一般用V值也可用Q值。</p>
<h2 id="sarsa-on-policy-td-控制">SARSA: On-policy TD 控制</h2>
<p>SARSA的命名源于一次迭代产生了五元组 <span class="math inline">\(S_t，A_t，R_{t+1}，S_{t+1}，A_{t+1}\)</span>。SARSA利用五个值做
action-value的 online update：</p>
<p><span class="math display">\[
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma Q(S_{t+1},
A_{t+1}) - Q(S_t,A_t)]
\]</span></p>
<p>对应的Q table更新实现为： </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SarsaAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] += self.lr * (r + self.discount * (self.q[s_next][a_next]) - self.q[s][a])</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>SARSA 在执行policy
后的Q值更新是对于针对于同一个policy的，完成了一次策略迭代（policy
iteration），这个特点区分于后面的Q-learning算法，这也是SARSA 被称为
On-policy 的原因。下面是完整算法伪代码。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Sarsa (on-policy TD Control) for estimating } Q \approx
q_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Choose } A \text{ from } S \text{ using policy derived
from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad \text{Choose  }A^{\prime} \text { from  } S^{\prime}
\text{ using policy derived from } Q \text{ (e.g., }
\epsilon\text{-greedy)} \\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
Q(S^{\prime}, A^{\prime}) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}; A \leftarrow A^{\prime} \\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<h3 id="sarsa-训练分析">SARSA 训练分析</h3>
<p>SARSA收敛较慢，1000次episode后还无法持久稳定，后面的Q-learning 和
Expected Sarsa 都可以在1000次episode学习长时间保持不倒的状态。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_sarsa_1000.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="q-learning-off-policy-td-控制">Q-Learning: Off-policy TD
控制</h2>
<p>Q-Learning 是深度学习时代前强化学习领域中的著名算法，它的 online
update 公式为： <span class="math display">\[
Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma
\max_{a}Q(S_{t+1}, a) - Q(S_t,A_t)]
\]</span></p>
<p>对应的 update_q() 方法具体实现 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">QLearningAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] += self.lr * (r + self.discount * np.<span class="hljs-built_in">max</span>(self.q[s_next]) - self.q[s][a])</span><br></pre></td></tr></tbody></table></figure><p></p>
本质上用现有的Q table中最好的action来bootrap 对应的最佳Q值，推导如下：
<div>
<p><span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}}
q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\
&amp;=\mathbb{E}[R \mid S_{t}=s, A_{t}=a] + \gamma\sum_{s^{\prime}}
p\left(s^{\prime}\mid s, a\right)\max _{a^{\prime}}
q_{*}\left(s^{\prime}, a^{\prime}\right) \\
&amp;\approx r + \gamma \max _{a^{\prime}} q_{*}\left(s^{\prime},
a^{\prime}\right)
\end{aligned}
\]</span></p>
</div>
<p>Q-Learning 被称为 off-policy 的原因是它并没有完成一次policy
iteration，而是直接用已有的 Q 来不断近似 <span class="math inline">\(Q_{*}\)</span>。</p>
<p>对比下面的Q-Learning 伪代码和之前的 SARSA
版本可以发现，Q-Learning少了一次模拟后的 <span class="math inline">\(A_{t+1}\)</span>，这也是Q-Learning
中执行policy和预估Q值（即off-policy）分离的一个特征。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Q-learning (off-policy TD Control) for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Algorithm parameters: step size }\alpha \in ({0,1}]\text{,
small }\epsilon &gt; 0 \\
&amp; \text{Initialize }Q(s,a),  \text{for all } s \in \mathcal{S}^{+},
a \in \mathcal{A}(s) \text{, arbitrarily except that } Q(terminal,
\cdot) = 0 \\
&amp; \text{Loop for each episode:}\\
&amp; \quad \text{Initialize }S\\
&amp; \quad \text{Loop for each step of episode:} \\
&amp; \quad \quad \text{Choose } A \text{ from } S \text{ using policy
derived from } Q \text{ (e.g., } \epsilon\text{-greedy)} \\
&amp; \quad \quad \text{Take action }A,  \text { observe } R, S^{\prime}
\\
&amp; \quad \quad Q(S,A) \leftarrow Q(S,A) + \alpha[R+\gamma
\max_{a}Q(S^{\prime}, a) - Q(S,A)] \\
&amp; \quad \quad S \leftarrow S^{\prime}\\
&amp; \quad \text{until }S\text{ is terminal} \\
\end{align*}
\]</span></p>
</div>
<h3 id="q-learning-训练分析">Q-Learning 训练分析</h3>
<p>Q-Learning 1000次episode就可以持久稳定住。</p>
<figure>
<img src="/zh/2020/rl-sutton-cartpole-sarsa-qlearning/cartpole_exp_sarsa_1000.gif">
<figcaption>
</figcaption>
</figure>
<h2 id="sarsa-改进版-expected-sarsa">SARSA 改进版 Expected SARSA</h2>
<p>Expected SARSA 改进了 SARSA
的地方在于考虑到了在某一状态下的现有策略动作分布，以此来减少variance，加快收敛，具体更新规则为：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
Q(S_t,A_t) &amp;\leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma
\mathbb{E}_{\pi}[Q(S_{t+1}, A_{t+1} \mid S_{t+1})] - Q(S_t,A_t)] \\
&amp;\leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma \sum_{a}
\pi\left(a\mid S_{t+1}\right) Q(S_{t+1}, a) - Q(S_t,A_t)] \\
\end{aligned}
\]</span></p>
</div>
<p>注意在实现中，update_q() 不仅更新了Q table，还显示更新了执行policy
<span class="math inline">\(\pi\)</span>。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ExpectedSarsaAgent</span>(<span class="hljs-params">CartPoleAbstractAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_q</span>(<span class="hljs-params">self, s: State, a: Action, r, s_next: State, a_next: Action</span>):</span></span><br><span class="line">        self.q[s][a] = self.q[s][a] + self.lr * (r + self.discount * np.dot(self.pi[s_next], self.q[s_next]) - self.q[s][a])</span><br><span class="line">        <span class="hljs-comment"># update pi[s]</span></span><br><span class="line">        best_a = np.random.choice(np.where(self.q[s] == <span class="hljs-built_in">max</span>(self.q[s]))[<span class="hljs-number">0</span>])</span><br><span class="line">        n_actions = self.env.action_space.n</span><br><span class="line">        self.pi[s][:] = self.epsilon / n_actions</span><br><span class="line">        self.pi[s][best_a] = <span class="hljs-number">1</span> - (n_actions - <span class="hljs-number">1</span>) * (self.epsilon / n_actions)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>同样的，Expected SARSA 1000次迭代也能比较好的学到最佳policy。</p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-blackjack-2/" itemprop="url">通过代码学Sutton强化学习4：21点游戏的蒙特卡洛On-Policy控制</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-29T18:45:01.000Z" itemprop="datePublished">9月 30 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            15 分钟 读完 (约 2204 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>这期继续Sutton强化学习第二版，第五章蒙特卡洛方法。在上期<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼5--">21点游戏的策略蒙特卡洛值预测</a>学习了如何用Monte
Carlo来预估给定策略 <span class="math inline">\(\pi\)</span> 的 <span class="math inline">\(V_{\pi}\)</span> 值之后，这一期我们用Monte
Carlo方法来解得21点游戏最佳策略 <span class="math inline">\(\pi_{*}\)</span>。</p>
<h2 id="蒙特卡洛策略提升">蒙特卡洛策略提升</h2>
<p>回顾一下，在<a href="/zh/2020/rl-sutton-blackjack-2/!--swig￼6--">Grid World
策略迭代和值迭代</a>中由于存在Policy Improvement
Theorem，我们可以利用环境dynamics信息计算出策略v值，再选取最greedy
action的方式改进策略，形成策略提示最终能够不断逼近最佳策略。 <span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} v_{*}
\]</span> Monte Carlo Control方法搜寻最佳策略 <span class="math inline">\(\pi{*}\)</span>，是否也能沿用同样的思路呢？答案是可行的。不过，不同于第四章中我们已知环境MDP就知道状态的前后依赖关系，进而从v值中能推断出策略
<span class="math inline">\(\pi\)</span>，在Monte
Carlo方法中，环境MDP是未知的，因而我们只能从action-value中下手，通过海量Monte
Carlo试验来近似 <span class="math inline">\(q_{\pi}\)</span>。有了策略 Q
值，再和MDP策略迭代方法一样，选取最greedy
action的策略，这种策略提示方式理论上被证明了最终能够不断逼近最佳策略。
<span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} q_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} q_{*}
\]</span></p>
<p>但是此方法有一个前提要满足，由于数据是依据策略 <span class="math inline">\(\pi_{i}\)</span>
生成的，理论上需要保证在无限次的模拟过程中，每个状态都必须被无限次访问到，才能保证最终每个状态的Q估值收敛到真实的
<span class="math inline">\(q_{*}\)</span>。满足这个前提的一个简单实现是强制随机环境初始状态，保证每个状态都能有一定概率被生成。这个思路就是
Monte Carlo Control with Exploring Starts算法，伪代码如下：</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Monte Carlo ES (Exploring Starts), for estimating } \pi
\approx \pi_{*} \\
&amp; \text{Initialize:} \\
&amp; \quad \pi(s) \in \mathcal A(s) \text{ arbitrarily for all }s \in
\mathcal{S} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Loop forever (for episode):}\\
&amp; \quad \text{Choose } S_0\in \mathcal{S}, A_0 \in \mathcal A(S_0)
\text{ randomly such that all pairs have probability &gt; 0} \\
&amp; \quad \text{Generate an episode from } S_0, A_0 \text{, following
} \pi : S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad \pi(S_t) \leftarrow
\operatorname{argmax}_a Q(S_t, a)\\
\end{align*}
\]</span></p>
</div>
<p>下面我们实现21点游戏的Monte Carlo ES
算法。21点游戏只有200个有效的状态，可以满足算法要求的生成episode前先随机选择某一状态的前提条件。</p>
<p>相对于上一篇，我们增加 ActionValue和Policy的类型定义，ActionValue表示
<span class="math inline">\(q(s, a)\)</span>
，是一个State到动作分布的Dict，Policy
类型也一样。Actions为一维ndarray，维数是离散动作数量。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>]</span><br><span class="line">Action = <span class="hljs-built_in">bool</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">Actions = np.ndarray</span><br><span class="line">ActionValue = <span class="hljs-type">Dict</span>[State, Actions]</span><br><span class="line">Policy = <span class="hljs-type">Dict</span>[State, Actions]</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>下面代码示例如何给定
Policy后，依据指定状态state的动作分布采样，决定下一动作。
</p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">policy: Policy</span><br><span class="line">A: ActionValue = policy[state]</span><br><span class="line">action = np.random.choice([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>], p=A/<span class="hljs-built_in">sum</span>(A))</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>整个算法的 python 代码实现如下：</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_exploring_starts</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        s0 = random.choice(states)</span><br><span class="line">        reset_env_with_s0(env, s0)</span><br><span class="line">        episode_history = gen_custom_s0_stochastic_episode(policy, env, s0)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                best_a = np.argmax(Q[s])</span><br><span class="line">                policy[s][best_a] = <span class="hljs-number">1.0</span></span><br><span class="line">                policy[s][<span class="hljs-number">1</span>-best_a] = <span class="hljs-number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<p>在MC Exploring Starts
算法中，我们需要指定环境初始状态，一种做法是env.reset()时接受初始状态，但是考虑到不去修改第三方实现的
BlackjackEnv类，采用一个取巧的办法，在调用reset()后直接改写env
的私有变量，这个逻辑封装在 reset_env_with_s0 方法中。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset_env_with_s0</span>(<span class="hljs-params">env: BlackjackEnv, s0: State</span>) -&gt; BlackjackEnv:</span></span><br><span class="line">    env.reset()</span><br><span class="line">    player_sum = s0[<span class="hljs-number">0</span>]</span><br><span class="line">    oppo_sum = s0[<span class="hljs-number">1</span>]</span><br><span class="line">    has_usable = s0[<span class="hljs-number">2</span>]</span><br><span class="line"></span><br><span class="line">    env.dealer[<span class="hljs-number">0</span>] = oppo_sum</span><br><span class="line">    <span class="hljs-keyword">if</span> has_usable:</span><br><span class="line">        env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span></span><br><span class="line">        env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">11</span></span><br><span class="line">    <span class="hljs-keyword">else</span>:</span><br><span class="line">        <span class="hljs-keyword">if</span> player_sum &gt; <span class="hljs-number">11</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">10</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">10</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            env.player[<span class="hljs-number">0</span>] = <span class="hljs-number">2</span></span><br><span class="line">            env.player[<span class="hljs-number">1</span>] = player_sum - <span class="hljs-number">2</span></span><br><span class="line">    <span class="hljs-keyword">return</span> env</span><br></pre></td></tr></tbody></table></figure>
<h2 id="算法结果的可视化和理论对比">算法结果的可视化和理论对比</h2>
下图是有Usable Ace情况下的理论最优策略。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_usable.png">
<figcaption>
理论最佳策略（有Ace）
</figcaption>
</figure>
Monte
Carlo方法策略提示的收敛是比较慢的，下图是运行10,000,000次episode后有Usable
Ace时的策略 <span class="math inline">\(\pi_{*}^{\prime}\)</span>。对比理论最优策略，MC
ES在不少的状态下还未收敛到理论最优解。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（有Ace）
</figcaption>
</figure>
同样的，下两张图是无Usable Ace情况下的理论最优策略和试验结果的对比。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/optimal_policy_no_usable.png">
<figcaption>
理论最佳策略（无Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_no_usable_policy.png">
<figcaption>
MC ES 10M的最佳策略（无Ace）
</figcaption>
</figure>
下面的两张图画出了运行代码10,000,000次episode后 <span class="math inline">\(\pi{*}\)</span>的V值图。
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_usable.png">
<figcaption>
MC ES 10M的最佳V值（有Ace）
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-2/mc_es_10m_no_usable.png">
<figcaption>
MC ES 10M的最佳V值（无Ace）
</figcaption>
</figure>
<h2 id="exploring-starts-蒙特卡洛控制改进">Exploring Starts
蒙特卡洛控制改进</h2>
<p>为了避免Monte Carlo ES
Control在初始时必须访问到任意状态的限制，教材中介绍了一种改进算法，On-policy
first-visit MC control for <span class="math inline">\(\epsilon
\text{-soft policies}\)</span> ，它同样基于Monte Carlo 预估Q值，但用
<span class="math inline">\(\epsilon \text{-soft}\)</span>
策略来代替最有可能的action策略作为下一次迭代策略，<span class="math inline">\(\epsilon \text{-soft}\)</span>
本质上来说就是对于任意动作都保留 <span class="math inline">\(\epsilon\)</span>
小概率的访问可能，权衡了exploration和exploitation，由于每个动作都可能被无限次访问到，Explorting
Starts中的强制随机初始状态就可以去除了。Monte Carlo ES Control 和
On-policy first-visit MC control for <span class="math inline">\(\epsilon \text{-soft policies}\)</span>
都属于on-policy算法，其区别于off-policy的本质在于预估 <span class="math inline">\(q_{\pi}(s,a)\)</span>时是否从同策略<span class="math inline">\(\pi\)</span>生成的数据来计算。一个比较subtle的例子是著名的Q-Learning，因为根据这个定义，Q-Learning属于off-policy。</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{On-policy first-visit MC control (for }\epsilon
\textbf{-soft policies), estimating } \pi \approx \pi_{*} \\
&amp; \text{Algorithm parameter: small } \epsilon &gt; 0 \\
&amp; \text{Initialize:} \\
&amp; \quad \pi \leftarrow \text{ an arbitrary } \epsilon \text{-soft
policy} \\
&amp; \quad Q(s, a) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S}, a \in \mathcal A(s) \\
&amp; \quad Returns(s, a) \leftarrow \text{ an empty list, for all }s
\in \mathcal{S}, a \in \mathcal A(s)\\
&amp; \\
&amp; \text{Repeat forever (for episode):}\\
&amp; \quad \text{Generate an episode following } \pi : S_0, A_0, R_1,
S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless the pair } S_t, A_t \text{ appears
in } S_0, A_0, S_1, A_1, ..., S_{t-1}, A_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t,
A_t) \\
&amp; \quad \quad \quad \quad Q(S_t, A_t) \leftarrow
\operatorname{average}(Returns(S_t, A_t))\\
&amp; \quad \quad \quad \quad A^{*} \leftarrow \operatorname{argmax}_a
Q(S_t, a)\\
&amp; \quad \quad \quad \quad \text{For all } a \in \mathcal A(S_t):\\
&amp; \quad \quad \quad \quad \quad \pi(a|S_t) \leftarrow
    \begin{cases}
      1 - \epsilon + \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a =
A^{*}\\
      \epsilon / |\mathcal A(S_t)| &amp; \text{ if } a \neq A^{*}\\
    \end{cases}       \\
\end{align*}
\]</span></p>
</div>
<p>伪代码对应的 Python 实现如下。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_control_epsilon_greedy</span>(<span class="hljs-params">env: BlackjackEnv, num_episodes, discount_factor=<span class="hljs-number">1.0</span>, epsilon=<span class="hljs-number">0.1</span></span>) \</span></span><br><span class="line"><span class="hljs-function">        -&gt; <span class="hljs-type">Tuple</span>[ActionValue, Policy]:</span></span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    states = <span class="hljs-built_in">list</span>(product(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>, <span class="hljs-number">22</span>), <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>), (<span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>)))</span><br><span class="line">    policy = {s: np.ones(env.action_space.n) * <span class="hljs-number">1.0</span> / env.action_space.n <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> states}</span><br><span class="line">    Q = defaultdict(<span class="hljs-keyword">lambda</span>: np.zeros(env.action_space.n))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_epsilon_greedy_policy</span>(<span class="hljs-params">policy: Policy, Q: ActionValue, s: State</span>):</span></span><br><span class="line">        policy[s] = np.ones(env.action_space.n, dtype=<span class="hljs-built_in">float</span>) * epsilon / env.action_space.n</span><br><span class="line">        best_action = np.argmax(Q[s])</span><br><span class="line">        policy[s][best_action] += (<span class="hljs-number">1.0</span> - epsilon)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        episode_history = gen_stochastic_episode(policy, env)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">and</span> s_a_r[<span class="hljs-number">1</span>] == a <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s, a] += G</span><br><span class="line">                returns_count[s, a] += <span class="hljs-number">1.0</span></span><br><span class="line">                Q[s][a] = returns_sum[s, a] / returns_count[s, a]</span><br><span class="line">                update_epsilon_greedy_policy(policy, Q, s)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> Q, policy</span><br></pre></td></tr></tbody></table></figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-blackjack-1/" itemprop="url">通过代码学Sutton强化学习3：21点游戏的策略蒙特卡洛值预测</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-25T18:45:01.000Z" itemprop="datePublished">9月 26 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            19 分钟 读完 (约 2814 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>从这期开始我们进入Sutton强化学习第二版，第五章蒙特卡洛方法。蒙特卡洛方法是一种在工程各领域都存在的基本方法，在强化领域中，其特点是无需知道环境的dynamics，只需不断模拟记录并分析数据即可逼近理论真实值。蒙特卡洛方法本篇将会用21点游戏作为示例来具体讲解其原理和代码实现。</p>
<h2 id="点游戏问题">21点游戏问题</h2>
<p>21点游戏是一个经典的赌博游戏。大致规则是玩家和庄家各发两张牌，一张明牌，一张暗牌。玩家和庄家可以决定加牌或停止加牌，新加的牌均为暗牌，最后比较两个玩家的牌面和，更接近21点的获胜。游戏的变化因素是牌Ace，既可以作为11也可以作为1来计算，算作11的时候称作usable。</p>
<p>Sutton教材中的21点游戏规则简化了几个方面用于控制问题状态数：</p>
<ul>
<li>已发的牌的无状态性：和一副牌的21点游戏不同的是，游戏环境简化为牌是可以无穷尽被补充的，一副牌的某一张被派发后，同样的牌会被补充进来，或者可以认为每次发放的牌都是从一副新牌中抽出的。统计学中的术语称为重复采样（sample
with replacement）。这种规则下极端情况下，玩家可以拥有
5个A或者5个2。另外，这会导致玩家无法通过开局看到的3张牌的信息推断后续发牌的概率，如此就大规模减小了游戏状态数。</li>
<li>庄家和玩家独立游戏，无需按轮次要牌。开局给定4张牌后，玩家先行动，加牌直至超21点或者停止要牌，如果超21点，玩家输，否则，等待庄家行动，庄家加牌直至超21点或者停止要牌，如果超21点，庄家输，否则比较两者的总点数。这种方式可以认为当玩家和庄家看到初始的三张牌后独立做一系列决策，最后比较结果，避免了交互模式下因为能观察到每一轮后对方牌数变化产生额外的信息而导致的游戏状态数爆炸。</li>
</ul>
<p>有以上两个规则的简化，21点游戏问题的总状态数有下面三个维度</p>
<ul>
<li><p>自己手中的点数和（12到21）</p></li>
<li><p>庄家明牌的点数（A到10)</p></li>
<li><p>庄家明牌是否有 A（True, False）。</p></li>
</ul>
<p>状态总计总数为三个维度的乘积 10 * 10 * 2 = 200。</p>
<p>关于游戏状态有几个比较subtle的假设或者要素。首先，玩家初始时能看到三张牌，这三张牌确定了状态的三个维度的值，当然也就确定了Agent的初始状态，随后按照独立游戏的规则进行，玩家根据初始状态依照某种策略决策要牌还是结束要牌，新拿的牌更新了游戏状态，玩家转移到新状态下继续做决策。举个例子，假设初始时玩家明牌为8，暗牌为6，庄家明牌为7，则游戏状态为Tuple
(14, 7,
False)。若玩家的策略为教材中的固定规则策略：没到20或者21继续要牌。下一步玩家拿到牌3，则此时新状态为
(17, 7, False)，按照策略继续要牌。</p>
<p>第二个方面是游戏的状态完全等价于玩家观察到的信息。比如尽管初始时有4张牌，真正的状态是这四张牌的值，但是出于简化目的，不考虑partially
observable
的情况，即不将暗牌纳入游戏状态中。另外，庄家做决策的时候也无法得知玩家的手中的总牌数。</p>
<p>第三个方面是关于玩家点数。考虑玩家初始时的两张牌为2，3，总点数是5，那么为何不将5加入到游戏状态中呢？原则上是可以将初始总和为2到11都加入到游戏状态，但是意义不大，原因在于我们已经假设了已发牌的无状态性，拿到的这两张牌并不会改变后续补充的牌的出现概率。当玩家初始总和为2到11时一定会追加牌，因为无论第三张牌是什么，都不会超过21点，只会增加获胜概率。若后续第三张牌为8，总和变成13，就进入了有效的游戏状态，因为此时如果继续要牌，获得10，则游戏输掉。因此，我们关心的游戏状态并不完全等价于所有可能的游戏状态。</p>
<h2 id="点游戏-openai-gym环境">21点游戏 OpenAI Gym环境</h2>
<p>OpenAI Gym
已经实现了Sutton版本的21点游戏环境，并按上述规则来进行。在安装完OpenAI
Gym包之后 import BlackjackEnv即可使用。</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> gym.envs.toy_text <span class="hljs-keyword">import</span> BlackjackEnv</span><br></pre></td></tr></tbody></table></figure>
<p>根据这个游戏环境，我们先来定义一些类型，可以令代码更具可读性和抽象化。State
上文说过是由三个分量组成的Tuple。Action 为bool类型
表示是否继续要牌。Reward 为+1或者-1，玩家叫牌过程中为0。StateValue
为书中的 <span class="math inline">\(V_{\pi}\)</span>，实现上是一个Dict。DeterministicPolicy
为一个函数，输入是某一状态，输出是唯一的决策动作。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">State = <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>]</span><br><span class="line">Action = <span class="hljs-built_in">bool</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">StateValue = <span class="hljs-type">Dict</span>[State, <span class="hljs-built_in">float</span>]</span><br><span class="line">DeterministicPolicy = <span class="hljs-type">Callable</span>[[State], Action]</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>以下代码是 BlackjackEnv 核心代码，step
方法的输入为玩家的决策动作（叫牌还是结束），并输出State, Reward,
is_done。简单解释一下代码逻辑，当玩家继续加牌时，需要判断是否超21点，如果没有超过的话，返回下一状态，同时reward
为0，等待下一step方法。若玩家停止叫牌，则按照庄家策略：小于17时叫牌。游戏终局时产生+1表示玩家获胜，-1表示庄家获胜。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BlackjackEnv</span>(<span class="hljs-params">gym.Env</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action</span>):</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> self.action_space.contains(action)</span><br><span class="line">        <span class="hljs-keyword">if</span> action:  <span class="hljs-comment"># hit: add a card to players hand and return</span></span><br><span class="line">            self.player.append(draw_card(self.np_random))</span><br><span class="line">            <span class="hljs-keyword">if</span> is_bust(self.player):</span><br><span class="line">                done = <span class="hljs-literal">True</span></span><br><span class="line">                reward = -<span class="hljs-number">1.</span></span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                done = <span class="hljs-literal">False</span></span><br><span class="line">                reward = <span class="hljs-number">0.</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:  <span class="hljs-comment"># stick: play out the dealers hand, and score</span></span><br><span class="line">            done = <span class="hljs-literal">True</span></span><br><span class="line">            <span class="hljs-keyword">while</span> sum_hand(self.dealer) &lt; <span class="hljs-number">17</span>:</span><br><span class="line">                self.dealer.append(draw_card(self.np_random))</span><br><span class="line">            reward = cmp(score(self.player), score(self.dealer))</span><br><span class="line">            <span class="hljs-keyword">if</span> self.natural <span class="hljs-keyword">and</span> is_natural(self.player) <span class="hljs-keyword">and</span> reward == <span class="hljs-number">1.</span>:</span><br><span class="line">                reward = <span class="hljs-number">1.5</span></span><br><span class="line">        <span class="hljs-keyword">return</span> self._get_obs(), reward, done, {}</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_get_obs</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> (sum_hand(self.player), self.dealer[<span class="hljs-number">0</span>], usable_ace(self.player))</span><br></pre></td></tr></tbody></table></figure>
<p>下面示例如何调用step方法生成一个episode的数据集。数据集的类型为
List[Tuple[State, Action, Reward]]。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gen_episode_data</span>(<span class="hljs-params">policy: DeterministicPolicy, env: BlackjackEnv</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[State, Action, Reward]]:</span></span><br><span class="line">    episode_history = []</span><br><span class="line">    state = env.reset()</span><br><span class="line">    done = <span class="hljs-literal">False</span></span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">        action = policy(state)</span><br><span class="line">        next_state, reward, done, _ = env.step(action)</span><br><span class="line">        episode_history.append((state, action, reward))</span><br><span class="line">        state = next_state</span><br><span class="line">    <span class="hljs-keyword">return</span> episode_history</span><br></pre></td></tr></tbody></table></figure>
<h2 id="策略的蒙特卡洛值预测">策略的蒙特卡洛值预测</h2>
<p>Monte Carlo Prediction解决如下问题：当给定Agent 策略<span class="math inline">\(\pi\)</span>时，反复试验来预估策略的 <span class="math inline">\(V_{\pi}\)</span>
值。具体来说，产生一系列的episode数据之后，对于出现了的所有状态分别计算其Return，再通过
average 某一状态 s 的Return来估计 <span class="math inline">\(V_{\pi}(s)\)</span>，理论上，依据大数定理（Law of
large numbers），在可以无限模拟的情况下，Monte Carlo prediction
一定会收敛到真实的 <span class="math inline">\(V_{\pi}\)</span>。算法实现上有两个略微不同的版本，一个版本称为
First-visit，另一个版本称为 Every-visit，区别在于如何计算出现的状态 s 的
Return值。</p>
<p>对于 First-visit 来说，当状态 s 第一次出现时计算一次
Returns，若继续出现状态 s 不再重复计算。对于Every-visit来说，每次出现 s
计算一次 Returns(s)。举个例子，某episode 数据如下： <span class="math display">\[
S_1, R_1, S_2, R_2, S_1, R_3, S_3, R_4
\]</span> First-visit 对于状态S1的Returns计算为</p>
<p><span class="math display">\[
Returns(S_1) = R_1 + R_2 + R_3 + R_4
\]</span></p>
<p>Every-visit 对于状态S1的Returns计算了两次，因为S1出现了两次。 <span class="math display">\[
\begin{align*}
Returns(S_1) = \frac{Return_1(S_1) + Return_2(S_1)}2 \\
= \frac{(R_1 + R_2 + R_3 + R_4) + (R_3 + R_4)} 2
\end{align*}
\]</span></p>
<p>下面用Monte
Carlo来模拟解得书中示例玩家固定策略的V值，策略具体为：加牌直到手中点数&gt;=20，代码为</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">fixed_policy</span>(<span class="hljs-params">observation</span>):</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    sticks if the player score is &gt;= 20 and hits otherwise.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line">    score, dealer_score, usable_ace = observation</span><br><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> score &gt;= <span class="hljs-number">20</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="first-visit-mc-predicition">First-visit MC Predicition</h3>
伪代码如下，注意考虑到实现上的高效性，在遍历episode序列数据时是从后向前扫的，这样可以边扫边更新G。
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{First-visit MC prediction, for estimating } V \approx
v_{\pi} \\
&amp; \text{Input: a policy } \pi \text{ to be evaluated} \\
&amp; \text{Initialize} \\
&amp; \quad V(s) \in \mathbb R \text{, arbitrarily, for all }s \in
\mathcal{S} \\
&amp; \quad Returns(s) \leftarrow \text{ an empty list, arbitrarily, for
all }s \in \mathcal{S} \\
&amp; \\
&amp; \text{Loop forever (for episode):}\\
&amp; \quad \text{Generate an episode following } \pi: S_0, A_0, R_1,
S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T\\
&amp; \quad G \leftarrow 0\\
&amp; \quad \text{Loop for each step of episode, } t = T-1, T-2, ...,
0:\\
&amp; \quad \quad \quad G \leftarrow \gamma G + R_{t+1}\\
&amp; \quad \quad \quad \text{Unless } S_t \text{ appears in } S_0, S_1,
..., S_{t-1}\\
&amp; \quad \quad \quad \quad \text{Append } G \text { to }Returns(S_t)
\\
&amp; \quad \quad \quad \quad V(S_t) \leftarrow
\operatorname{average}(Returns(S_t))\\
\end{align*}
\]</span></p>
</div>
<p>对应的 python 实现</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_prediction_first_visit</span>(<span class="hljs-params">policy: DeterministicPolicy, env: BlackjackEnv,</span></span></span><br><span class="line"><span class="hljs-params"><span class="hljs-function">                              num_episodes, discount_factor=<span class="hljs-number">1.0</span></span>) -&gt; StateValue:</span></span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        episode_history = gen_episode_data(policy, env)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(s_a_r[<span class="hljs-number">0</span>] == s <span class="hljs-keyword">for</span> s_a_r <span class="hljs-keyword">in</span> episode_history[<span class="hljs-number">0</span>: t]):</span><br><span class="line">                returns_sum[s] += G</span><br><span class="line">                returns_count[s] += <span class="hljs-number">1.0</span></span><br><span class="line"></span><br><span class="line">    V = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    V.update({s: returns_sum[s] / returns_count[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> returns_sum.keys()})</span><br><span class="line">    <span class="hljs-keyword">return</span> V</span><br></pre></td></tr></tbody></table></figure>
<h3 id="every-visit-mc-prediciton">Every-visit MC Prediciton</h3>
<p>Every-visit 代码实现相对更简单一些，t
从后往前遍历时更新对应s的状态变量。如下所示</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mc_prediction_every_visit</span>(<span class="hljs-params">policy: DeterministicPolicy, env: BlackjackEnv,</span></span></span><br><span class="line"><span class="hljs-params"><span class="hljs-function">                              num_episodes, discount_factor=<span class="hljs-number">1.0</span></span>) -&gt; StateValue:</span></span><br><span class="line">    returns_sum = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    returns_count = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> episode_i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, num_episodes + <span class="hljs-number">1</span>):</span><br><span class="line">        episode_history = gen_episode_data(policy, env)</span><br><span class="line"></span><br><span class="line">        G = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(episode_history) - <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>):</span><br><span class="line">            s, a, r = episode_history[t]</span><br><span class="line">            G = discount_factor * G + r</span><br><span class="line">            returns_sum[s] += G</span><br><span class="line">            returns_count[s] += <span class="hljs-number">1.0</span></span><br><span class="line"></span><br><span class="line">    V = defaultdict(<span class="hljs-built_in">float</span>)</span><br><span class="line">    V.update({s: returns_sum[s] / returns_count[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> returns_sum.keys()})</span><br><span class="line">    <span class="hljs-keyword">return</span> V</span><br></pre></td></tr></tbody></table></figure>
<h2 id="策略-v值-3d-可视化">策略 V值 3D 可视化</h2>
<p>运行first-visit
算法，模拟10000次episode，fixed_policy的V值的3D图为下面两张图，分别是不含usable
Ace和包含usable
Ace。总的说来，一旦玩家能到达20点或21点获胜概率极大，到达13-17获胜概率较小，在11-13时有一定获胜概率，比较符合经验直觉。</p>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-1/mc_v_first_visit_10000_no_usable.png">
<figcaption>
first-visit MC 10000次没有usable A的V值
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-1/mc_v_first_visit_10000_usable.png">
<figcaption>
first-visit MC 10000次含有usable A的V值
</figcaption>
</figure>
<p>同样运行every-visit
算法，模拟10000次的V值图。对比两种方法结果比较接近。</p>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-1/mc_v_every_visit_10000_no_usable.png">
<figcaption>
every-visit MC 10000次没有usable A的V值
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-blackjack-1/mc_v_every_visit_10000_usable.png">
<figcaption>
every-visit MC 10000次含有usable A的V值
</figcaption>
</figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-gridworld-2/" itemprop="url">通过代码学Sutton强化学习2：Grid World 策略迭代和值迭代</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-11T18:45:01.000Z" itemprop="datePublished">9月 12 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            15 分钟 读完 (约 2285 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>上一期 <a href="/zh/2020/rl-sutton-gridworld-2/!--swig￼6--">通过代码学Sutton强化学习1：Grid World
OpenAI环境和策略评价算法</a>，我们引入了 Grid World
问题，实现了对应的OpenAI Gym
环境，也分析了其最佳策略和对应的V值。这一期中，继续通过这个例子详细讲解策略提升（Policy
Improvment）、策略迭代（Policy Iteration）、值迭代（Value
Iteration）和异步迭代方法。</p>
<h2 id="回顾-grid-world-问题">回顾 Grid World 问题</h2>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/grid_world.png">
<figcaption>
Grid World 问题
</figcaption>
</figure>
在Grid World 中，Agent初始可以出现在编号1-14的网格中，Agent
每往四周走一步得到 -1
reward，因此需要尽快走到两个出口。当然最佳策略是以最小步数往出口逃离，如下所示。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/optimal_policy.png">
<figcaption>
Grid World 最佳策略
</figcaption>
</figure>
<p>最佳策略对应的状态V值和3D heatmap如下 </p><figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[ <span class="hljs-number">0.</span> -<span class="hljs-number">1.</span> -<span class="hljs-number">2.</span> -<span class="hljs-number">3.</span>]</span><br><span class="line"> [-<span class="hljs-number">1.</span> -<span class="hljs-number">2.</span> -<span class="hljs-number">3.</span> -<span class="hljs-number">2.</span>]</span><br><span class="line"> [-<span class="hljs-number">2.</span> -<span class="hljs-number">3.</span> -<span class="hljs-number">2.</span> -<span class="hljs-number">1.</span>]</span><br><span class="line"> [-<span class="hljs-number">3.</span> -<span class="hljs-number">2.</span> -<span class="hljs-number">1.</span>  <span class="hljs-number">0.</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/value_3d.png">
<figcaption>
Grid World V值 3D heatmap
</figcaption>
</figure>
<h2 id="策略迭代">策略迭代</h2>
<p>上一篇中，我们知道如何evaluate 给定policy <span class="math inline">\(\pi\)</span> 的 <span class="math inline">\(v_{\pi}\)</span>值，那么是否可能在此基础上改进生成更好的策略
<span class="math inline">\(\pi^{\prime}\)</span>。如果可以，能否最终找到最佳策略<span class="math inline">\({\pi}_{*}\)</span>？答案是肯定的，因为存在策略提升定理（Policy
Improvement Theorem）。</p>
<h3 id="策略提升定理">策略提升定理</h3>
<p>在 4.2 节 Policy Improvement Theorem 可以证明，利用 <span class="math inline">\(v_{\pi}\)</span> 信息对于每个状态采取最 greedy 的
action （又称exploitation）能够保证生成的新 <span class="math inline">\({\pi}^{\prime}\)</span> 是不差于旧的policy <span class="math inline">\({\pi}\)</span>，即</p>
<div>
<p><span class="math display">\[
q_{\pi}(s, {\pi}^{\prime}(s)) \gt v_{\pi}(s)
\]</span></p>
</div>
<div>
<p><span class="math display">\[
v_{\pi^{\prime}}(s) \gt v_{\pi}(s)
\]</span></p>
</div>
因此，可以通过在当前policy求得v值，再选取最greedy
action的方式形成如下迭代，就能够不断逼近最佳策略。
<div>
<p><span class="math display">\[
\pi_{0} \stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{0}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{1}
\stackrel{\mathrm{E}}{\longrightarrow} v_{\pi_{1}}
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{2}
\stackrel{\mathrm{E}}{\longrightarrow} \cdots
\stackrel{\mathrm{I}}{\longrightarrow} \pi_{*}
\stackrel{\mathrm{E}}{\longrightarrow} v_{*}
\]</span></p>
</div>
<h3 id="策略迭代算法">策略迭代算法</h3>
以下为书中4.3的policy iteration伪代码。其中policy
evaluation的算法在上一篇中已经实现。Policy improvement
的精髓在于一次遍历所有状态后，通过policy
的最大Q值找到该状态的最佳action，并更新成最新policy，循环直至没有 action
变更。
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Policy Iteration (using iterative policy evaluation) for
estimating } \pi\approx {\pi}_{*} \\
&amp;1. \quad \text{Initialization} \\
&amp; \quad \quad V(s) \in \mathbb R\text{ and } \pi(s) \in \mathcal
A(s) \text{ arbitrarily for all }s \in \mathcal{S} \\
&amp; \\
&amp;2. \quad \text{Policy Evaluation} \\
&amp; \quad \quad \text{Loop:}\\
&amp; \quad \quad \Delta \leftarrow 0\\
&amp; \quad \quad \text{Loop for each } s \in \mathcal{S}:\\
&amp; \quad \quad \quad \quad v \leftarrow V(s) \\
&amp; \quad \quad \quad \quad V(s) \leftarrow \sum_{s^{\prime}, r}
p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
V\left(s^{\prime}\right)\right] \\
&amp; \quad \quad \quad \quad \Delta \leftarrow \max(\Delta, |v-V(s)|)
\\
&amp; \quad \quad \text{until } \Delta &lt; \theta \text{ (a small
positive number determining the accuracy of estimation)}\\
&amp; \\
&amp;3. \quad \text{Policy Improvement} \\
&amp; \quad \quad policy\text{-}stable\leftarrow true \\
&amp; \quad \quad \text{Loop for each } s \in \mathcal{S}:\\
&amp; \quad \quad \quad \quad old\text{-}action\leftarrow \pi(s) \\
&amp; \quad \quad \quad \quad \pi(s) \leftarrow
\operatorname{argmax}_{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid
s, a\right)\left[r+\gamma V\left(s^{\prime}\right)\right] \\
&amp; \quad \quad \quad \quad \text{If } old\text{-}action \neq
\pi\text{,then }policy\text{-}stable\leftarrow false \\
&amp; \quad \quad \text{If } policy\text{-}stable \text{, then stop and
return }V \approx v_{*} \text{ and } \pi\approx {\pi}_{*}\text{; else go
to 2}
\end{align*}
\]</span></p>
</div>
<p>注意到状态Q值 <span class="math inline">\(q_{\pi}(s, a)\)</span>
会被多处调用，将其封装为单独的函数。</p>
<div>
<p><span class="math display">\[
\begin{aligned}
q_{\pi}(s, a) &amp;=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>Q值函数实现如下： </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action_value</span>(<span class="hljs-params">env: GridWorldEnv, state: State, V: StateValue, gamma=<span class="hljs-number">1.0</span></span>) -&gt; ActionValue:</span></span><br><span class="line">    q = np.zeros(env.nA)</span><br><span class="line">    <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nA):</span><br><span class="line">        <span class="hljs-keyword">for</span> prob, next_state, reward, done <span class="hljs-keyword">in</span> env.P[state][a]:</span><br><span class="line">            q[a] += prob * (reward + gamma * V[next_state])</span><br><span class="line">    <span class="hljs-keyword">return</span> q</span><br></pre></td></tr></tbody></table></figure><p></p>
<p>有了 action_value 和上期的 policy_evaluate，policy iteration
实现完全对应上面的伪代码。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">policy_improvement</span>(<span class="hljs-params">env: GridWorldEnv, policy: Policy, V: StateValue, gamma=<span class="hljs-number">1.0</span></span>) -&gt; <span class="hljs-built_in">bool</span>:</span></span><br><span class="line">    policy_stable = <span class="hljs-literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">        old_action = np.argmax(policy[s])</span><br><span class="line">        Q_s = action_value(env, s, V)</span><br><span class="line">        best_action = np.argmax(Q_s)</span><br><span class="line">        policy[s] = np.eye(env.nA)[best_action]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> old_action != best_action:</span><br><span class="line">            policy_stable = <span class="hljs-literal">False</span></span><br><span class="line">    <span class="hljs-keyword">return</span> policy_stable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">policy_iteration</span>(<span class="hljs-params">env: GridWorldEnv, policy: Policy, gamma=<span class="hljs-number">1.0</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Policy, StateValue]:</span></span><br><span class="line">    <span class="hljs-built_in">iter</span> = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">        V = policy_evaluate(policy, env, gamma)</span><br><span class="line">        policy_stable = policy_improvement(env, policy, V)</span><br><span class="line">        <span class="hljs-built_in">iter</span> += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> policy_stable:</span><br><span class="line">            <span class="hljs-keyword">return</span> policy, V</span><br></pre></td></tr></tbody></table></figure><p></p>
Grid World
例子通过两轮迭代就可以收敛，以下是初始时随机策略的V值和第一次迭代后的V值。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/policy_iter_0.png">
<figcaption>
初始随机策略 V 值
</figcaption>
</figure>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/policy_iter_1.png">
<figcaption>
第一次迭代后的 V 值
</figcaption>
</figure>
<h2 id="值迭代">值迭代</h2>
<p>值迭代（ Value Iteration）的本质是，将policy iteration中的policy
evaluation过程从不断循环到收敛直至小于theta，改成只执行一遍，并直接用最佳Q值更新到状态V值，如此可以不用显示地算出<span class="math inline">\({\pi}\)</span>
而直接在V值上迭代。具体迭代公式如下：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{k+1}(s) &amp; \doteq \max _{a} \mathbb{E}\left[R_{t+1}+\gamma
v_{k}\left(S_{t+1}\right) \mid S_{t}=s, A_{t}=a\right] \\
&amp;=\max_{a}  q_{\pi_k}(s, a) \\
&amp;=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>完整的伪代码为：</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Value Iteration, for estimating } \pi\approx \pi_{*} \\
&amp; \text{Algorithm parameter: a small threshold } \theta &gt; 0
\text{ determining accuracy of estimation} \\
&amp; \text{Initialize } V(s), \text{for all } s \in \mathcal{S}^{+}
\text{, arbitrarily except that } V (terminal) = 0\\
&amp; \\
&amp;1: \text{Loop:}\\
&amp;2: \quad \quad \Delta \leftarrow 0\\
&amp;3: \quad \quad \text{Loop for each } s \in \mathcal{S}:\\
&amp;4: \quad \quad \quad \quad v \leftarrow V(s) \\
&amp;5: \quad \quad \quad \quad V(s) \leftarrow \operatorname{max}_{a}
\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
V\left(s^{\prime}\right)\right] \\
&amp;6: \quad \quad \quad \quad \Delta \leftarrow \max(\Delta, |v-V(s)|)
\\
&amp;7: \text{until } \Delta &lt; \theta \\
&amp; \\
&amp; \text{Output a deterministic policy, }\pi\approx \pi_{*} \text{,
such that} \\
&amp; \quad \quad \pi(s) \leftarrow \operatorname{argmax}_{a}
\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
V\left(s^{\prime}\right)\right]
\end{align*}
\]</span></p>
</div>
<p>代码实现也比较直接，可以复用上面已经实现的 action_value 函数。
</p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">value_iteration</span>(<span class="hljs-params">env:GridWorldEnv, gamma=<span class="hljs-number">1.0</span>, theta=<span class="hljs-number">0.0001</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Policy, StateValue]:</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">        delta = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">            action_values = action_value(env, s, V, gamma=gamma)</span><br><span class="line">            best_action_value = np.<span class="hljs-built_in">max</span>(action_values)</span><br><span class="line">            delta = <span class="hljs-built_in">max</span>(delta, np.<span class="hljs-built_in">abs</span>(best_action_value - V[s]))</span><br><span class="line">            V[s] = best_action_value</span><br><span class="line">        <span class="hljs-keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line"></span><br><span class="line">    policy = np.zeros([env.nS, env.nA])</span><br><span class="line">    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">        action_values = action_value(env, s, V, gamma=gamma)</span><br><span class="line">        best_action = np.argmax(action_values)</span><br><span class="line">        policy[s, best_action] = <span class="hljs-number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> policy, V</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="异步迭代">异步迭代</h2>
<p>在第4.5节中提到了DP迭代方式的改进版：异步方式迭代（Asychronous
Iteration）。这里的异步是指每一轮无需全部扫一遍所有状态，而是根据上一轮变化的状态决定下一轮需要最多计算的状态数，类似于Dijkstra最短路径算法中用
heap
来维护更新节点集合，减少运算量。下面我们通过异步值迭代来演示异步迭代的工作方式。</p>
<p>下图表示状态的变化方向，若上一轮 <span class="math inline">\(V(s)\)</span> 发生更新，那么下一轮就要考虑状态 s
可能会影响到上游状态的集合（
p1，p2），避免下一轮必须遍历所有状态的V值计算。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-2/async_propa.png">
<figcaption>
Async 反向传播
</figcaption>
</figure>
<p>要做到部分更新就必须知道每个状态可能影响到的上游状态集合，上图对应的映射关系可以表示为</p>
<div>
<p><span class="math display">\[
\begin{align*}
s'_1 &amp;\rightarrow \{s\} \\
s'_2 &amp;\rightarrow \{s\} \\
s &amp;\rightarrow \{p_1, p_2\}
\end{align*}
\]</span></p>
</div>
<p>建立映射关系的代码如下，build_reverse_mapping 返回类型为 Dict[State,
Set[State]]。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">build_reverse_mapping</span>(<span class="hljs-params">env:GridWorldEnv</span>) -&gt; <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Set</span>[State]]:</span></span><br><span class="line">    MAX_R, MAX_C = env.shape[<span class="hljs-number">0</span>], env.shape[<span class="hljs-number">1</span>]</span><br><span class="line">    mapping = {s: <span class="hljs-built_in">set</span>() <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, MAX_R * MAX_C)}</span><br><span class="line">    action_delta = {Action.UP: (-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.DOWN: (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.LEFT: (<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), Action.RIGHT: (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)}</span><br><span class="line">    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, MAX_R * MAX_C):</span><br><span class="line">        r = s // MAX_R</span><br><span class="line">        c = s % MAX_R</span><br><span class="line">        <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action):</span><br><span class="line">            neighbor_r = <span class="hljs-built_in">min</span>(MAX_R - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, r + action_delta[a][<span class="hljs-number">0</span>]))</span><br><span class="line">            neighbor_c = <span class="hljs-built_in">min</span>(MAX_C - <span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, c + action_delta[a][<span class="hljs-number">1</span>]))</span><br><span class="line">            s_ = neighbor_r * MAX_R + neighbor_c</span><br><span class="line">            mapping[s_].add(s)</span><br><span class="line">    <span class="hljs-keyword">return</span> mapping</span><br></pre></td></tr></tbody></table></figure>
<p>有了描述状态依赖的映射 dict 后，代码也比较简洁，changed_state_set
变量保存了这轮必须计算的状态集合。新的一轮迭代时，将下一轮需要计算的状态保存到
changed_state_set_ 中，本轮结束后，changed_state_set
更新成changed_state_set_，开始下一轮循环直至没有状态需要更新。
</p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">value_iteration_async</span>(<span class="hljs-params">env:GridWorldEnv, gamma=<span class="hljs-number">1.0</span>, theta=<span class="hljs-number">0.0001</span></span>) -&gt; <span class="hljs-type">Tuple</span>[Policy, StateValue]:</span></span><br><span class="line">    mapping = build_reverse_mapping(env)</span><br><span class="line"></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    changed_state_set = <span class="hljs-built_in">set</span>(s <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS))</span><br><span class="line"></span><br><span class="line">    <span class="hljs-built_in">iter</span> = <span class="hljs-number">0</span></span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(changed_state_set) &gt; <span class="hljs-number">0</span>:</span><br><span class="line">        changed_state_set_ = <span class="hljs-built_in">set</span>()</span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> changed_state_set:</span><br><span class="line">            action_values = action_value(env, s, V, gamma=gamma)</span><br><span class="line">            best_action_value = np.<span class="hljs-built_in">max</span>(action_values)</span><br><span class="line">            v_diff = np.<span class="hljs-built_in">abs</span>(best_action_value - V[s])</span><br><span class="line">            <span class="hljs-keyword">if</span> v_diff &gt; theta:</span><br><span class="line">                changed_state_set_.update(mapping[s])</span><br><span class="line">                V[s] = best_action_value</span><br><span class="line">        changed_state_set = changed_state_set_</span><br><span class="line">        <span class="hljs-built_in">iter</span> += <span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    policy = np.zeros([env.nS, env.nA])</span><br><span class="line">    <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">        action_values = action_value(env, s, V, gamma=gamma)</span><br><span class="line">        best_action = np.argmax(action_values)</span><br><span class="line">        policy[s, best_action] = <span class="hljs-number">1.0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-keyword">return</span> policy, V</span><br></pre></td></tr></tbody></table></figure>
比较值迭代和异步值迭代方法后发现，值迭代用了4次循环，每次涉及所有状态，总计算状态数为
4 x 16 = 64。异步值迭代也用了4次循环，但是总计更新了54个状态。由于Grid
World
的状态数很少，异步值迭代优势并不明显，但是对于状态数众多并且迭代最终集中在少部分状态的环境下，节省的计算量还是很可观的。<p></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/rl-sutton-gridworld-1/" itemprop="url">通过代码学Sutton强化学习1：Grid World OpenAI环境和策略评价算法</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-09-03T18:45:01.000Z" itemprop="datePublished">9月 4 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            19 分钟 读完 (约 2827 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>经典教材Reinforcement Learning: An Introduction
第二版由强化领域权威Richard S. Sutton 和 Andrew G. Barto
完成编写，内容深入浅出，非常适合初学者。在本篇中，引入Grid
World示例，结合强化学习核心概念，并用python代码实现OpenAI
Gym的模拟环境，进一步实现策略评价算法。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/rl_sutton.png">
<figcaption>
</figcaption>
</figure>
<h2 id="grid-world-问题">Grid World 问题</h2>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/grid_world.png">
<figcaption>
</figcaption>
</figure>
<p>第四章例子4.1提出了一个简单的离散空间状态问题：Grid
World，其大致意思是在4x4的网格世界中有14个格子是非终点状态，在这些非终点状态的格子中可以往上下左右四个方向走，直至走到两个终点状态格子，则游戏结束。每走一步，Agent收获reward
-1，表示Agent希望在Grid World中尽早出去。另外，Agent在Grid
World边缘时，无法继续往外只能呆在原地，reward也是-1。</p>
<h2 id="finite-mdp-模型">Finite MDP 模型</h2>
先来回顾一下强化学习的建模基础：有限马尔可夫决策过程（Finite Markov
Decision Process, Finite
MDP）。如下图，强化学习模型将世界抽象成两个实体，强化学习解决目标的主体Agent和其他外部环境。它们之间的交互过程遵从有限马尔可夫决策过程：若Agent在t时间步骤时处于状态
<span class="math inline">\(S_t\)</span>，采取动作 <span class="math inline">\(A_t\)</span>，然后环境根据自身机制，产生Reward
<span class="math inline">\(R_{t+1}\)</span> 并将Agent状态变为 <span class="math inline">\(S_{t+1}\)</span>。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/env_agent.png">
<figcaption>
</figcaption>
</figure>
<p>环境自身机制又称为dynamics，工程上可以看成一个输入(S, A)，输出(S,
R)的方法。由于MDP包含随机过程，某个输入并不能确定唯一输出，而会根据概率分布输出不同的(S,
R)。Finite MDP简化了时间对于模型的影响，因为(S, R)只和(S,
A)有关，不和时间t有关。另外，有限指的是S，A，R的状态数量是有限的。</p>
<p>数学上dynamics可以如下表示</p>
<div>
<p><span class="math display">\[
p\left(s^{\prime}, r \mid s, a\right) \doteq
\operatorname{Pr}\left\{S_{t}=s^{\prime}, R_{t}=r \mid S_{t-1}=s,
A_{t-1}=a\right\}
\]</span></p>
</div>
<p>即是四元组作为输入的概率函数 <span class="math inline">\(p: S \times
R \times S \times A \rightarrow [0, 1]\)</span>。</p>
<p>满足 <span class="math display">\[
\sum_{s^{\prime} \in \mathcal{S}} \sum_{r \in \mathcal{R}}
p\left(s^{\prime}, r \mid s, a\right)=1, \text { for all } s \in
\mathcal{S}, a \in \mathcal{A}(s)
\]</span></p>
以Grid
World为例，当Agent处于编号1的网格时，可以往四个方向走，往任意方向走都只产生一种
S,
R，因为这个简单的游戏是确定性的，不存在某一动作导致stochastic状态。例如，在1号网格往左就到了终点网格（编号0），得到Reward
-1这个规则可以如下表示 <span class="math display">\[
p\left(s^{\prime}=0, r=-1 \mid s=1, a=\text{L}\right) = 1
\]</span> 因此，状态s=1的所有dynamics概率映射为
<div>
<p><span class="math display">\[
\begin{aligned}
p\left(s^{\prime}=0, r=-1 \mid s=1, a=\text{L}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=2, r=-1 \mid s=1, a=\text{R}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=1, r=-1 \mid s=1, a=\text{U}\right) &amp;=&amp; 1 \\
p\left(s^{\prime}=5, r=-1 \mid s=1, a=\text{D}\right) &amp;=&amp; 1
\end{aligned}
\]</span></p>
</div>
<h2 id="强化学习的目的">强化学习的目的</h2>
<p>在给定了问题以及定义了强化学习的模型之后，强化学习的目的当然是通过学习让Agent能够学到最佳策略<span class="math inline">\(\pi_{*}\)</span>，也就是在某个状态下的行动分布，记成
<span class="math inline">\(\pi(a|s)\)</span>。对应在数值上的优化目标是Agent在一系列过程中采取某种策略的reward总和的期望（Expected
Return）。下面公式定义了t步往后的reward总和，其中 <span class="math inline">\(\gamma\)</span> 为discount
factor，用于权衡短期和长期reward对于当前Agent的效用影响。等式最后一步的意义是t步后的reward总和等价于t步所获的立即reward
<span class="math inline">\(R_{t+1}\)</span>，加上t+1步后的reward总和
<span class="math inline">\(\gamma G_{t+1}\)</span>。</p>
<div>
<p><span class="math display">\[
\begin{aligned}
G_{t} &amp; \doteq R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\gamma^{3}
R_{t+4}+\cdots \\
&amp;=R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\gamma^{2}
R_{t+4}+\cdots\right) \\
&amp;=R_{t+1}+\gamma G_{t+1}
\end{aligned}
\]</span></p>
</div>
<p>有了reward总和的定义，评价Agent策略 <span class="math inline">\(\pi\)</span> 就可以定义成Agent在状态 s
时采用此策略的Expected Return。</p>
<p><span class="math display">\[
v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]
\]</span></p>
<p>下面公式推导了 <span class="math inline">\(v_{\pi}(s)\)</span>
数值上和相关状态 <span class="math inline">\(s{\prime}\)</span>
的关系：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{\pi}(s) &amp;\doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s\right]
\\
&amp;=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}
\mid S_{t}=s\right]\\
&amp;=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} \mid S_{t}=s\right]
\\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}} \sum_{r}
p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
\mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]\right] \\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r
\mid s, a\right)\left[r+\gamma v_{\pi}\left(s^{\prime}\right)\right]
\quad \text { for all } s \in \mathcal{S}
\end{aligned}
\]</span></p>
</div>
<p>注意到如果将 <span class="math inline">\(v_{\pi}(s)\)</span>
看成未知数，上式即形成 <span class="math inline">\(\mid \mathcal{S}
\mid\)</span> 个未知变量的方程组，可以在数值上解得各个 <span class="math inline">\(v_{\pi}(s)\)</span>。</p>
书中用Backup Diagram来表示递推关系，下图是<span class="math inline">\(v_{\pi}(s)\)</span>的backup diagram。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_v_pi.png">
<figcaption>
</figcaption>
</figure>
<p>尽管v值可以来衡量策略，但由于<span class="math inline">\(v_{\pi}(s)\)</span> 是Agent在策略<span class="math inline">\(\pi(a|s)\)</span>的Expected
Return，将不同的action拆出来单独计算Expected
Return，这样的做法有时更为直接，这就是著名的Q Learning中的q
值，记成<span class="math inline">\(q_{\pi}(s, a)\)</span> 。</p>
<p><span class="math display">\[
q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid S_{t}=s,
A_{t}=a\right]
\]</span></p>
<p>下面是 $q_{}(s, a) $ 的递推 backup diagram。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_q_pi.png">
<figcaption>
</figcaption>
</figure>
<h2 id="bellman-最佳原则">Bellman 最佳原则</h2>
<p>对于所有状态集合<span class="math inline">\(\mathcal{S}\)</span>，策略<span class="math inline">\({\pi}\)</span>的评价指标 <span class="math inline">\(v_{\pi}(s)\)</span>
是一个向量，本质上是无法相互比较的。但由于存在Bellman
最佳原则（Bellman's principle of
optimality）：在有限状态情况下，一定存在一个或者多个最好的策略 <span class="math inline">\({\pi}_{*}\)</span>，它在所有状态下的v值都是最好的，即
<span class="math inline">\(v_{\pi_{*}}(s) \ge v_{\pi^{\prime}}(s) \text
{ for all } s \in \mathcal{S}\)</span>。</p>
<p>因此，最佳v值定义为最佳策略 <span class="math inline">\({\pi}_{*}\)</span> 对应的 v 值</p>
<p><span class="math display">\[
v_{*}(s) \doteq \max_{\pi} v_{\pi}(s)
\]</span></p>
<p>同理，也存在最佳q值，记为 <span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;\doteq \max_{\pi} q_{\pi}(s,a)
\end{aligned}
\]</span></p>
<p>将 <span class="math inline">\(v_{*}(s)\)</span> 改写成递推形式，称为
Bellman Optimality Equation，推导如下</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{*}(s) &amp;=\max _{a \in \mathcal{A}(s)} q_{\pi_{*}}(s, a) \\
&amp;=\max _{a} \mathbb{E}_{\pi_{*}}\left[G_{t} \mid S_{t}=s,
A_{t}=a\right] \\
&amp;=\max _{a} \mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_{t+1} \mid
S_{t}=s, A_{t}=a\right] \\
&amp;=\max _{a} \mathbb{E}\left[R_{t+1}+\gamma v_{*}\left(S_{t+1}\right)
\mid S_{t}=s, A_{t}=a\right] \\
&amp;=\max _{a} \sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma v_{*}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>直觉上可以理解为状态 s
对应的最佳v值是只采取此状态下的最佳动作后的Expected Return。</p>
<p>最佳q值递归形式的意义为最佳策略下状态s时采取行动 a 的Expected
Return，等于所有可能后续状态 s' 下采取最优行动的Expected
Return的均值。推导如下：</p>
<div>
<p><span class="math display">\[
\begin{aligned}
q_{*}(s, a) &amp;=\mathbb{E}\left[R_{t+1}+\gamma \max _{a^{\prime}}
q_{*}\left(S_{t+1}, a^{\prime}\right) \mid S_{t}=s, A_{t}=a\right] \\
&amp;=\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s,
a\right)\left[r+\gamma \max _{a^{\prime}} q_{*}\left(s^{\prime},
a^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<span class="math inline">\(v_{*}(s), q_{*}(s, a)\)</span> 的backup
diagram 如下图
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/backup_optimal.png">
<figcaption>
</figcaption>
</figure>
<h2 id="grid-world-最佳策略和v值">Grid World 最佳策略和V值</h2>
<p>Grid World 的最佳策略如下：尽可能快的走出去</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/optimal_policy.png">
<figcaption>
Grid World最佳策略
</figcaption>
</figure>
<p>上面的2D图中不同颜色表示不同V值，终点格子的红色表示0，隔着一步的黄色为-1，隔两步的绿色为-2，最远的紫色为-3。下面是立体图示。</p>
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/value_3d.png">
<figcaption>
Grid World最佳策略V值
</figcaption>
</figure>
<h2 id="grid-world-openai-gym-环境">Grid World OpenAI Gym 环境</h2>
<p>下面是OpenAI Gym框架下Grid
World环境的代码实现。本质是在GridWorldEnv构造函数中构建MDP，类型定义如下</p>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MDP = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[Prob, State, Reward, <span class="hljs-built_in">bool</span>]]]]</span><br><span class="line"></span><br><span class="line"><span class="hljs-comment"># P[state][action] = [</span></span><br><span class="line"><span class="hljs-comment">#    (prob1, next_state1, reward1, is_done),</span></span><br><span class="line"><span class="hljs-comment">#    (prob2, next_state2, reward2, is_done), ...]</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Action</span>(<span class="hljs-params">Enum</span>):</span></span><br><span class="line">    UP = <span class="hljs-number">0</span></span><br><span class="line">    DOWN = <span class="hljs-number">1</span></span><br><span class="line">    LEFT = <span class="hljs-number">2</span></span><br><span class="line">    RIGHT = <span class="hljs-number">3</span></span><br><span class="line"></span><br><span class="line">State = <span class="hljs-built_in">int</span></span><br><span class="line">Reward = <span class="hljs-built_in">float</span></span><br><span class="line">Prob = <span class="hljs-built_in">float</span></span><br><span class="line">Policy = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, Prob]]</span><br><span class="line">Value = <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]</span><br><span class="line">StateSet = <span class="hljs-type">Set</span>[<span class="hljs-built_in">int</span>]</span><br><span class="line">NonTerminalStateSet = <span class="hljs-type">Set</span>[<span class="hljs-built_in">int</span>]</span><br><span class="line">MDP = <span class="hljs-type">Dict</span>[State, <span class="hljs-type">Dict</span>[Action, <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[Prob, State, Reward, <span class="hljs-built_in">bool</span>]]]]</span><br><span class="line"><span class="hljs-comment"># P[s][a] = [(prob, next_state, reward, is_done), ...]</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GridWorldEnv</span>(<span class="hljs-params">discrete.DiscreteEnv</span>):</span></span><br><span class="line">    <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">    Grid World environment described in Sutton and Barto Reinforcement Learning 2nd, chapter 4.</span></span><br><span class="line"><span class="hljs-string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, shape=[<span class="hljs-number">4</span>,<span class="hljs-number">4</span>]</span>):</span></span><br><span class="line">        self.shape = shape</span><br><span class="line">        nS = np.prod(shape)</span><br><span class="line">        nA = <span class="hljs-built_in">len</span>(<span class="hljs-built_in">list</span>(Action))</span><br><span class="line">        MAX_R = shape[<span class="hljs-number">0</span>]</span><br><span class="line">        MAX_C = shape[<span class="hljs-number">1</span>]</span><br><span class="line">        self.grid = np.arange(nS).reshape(shape)</span><br><span class="line">        isd = np.ones(nS) / nS</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># P[s][a] = [(prob, next_state, reward, is_done), ...]</span></span><br><span class="line">        P: MDP = {}</span><br><span class="line">        action_delta = {Action.UP: (-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.DOWN: (<span class="hljs-number">1</span>, <span class="hljs-number">0</span>), Action.LEFT: (<span class="hljs-number">0</span>, -<span class="hljs-number">1</span>), Action.RIGHT: (<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)}</span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, MAX_R * MAX_C):</span><br><span class="line">            P[s] = {a.value : [] <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action)}</span><br><span class="line">            is_terminal = self.is_terminal(s)</span><br><span class="line">            <span class="hljs-keyword">if</span> is_terminal:</span><br><span class="line">                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action):</span><br><span class="line">                    P[s][a.value] = [(<span class="hljs-number">1.0</span>, s, <span class="hljs-number">0</span>, <span class="hljs-literal">True</span>)]</span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                r = s // MAX_R</span><br><span class="line">                c = s % MAX_R</span><br><span class="line">                <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> <span class="hljs-built_in">list</span>(Action):</span><br><span class="line">                    neighbor_r = <span class="hljs-built_in">min</span>(MAX_R-<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, r + action_delta[a][<span class="hljs-number">0</span>]))</span><br><span class="line">                    neighbor_c = <span class="hljs-built_in">min</span>(MAX_C-<span class="hljs-number">1</span>, <span class="hljs-built_in">max</span>(<span class="hljs-number">0</span>, c + action_delta[a][<span class="hljs-number">1</span>]))</span><br><span class="line">                    s_ = neighbor_r * MAX_R + neighbor_c</span><br><span class="line">                    P[s][a.value] = [(<span class="hljs-number">1.0</span>, s_, -<span class="hljs-number">1</span>, <span class="hljs-literal">False</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="hljs-built_in">super</span>(GridWorldEnv, self).__init__(nS, nA, P, isd)</span><br></pre></td></tr></tbody></table></figure>
<h2 id="策略评估policy-evaluation">策略评估（Policy Evaluation）</h2>
<p>策略评估需要解决在给定环境dynamics和Agent策略 <span class="math inline">\(\pi\)</span>下，计算策略的v值 <span class="math inline">\(v_{\pi}\)</span>。由于所有数量关系都已知，可以通过解方程组的方式求得，但通常会通过数值迭代的方式来计算，即通过一系列
<span class="math inline">\(v_{0}, v_{1}, ..., v_{k}\)</span> 收敛至
<span class="math inline">\(v_{\pi}\)</span>。如下迭代方式已经得到证明，当
<span class="math inline">\(k \rightarrow \infty\)</span> 一定收敛至
<span class="math inline">\(v_{\pi}\)</span>。</p>
<div>
<p><span class="math display">\[
\begin{aligned}
v_{k+1}(s) &amp; \doteq \mathbb{E}_{\pi}\left[R_{t+1}+\gamma
v_{k}\left(S_{t+1}\right) \mid S_{t}=s\right] \\
&amp;=\sum_{a} \pi(a \mid s) \sum_{s^{\prime}, r} p\left(s^{\prime}, r
\mid s, a\right)\left[r+\gamma v_{k}\left(s^{\prime}\right)\right]
\end{aligned}
\]</span></p>
</div>
<p>书中具体伪代码如下</p>
<div>
<p><span class="math display">\[
\begin{align*}
&amp;\textbf{Iterative Policy Evaluation, for estimating } V\approx
v_{\pi} \\
&amp; \text{Input } {\pi}, \text{the policy to be evaluated} \\
&amp; \text{Algorithm parameter: a small threshold } \theta &gt; 0
\text{ determining accuracy of estimation} \\
&amp; \text{Initialize } V(s), \text{for all } s \in \mathcal{S}^{+}
\text{, arbitrarily except that } V (terminal) = 0\\
&amp; \\
&amp;1: \text{Loop:}\\
&amp;2: \quad \quad \Delta \leftarrow 0\\
&amp;3: \quad \quad \text{Loop for each } s \in \mathcal{S}:\\
&amp;4: \quad \quad \quad \quad v \leftarrow V(s) \\
&amp;5: \quad \quad \quad \quad V(s) \leftarrow \sum_{a} \pi(a \mid s)
\sum_{s^{\prime}, r} p\left(s^{\prime}, r \mid s, a\right)\left[r+\gamma
V\left(s^{\prime}\right)\right] \\
&amp;6: \quad \quad \quad \quad \Delta \leftarrow \max(\Delta, |v-V(s)|)
\\
&amp;7: \text{until } \Delta &lt; \theta
\end{align*}
\]</span></p>
</div>
<p>下面是python
代码实现，注意这里单run迭代时，新的v值直接覆盖数组里的旧v值，这种做法在书中被证明不仅有效，甚至更为高效。这种做法称为原地（in
place）更新。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">policy_evaluate</span>(<span class="hljs-params">policy: Policy, env: GridWorldEnv, gamma=<span class="hljs-number">1.0</span>, theta=<span class="hljs-number">0.0001</span></span>):</span></span><br><span class="line">    V = np.zeros(env.nS)</span><br><span class="line">    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">        delta = <span class="hljs-number">0</span></span><br><span class="line">        <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(env.nS):</span><br><span class="line">            v = <span class="hljs-number">0</span></span><br><span class="line">            <span class="hljs-keyword">for</span> a, action_prob <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(policy[s]):</span><br><span class="line">                <span class="hljs-keyword">for</span> prob, next_state, reward, done <span class="hljs-keyword">in</span> env.P[s][a]:</span><br><span class="line">                    v += action_prob * prob * (reward + gamma * V[next_state])</span><br><span class="line">            delta = <span class="hljs-built_in">max</span>(delta, np.<span class="hljs-built_in">abs</span>(v - V[s]))</span><br><span class="line">            V[s] = v</span><br><span class="line">        <span class="hljs-keyword">if</span> delta &lt; theta:</span><br><span class="line">            <span class="hljs-keyword">break</span></span><br><span class="line">    <span class="hljs-keyword">return</span> np.array(V)</span><br></pre></td></tr></tbody></table></figure>
<p>输入策略为随机选择方向，运行上面的policy_evaluate最终多轮收敛后的V值输出为</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[  <span class="hljs-number">0.</span>         -<span class="hljs-number">13.99931242</span> -<span class="hljs-number">19.99901152</span> -<span class="hljs-number">21.99891199</span>]</span><br><span class="line"> [-<span class="hljs-number">13.99931242</span> -<span class="hljs-number">17.99915625</span> -<span class="hljs-number">19.99908389</span> -<span class="hljs-number">19.99909436</span>]</span><br><span class="line"> [-<span class="hljs-number">19.99901152</span> -<span class="hljs-number">19.99908389</span> -<span class="hljs-number">17.99922697</span> -<span class="hljs-number">13.99942284</span>]</span><br><span class="line"> [-<span class="hljs-number">21.99891199</span> -<span class="hljs-number">19.99909436</span> -<span class="hljs-number">13.99942284</span>   <span class="hljs-number">0.</span>        ]]</span><br></pre></td></tr></tbody></table></figure>
在3D V值图中可以发现，由于是随机选择方向的策略，
Agent在每个格子的V值绝对数值要比最佳V值大，意味着随机策略下Agent在Grid
World会得到更多的负reward。
<figure>
<img src="/zh/2020/rl-sutton-gridworld-1/random_policy_v.png">
<figcaption>
Grid World随机策略V值
</figcaption>
</figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/" itemprop="url">组合游戏系列3: 井字棋、五子棋的OpenAI Gym GUI环境</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-07-23T18:45:01.000Z" itemprop="datePublished">7月 24 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            18 分钟 读完 (约 2749 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>继上一篇完成了井字棋（N子棋）的minimax
最佳策略后，我们基于Pygame来创造一个图形游戏环境，可供人机和机器对弈，为后续模拟AlphaGo的自我强化学习算法做环境准备。OpenAI
Gym 在强化学习领域是事实标准，我们最终封装成OpenAI
Gym的接口。本篇所有代码都在<a target="_blank" rel="noopener" href="https://github.com/MyEncyclopedia/ConnectNGym">github.com/MyEncyclopedia/ConnectNGym</a>。</p>
<ul>
<li><p><a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/zh/combinatorial-game-1-minimax.md">第一篇:
Leetcode中的Minimax 和 Alpha Beta剪枝</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/zh/combinatorial-game-2-tictactoe.md">第二篇:
井字棋Leetcode系列题解和Minimax最佳策略实现</a></p></li>
<li><p><strong><a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/zh/combinatorial-game-3-openai-gym-pygame.md">第三篇:
井字棋、五子棋的OpenAI Gym GUI环境</a></strong></p></li>
<li><p><a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/zh/combinatorial-game-4-alphago-zero-theory/index.md">第四篇:
AlphaGo Zero 强化学习算法原理深度分析</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-3-openai-gym-pygame/zh/combinatorial-game-5-alphago-zero-connect-n/index.md">第五篇:
井字棋、五子棋AlphaGo Zero 算法实战</a></p></li>
</ul>
<h2 id="井字棋五子棋-pygame-实现">井字棋、五子棋 Pygame 实现</h2>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/pygame.gif">
<figcaption>
Pygame 井字棋玩家对弈效果
</figcaption>
</figure>
<p>Python
上有Tkinter，PyQt等跨平台GUI类库，主要用于桌面程序编程，但此类库容量较大，编程也相对麻烦。Pygame具有代码少，开发快的优势，比较适合快速开发五子棋这类桌面小游戏。
### Pygame 极简入门</p>
<p>与所有的GUI开发相同，Pygame也是基于事件的单线程编程模型。下面的例子包含了显示一个最简单GUI窗口，操作系统产生事件并发送到Pygame窗口，while
True
控制了python主线程永远轮询事件。我们在这里仅仅判断了当前是否是关闭应用程序事件，如果是则退出进程。此外，clock
用于控制FPS。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> sys</span><br><span class="line"><span class="hljs-keyword">import</span> pygame</span><br><span class="line">pygame.init()</span><br><span class="line">display = pygame.display.set_mode((<span class="hljs-number">800</span>,<span class="hljs-number">600</span>))</span><br><span class="line">clock = pygame.time.Clock()</span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">	<span class="hljs-keyword">for</span> event <span class="hljs-keyword">in</span> pygame.event.get():</span><br><span class="line">		<span class="hljs-keyword">if</span> event.<span class="hljs-built_in">type</span> == pygame.QUIT:</span><br><span class="line">			sys.exit(<span class="hljs-number">0</span>)</span><br><span class="line">		<span class="hljs-keyword">else</span>:</span><br><span class="line">			pygame.display.update()</span><br><span class="line">			clock.tick(<span class="hljs-number">1</span>)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="pygameboard-主体代码">PyGameBoard 主体代码</h3>
PyGameBoard类封装了Pygame实现游戏交互和显示的逻辑。上一篇中，我们完成了ConnectNGame逻辑，这里PyGameBoard需要在初始化时，指定传入ConnectNGame
实例（见下图），支持通过API
方式改变其状态，也支持GUI交互方式等待人类玩家的输入。next_user_input(self)实现了等待人类玩家输入的逻辑，本质上是循环检查GUI事件直到有合法的落子产生。
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/diagram-pygameboard.svg">
<figcaption>
PyGameBoard Class Diagram
</figcaption>
</figure>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PyGameBoard</span>:</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, connectNGame: ConnectNGame</span>):</span></span><br><span class="line">		self.connectNGame = connectNGame</span><br><span class="line">		pygame.init()</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">next_user_input</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]:</span></span><br><span class="line">		self.action = <span class="hljs-literal">None</span></span><br><span class="line">		<span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> self.action:</span><br><span class="line">			self.check_event()</span><br><span class="line">			self._render()</span><br><span class="line">			self.clock.tick(<span class="hljs-number">60</span>)</span><br><span class="line">		<span class="hljs-keyword">return</span> self.action</span><br><span class="line">  </span><br><span class="line">  <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">move</span>(<span class="hljs-params">self, r: <span class="hljs-built_in">int</span>, c: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">		<span class="hljs-keyword">return</span> self.connectNGame.move(r, c)</span><br><span class="line">  </span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:</span><br><span class="line">	connectNGame = ConnectNGame()</span><br><span class="line">	pygameBoard = PyGameBoard(connectNGame)</span><br><span class="line">	<span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> pygameBoard.isGameOver():</span><br><span class="line">		pos = pygameBoard.next_user_input()</span><br><span class="line">		pygameBoard.move(*pos)</span><br><span class="line"></span><br><span class="line">	pygame.quit()</span><br></pre></td></tr></tbody></table></figure>
<p>check_event
较之极简版本增加了处理用户输入事件，这里我们仅支持人类玩家鼠标输入。方法_handle_user_input
将鼠标点击事件转换成棋盘行列值，并判断点击位置是否合法，合法则返回落子位置，类型为Tuple[int,
int]，例如(0, 0)表示棋盘最左上角位置。 </p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">check_event</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">	<span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> pygame.event.get():</span><br><span class="line">		<span class="hljs-keyword">if</span> e.<span class="hljs-built_in">type</span> == pygame.QUIT:</span><br><span class="line">			pygame.quit()</span><br><span class="line">			sys.exit(<span class="hljs-number">0</span>)</span><br><span class="line">		<span class="hljs-keyword">elif</span> e.<span class="hljs-built_in">type</span> == pygame.MOUSEBUTTONDOWN:</span><br><span class="line">			self._handle_user_input(e)</span><br><span class="line">    </span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_handle_user_input</span>(<span class="hljs-params">self, e: Event</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]:</span></span><br><span class="line">	origin_x = self.start_x - self.edge_size</span><br><span class="line">	origin_y = self.start_y - self.edge_size</span><br><span class="line">	size = (self.board_size - <span class="hljs-number">1</span>) * self.grid_size + self.edge_size * <span class="hljs-number">2</span></span><br><span class="line">	pos = e.pos</span><br><span class="line">	<span class="hljs-keyword">if</span> origin_x &lt;= pos[<span class="hljs-number">0</span>] &lt;= origin_x + size <span class="hljs-keyword">and</span> origin_y &lt;= pos[<span class="hljs-number">1</span>] &lt;= origin_y + size:</span><br><span class="line">		<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.connectNGame.gameOver:</span><br><span class="line">			x = pos[<span class="hljs-number">0</span>] - origin_x</span><br><span class="line">			y = pos[<span class="hljs-number">1</span>] - origin_y</span><br><span class="line">			r = <span class="hljs-built_in">int</span>(y // self.grid_size)</span><br><span class="line">			c = <span class="hljs-built_in">int</span>(x // self.grid_size)</span><br><span class="line">			valid = self.connectNGame.checkAction(r, c)</span><br><span class="line">			<span class="hljs-keyword">if</span> valid:</span><br><span class="line">				self.action = (r, c)</span><br><span class="line">				<span class="hljs-keyword">return</span> self.action</span><br></pre></td></tr></tbody></table></figure><p></p>
<h2 id="openai-gym-接口规范">OpenAI Gym 接口规范</h2>
<p>OpenAI
Gym规范了Agent和环境（Env）之间的互动，核心抽象接口类是gym.Env，自定义的游戏环境需要继承Env，并实现
reset、step和render方法。下面我们看一下如何具体实现ConnectNGym的这几个方法：</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConnectNGym</span>(<span class="hljs-params">gym.Env</span>):</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>) -&gt; ConnectNGame:</span></span><br><span class="line">		<span class="hljs-string">"""Resets the state of the environment and returns an initial observation.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Returns:</span></span><br><span class="line"><span class="hljs-string">			observation (object): the initial observation.</span></span><br><span class="line"><span class="hljs-string">		"""</span></span><br><span class="line">		<span class="hljs-keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Tuple</span>[ConnectNGame, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-literal">None</span>]:</span></span><br><span class="line">		<span class="hljs-string">"""Run one timestep of the environment's dynamics. When end of</span></span><br><span class="line"><span class="hljs-string">		episode is reached, you are responsible for calling `reset()`</span></span><br><span class="line"><span class="hljs-string">		to reset this environment's state.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Accepts an action and returns a tuple (observation, reward, done, info).</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Args:</span></span><br><span class="line"><span class="hljs-string">			action (object): an action provided by the agent</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Returns:</span></span><br><span class="line"><span class="hljs-string">			observation (object): agent's observation of the current environment</span></span><br><span class="line"><span class="hljs-string">			reward (float) : amount of reward returned after previous action</span></span><br><span class="line"><span class="hljs-string">			done (bool): whether the episode has ended, in which case further step() calls will return undefined results</span></span><br><span class="line"><span class="hljs-string">			info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</span></span><br><span class="line"><span class="hljs-string">		"""</span></span><br><span class="line">		<span class="hljs-keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">'human'</span></span>):</span></span><br><span class="line">		<span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">		Renders the environment.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		The set of supported modes varies per environment. (And some</span></span><br><span class="line"><span class="hljs-string">		environments do not support rendering at all.) By convention,</span></span><br><span class="line"><span class="hljs-string">		if mode is:</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		- human: render to the current display or terminal and</span></span><br><span class="line"><span class="hljs-string">			return nothing. Usually for human consumption.</span></span><br><span class="line"><span class="hljs-string">		- rgb_array: Return an numpy.ndarray with shape (x, y, 3),</span></span><br><span class="line"><span class="hljs-string">			representing RGB values for an x-by-y pixel image, suitable</span></span><br><span class="line"><span class="hljs-string">			for turning into a video.</span></span><br><span class="line"><span class="hljs-string">		- ansi: Return a string (str) or StringIO.StringIO containing a</span></span><br><span class="line"><span class="hljs-string">			terminal-style text representation. The text can include newlines</span></span><br><span class="line"><span class="hljs-string">			and ANSI escape sequences (e.g. for colors).</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Note:</span></span><br><span class="line"><span class="hljs-string">		Make sure that your class's metadata 'render.modes' key includes</span></span><br><span class="line"><span class="hljs-string">		the list of supported modes. It's recommended to call super()</span></span><br><span class="line"><span class="hljs-string">		in implementations to use the functionality of this method.</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">		Args:</span></span><br><span class="line"><span class="hljs-string">			mode (str): the mode to render with</span></span><br><span class="line"><span class="hljs-string">		"""</span></span><br><span class="line">		<span class="hljs-keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="reset-方法">reset 方法</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>) -&gt; ConnectNGame</span></span><br></pre></td></tr></tbody></table></figure>
<p>重置环境状态，并返回给Agent重置后环境下观察到的状态。ConnectNGym内部维护了ConnectNGame实例作为自身状态，每个agent落子后会更新这个实例。由于棋类游戏对于玩家来说是完全信息的，我们直接返回ConnectNGame的deepcopy。</p>
<h3 id="step-方法">step 方法</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Tuple</span>[ConnectNGame, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-literal">None</span>]</span></span><br></pre></td></tr></tbody></table></figure>
<p>Agent 选择了某一action后，由环境来执行这个action并返回4个值：1.
执行后的环境Agent观察到的状态；2.
环境执行了这个action回馈给agent的reward；3. 环境是否结束；4.
其余信息。</p>
<p>step方法是最核心的接口，因此举例来说明ConnectNGym中的输入和输出：</p>
初始状态
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/s0.png">
<figcaption>
状态 ((0, 0, 0), (0, 0, 0), (0, 0, 0))
</figcaption>
</figure>
<p>Agent A 选择action = (0, 0)，执行ConnectNGym.step 后返回值：status =
((1, 0, 0), (0, 0, 0), (0, 0, 0))，reward = 0，game_end = False</p>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/s1.png">
<figcaption>
状态 ((1, 0, 0), (0, 0, 0), (0, 0, 0))
</figcaption>
</figure>
<p>Agent B 选择action = (1, 1)，执行ConnectNGym.step 后返回值：status =
((1, 0, 0), (0, -1, 0), (0, 0, 0))，reward = 0，game_end = False</p>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/s2.png">
<figcaption>
状态 ((1, 0, 0), (0, -1, 0), (0, 0, 0))
</figcaption>
</figure>
重复此过程直至游戏结束，下面是5步后游戏可能达到的最终状态
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/s5.png">
<figcaption>
终结状态 ((1, 1, 1), (-1, -1, 0), (0, 0, 0))
</figcaption>
</figure>
<p>此时step的返回值为：status = ((1, 1, 1), (-1, -1, 0), (0, 0,
0))，reward = 1，game_end = True</p>
<h3 id="render-方法">render 方法</h3>
<figure class="highlight python hljs"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">'human'</span></span>)</span></span><br></pre></td></tr></tbody></table></figure>
<p>展现环境，通过mode区分是否是人类玩家。</p>
<h3 id="connectngym-代码">ConnectNGym 代码</h3>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConnectNGym</span>(<span class="hljs-params">gym.Env</span>):</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, pygameBoard: PyGameBoard, isGUI=<span class="hljs-literal">True</span>, displaySec=<span class="hljs-number">2</span></span>):</span></span><br><span class="line">		self.pygameBoard = pygameBoard</span><br><span class="line">		self.isGUI = isGUI</span><br><span class="line">		self.displaySec = displaySec</span><br><span class="line">		self.action_space = spaces.Discrete(pygameBoard.board_size * pygameBoard.board_size)</span><br><span class="line">		self.observation_space = spaces.Discrete(pygameBoard.board_size * pygameBoard.board_size)</span><br><span class="line">		self.seed()</span><br><span class="line">		self.reset()</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>) -&gt; ConnectNGame:</span></span><br><span class="line">		self.pygameBoard.connectNGame.reset()</span><br><span class="line">		<span class="hljs-keyword">return</span> copy.deepcopy(self.pygameBoard.connectNGame)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action: <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Tuple</span>[ConnectNGame, <span class="hljs-built_in">int</span>, <span class="hljs-built_in">bool</span>, <span class="hljs-literal">None</span>]:</span></span><br><span class="line">		<span class="hljs-comment"># assert self.action_space.contains(action)</span></span><br><span class="line"></span><br><span class="line">		r, c = action</span><br><span class="line">		reward = REWARD_NONE</span><br><span class="line">		result = self.pygameBoard.move(r, c)</span><br><span class="line">		<span class="hljs-keyword">if</span> self.pygameBoard.isGameOver():</span><br><span class="line">			reward = result</span><br><span class="line"></span><br><span class="line">		<span class="hljs-keyword">return</span> copy.deepcopy(self.pygameBoard.connectNGame), reward, <span class="hljs-keyword">not</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">render</span>(<span class="hljs-params">self, mode=<span class="hljs-string">'human'</span></span>):</span></span><br><span class="line">		<span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.isGUI:</span><br><span class="line">			self.pygameBoard.connectNGame.drawText()</span><br><span class="line">			time.sleep(self.displaySec)</span><br><span class="line">		<span class="hljs-keyword">else</span>:</span><br><span class="line">			self.pygameBoard.display(sec=self.displaySec)</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_available_actions</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">		<span class="hljs-keyword">return</span> self.pygameBoard.getAvailablePositions()</span><br></pre></td></tr></tbody></table></figure>
<h2 id="井字棋n子棋minimax策略玩家">井字棋（N子棋）Minimax策略玩家</h2>
<p>图中当k=3,m=n=3即井字棋游戏中，两个minimax策略玩家的对弈效果，游戏结局符合已知的结论：井字棋的解是先手被对方逼平。</p>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/self_play.gif">
<figcaption>
Minimax策略AI对弈
</figcaption>
</figure>
<h3 id="镜像游戏状态的dp处理">镜像游戏状态的DP处理</h3>
<p>上一篇中，我们确认了井字棋的总状态数是5478。当k=3,
m=n=4时是6035992，k=4,
m=n=4时是9722011，总的来说游戏状态数是以指数级增长的。上一版minimax
DP策略还有改善的空间，第一种是旋转格局的处理。对于任意一种棋盘格局可以得到90度旋转后的另外三种格局，它们的最佳结局是一致的。因此，我们在递归过程中解得某一棋盘格局后，将其另外三种旋转后格局的解也一起缓存起来。例如：</p>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/pos.png">
<figcaption>
游戏状态1
</figcaption>
</figure>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/pos_others.png">
<figcaption>
旋转后的三种游戏状态
</figcaption>
</figure>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">similarStatus</span>(<span class="hljs-params">self, status: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]]:</span></span><br><span class="line">	ret = []</span><br><span class="line">	rotatedS = status</span><br><span class="line">	<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):</span><br><span class="line">		rotatedS = self.rotate(rotatedS)</span><br><span class="line">		ret.append(rotatedS)</span><br><span class="line">	<span class="hljs-keyword">return</span> ret</span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rotate</span>(<span class="hljs-params">self, status: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]:</span></span><br><span class="line">	N = <span class="hljs-built_in">len</span>(status)</span><br><span class="line">	board = [[ConnectNGame.AVAILABLE] * N <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)]</span><br><span class="line"></span><br><span class="line">	<span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):</span><br><span class="line">		<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N):</span><br><span class="line">			board[c][N - <span class="hljs-number">1</span> - r] = status[r][c]</span><br><span class="line"></span><br><span class="line">	<span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>([<span class="hljs-built_in">tuple</span>(board[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)])</span><br></pre></td></tr></tbody></table></figure>
<h3 id="minimax-策略预计算">Minimax 策略预计算</h3>
<p>之前我们对每个棋局去计算最佳的下一步，并在此过程中做了剪枝，即当已经找到当前玩家必胜落子时直接返回。这对于单一局面的计算是较优的，但是AI
Agent
需要在每一步都重复这个过程，当棋盘大小&gt;3时运算非常耗时，因此我们来做第二种优化。初始空棋盘时使用Minimax来保证遍历所有状态，缓存所有棋局的最佳结果。对于AI
Agent面临的每个棋局只需查找此棋局下所有的可能落子位置，并返回最佳决定，这样大大减少了每次棋局下重复的minimax递归计算。相关代码如下。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PlannedMinimaxStrategy</span>(<span class="hljs-params">Strategy</span>):</span></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, game: ConnectNGame</span>):</span></span><br><span class="line">		<span class="hljs-built_in">super</span>().__init__()</span><br><span class="line">		self.game = copy.deepcopy(game)</span><br><span class="line">		self.dpMap = {}  <span class="hljs-comment"># game_status =&gt; result, move</span></span><br><span class="line">		self.result = self.minimax(game.getStatus())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">		game = copy.deepcopy(game)</span><br><span class="line"></span><br><span class="line">		player = game.currentPlayer</span><br><span class="line">		bestResult = player * -<span class="hljs-number">1</span>  <span class="hljs-comment"># assume opponent win as worst result</span></span><br><span class="line">		bestMove = <span class="hljs-literal">None</span></span><br><span class="line">		<span class="hljs-keyword">for</span> move <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">			game.move(*move)</span><br><span class="line">			status = game.getStatus()</span><br><span class="line">			game.undo()</span><br><span class="line"></span><br><span class="line">			result = self.dpMap[status]</span><br><span class="line"></span><br><span class="line">			<span class="hljs-keyword">if</span> player == ConnectNGame.PLAYER_A:</span><br><span class="line">				bestResult = <span class="hljs-built_in">max</span>(bestResult, result)</span><br><span class="line">			<span class="hljs-keyword">else</span>:</span><br><span class="line">				bestResult = <span class="hljs-built_in">min</span>(bestResult, result)</span><br><span class="line">			<span class="hljs-comment"># update bestMove if any improvement</span></span><br><span class="line">			bestMove = move <span class="hljs-keyword">if</span> bestResult == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">			<span class="hljs-built_in">print</span>(<span class="hljs-string">f'move <span class="hljs-subst">{move}</span> =&gt; <span class="hljs-subst">{result}</span>'</span>)</span><br><span class="line"></span><br><span class="line">		<span class="hljs-keyword">return</span> bestResult, bestMove</span><br></pre></td></tr></tbody></table></figure>
<h2 id="agent-类和对弈逻辑">Agent 类和对弈逻辑</h2>
<p>Agent 类的抽象并不是 OpenAI
Gym的规范，出于代码扩展性，我们也封装了Agent基类及其子类，包括AI玩家和人类玩家。BaseAgent需要子类实现
act方法，默认实现为随机决定。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BaseAgent</span>(<span class="hljs-params"><span class="hljs-built_in">object</span></span>):</span></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">		<span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, game: PyGameBoard, available_actions</span>):</span></span><br><span class="line">		<span class="hljs-keyword">return</span> random.choice(available_actions)</span><br></pre></td></tr></tbody></table></figure>
<p>AIAgent 实现act并代理给 strategy 的action方法。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AIAgent</span>(<span class="hljs-params">BaseAgent</span>):</span></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, strategy: Strategy</span>):</span></span><br><span class="line">		self.strategy = strategy</span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, game: PyGameBoard, available_actions</span>):</span></span><br><span class="line">		result, move = self.strategy.action(game.connectNGame)</span><br><span class="line">		<span class="hljs-keyword">assert</span> move <span class="hljs-keyword">in</span> available_actions</span><br><span class="line">		<span class="hljs-keyword">return</span> move</span><br></pre></td></tr></tbody></table></figure>
<p>HumanAgent 实现act并代理给 PyGameBoard 的next_user_input方法。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HumanAgent</span>(<span class="hljs-params">BaseAgent</span>):</span></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">		<span class="hljs-keyword">pass</span></span><br><span class="line"></span><br><span class="line">	<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">act</span>(<span class="hljs-params">self, game: PyGameBoard, available_actions</span>):</span></span><br><span class="line">		<span class="hljs-keyword">return</span> game.next_user_input()</span><br></pre></td></tr></tbody></table></figure>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/diagram-agent.svg">
<figcaption>
Agent Class Diagram
</figcaption>
</figure>
<p>下面代码展示如何将Agent，ConnectNGym，PyGameBoard
等所有上述类串联起来，完成人人对弈，人机对弈。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play_ai_vs_ai</span>(<span class="hljs-params">env: ConnectNGym</span>):</span></span><br><span class="line">	plannedMinimaxAgent = AIAgent(PlannedMinimaxStrategy(env.pygameBoard.connectNGame))</span><br><span class="line">	play(env, plannedMinimaxAgent, plannedMinimaxAgent)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">play</span>(<span class="hljs-params">env: ConnectNGym, agent1: BaseAgent, agent2: BaseAgent</span>):</span></span><br><span class="line">	agents = [agent1, agent2]</span><br><span class="line"></span><br><span class="line">	<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">		env.reset()</span><br><span class="line">		done = <span class="hljs-literal">False</span></span><br><span class="line">		agent_id = -<span class="hljs-number">1</span></span><br><span class="line">		<span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:</span><br><span class="line">			agent_id = (agent_id + <span class="hljs-number">1</span>) % <span class="hljs-number">2</span></span><br><span class="line">			available_actions = env.get_available_actions()</span><br><span class="line">			agent = agents[agent_id]</span><br><span class="line">			action = agent.act(pygameBoard, available_actions)</span><br><span class="line">			_, reward, done, info = env.step(action)</span><br><span class="line">			env.render(<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line">			<span class="hljs-keyword">if</span> done:</span><br><span class="line">				<span class="hljs-built_in">print</span>(<span class="hljs-string">f'result=<span class="hljs-subst">{reward}</span>'</span>)</span><br><span class="line">				time.sleep(<span class="hljs-number">3</span>)</span><br><span class="line">				<span class="hljs-keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:</span><br><span class="line">	pygameBoard = PyGameBoard(connectNGame=ConnectNGame(board_size=<span class="hljs-number">3</span>, N=<span class="hljs-number">3</span>))</span><br><span class="line">	env = ConnectNGym(pygameBoard)</span><br><span class="line">	env.render(<span class="hljs-literal">True</span>)</span><br><span class="line"></span><br><span class="line">	play_ai_vs_ai(env)</span><br></pre></td></tr></tbody></table></figure>
<figure>
<img src="/zh/2020/combinatorial-game-3-openai-gym-pygame/diagram-overview.svg">
<figcaption>
Class Diagram 总览
</figcaption>
</figure>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
        <article class="article content gallery" itemscope itemprop="blogPost">
    <aside id='article-toc' role="navigation" class='fixed'>
        <div id='article-toc-inner'>
            
        </div>
        
        </aside>
                <style>
            #article-toc-inner:after,#article-toc-inner:before,.inner:after,.inner:before {
                content: "";
                display: table
            }
            
            #article-toc-inner:after,.inner:after {
                clear: both
            }
            @media screen {
                #article-toc-inner,.inner {
                    padding: 0 20px
                }
            }
            #article-toc {
                display: none;
                float: left;
                width: 25%;
                margin-left: -220px;
                opacity: .8
            }
            @media screen and (min-width:769px) {
                #article-toc {
                    display: block
                }
            }
 
           #article-toc.fixed {
                position: absolute;
                top: 0;
                bottom: 0;
                left: 10 px;
                padding-top: 55px;
            }
            .fixed #article-toc-inner {
                position: fixed;
                width: 220px;
                top: 0;
                bottom: 0;
                padding-top: 55px;
            }
            </style>
    <h1 class="article-title is-size-3 is-size-4-mobile" itemprop="name">
        
            <a href="/zh/2020/combinatorial-game-2-tictactoe/" itemprop="url">组合游戏系列2: 井字棋Leetcode系列题解和Minimax最佳策略实现</a>
        
    </h1>
    <div class="article-meta columns is-variable is-1 is-multiline is-mobile is-size-7-mobile">
        <span class="column is-narrow">
            
                <time datetime="2020-07-11T18:45:01.000Z" itemprop="datePublished">7月 12 2020</time>
            
        </span>
        
        <span class="column is-narrow article-category">
            <i class="far fa-folder"></i>
            <a class="article-category-link" href="/categories/Tech-Blog/">Tech Blog</a>
        </span>
        
        
        <span class="column is-narrow">
            
            
            32 分钟 读完 (约 4735 字)
        </span>
        
    </div>
    <div class="article-entry is-size-6-mobile" itemprop="articleBody">
    
        <html><head></head><body><p>继上一篇介绍了Minimax 和Alpha Beta
剪枝算法之后，本篇选择了Leetcode中的井字棋游戏题目，积累相关代码后实现井字棋游戏并扩展到五子棋和N子棋（战略井字棋），随后用Minimax和Alpha
Beta剪枝算法解得小规模下N子棋的游戏结局，并分析其状态数量和每一步的最佳策略。后续篇章中，我们基于本篇代码完成一个N子棋的OpenAI
Gym
图形环境，可用于人机对战或机器对战，并最终实现棋盘规模稍大的五子棋或者N子棋中的蒙特卡洛树搜索（MCTS）算法。</p>
<ul>
<li><p><a href="/zh/2020/combinatorial-game-2-tictactoe/zh/combinatorial-game-1-minimax.md">第一篇:
Leetcode中的Minimax 和 Alpha Beta剪枝</a></p></li>
<li><p><strong><a href="/zh/2020/combinatorial-game-2-tictactoe/zh/combinatorial-game-2-tictactoe.md">第二篇:
井字棋Leetcode系列题解和Minimax最佳策略实现</a></strong></p></li>
<li><p><a href="/zh/2020/combinatorial-game-2-tictactoe/zh/combinatorial-game-3-openai-gym-pygame.md">第三篇:
井字棋、五子棋的OpenAI Gym GUI环境</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-2-tictactoe/zh/combinatorial-game-4-alphago-zero-theory/index.md">第四篇:
AlphaGo Zero 强化学习算法原理深度分析</a></p></li>
<li><p><a href="/zh/2020/combinatorial-game-2-tictactoe/zh/combinatorial-game-5-alphago-zero-connect-n/index.md">第五篇:
井字棋、五子棋AlphaGo Zero 算法实战</a></p></li>
</ul>
<h2 id="leetcode-上的井字棋系列">Leetcode 上的井字棋系列</h2>
<h3 id="leetcode-1275.-找出井字棋的获胜者-简单"><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/find-winner-on-a-tic-tac-toe-game/">Leetcode
1275. 找出井字棋的获胜者 (简单)</a></h3>
<blockquote>
<p>A 和&nbsp;B&nbsp;在一个&nbsp;3&nbsp;x&nbsp;3&nbsp;的网格上玩井字棋。<br>
井字棋游戏的规则如下：<br> 玩家轮流将棋子放在空方格 (" ") 上。<br>
第一个玩家 A 总是用&nbsp;"X" 作为棋子，而第二个玩家 B 总是用 "O"
作为棋子。<br> "X" 和 "O"
只能放在空方格中，而不能放在已经被占用的方格上。<br> 只要有 3
个相同的（非空）棋子排成一条直线（行、列、对角线）时，游戏结束。<br>
如果所有方块都放满棋子（不为空），游戏也会结束。<br>
游戏结束后，棋子无法再进行任何移动。<br> 给你一个数组
moves，其中每个元素是大小为 2
的另一个数组（元素分别对应网格的行和列），它按照 A 和 B 的行动顺序（先 A
后 B）记录了两人各自的棋子位置。<br> 如果游戏存在获胜者（A 或
B），就返回该游戏的获胜者；如果游戏以平局结束，则返回
"Draw"；如果仍会有行动（游戏未结束），则返回 "Pending"。<br>
你可以假设&nbsp;moves&nbsp;都 有效（遵循井字棋规则），网格最初是空的，A
将先行动。</p>
</blockquote>
<blockquote>
<p>示例 1：<br> 输入：moves = [[0,0],[2,0],[1,1],[2,1],[2,2]]<br>
输出："A"<br> 解释："A" 获胜，他总是先走。<br> "X " "X " "X " "X " "X
"<br> " " -&gt; " " -&gt; " X " -&gt; " X " -&gt; " X "<br> " " "O "
"O " "OO " "OOX"<br></p>
</blockquote>
<blockquote>
<p>示例 2： 输入：moves = [[0,0],[1,1],[0,1],[0,2],[1,0],[2,0]]<br>
输出："B"<br> 解释："B" 获胜。<br> "X " "X " "XX " "XXO" "XXO"
"XXO"<br> " " -&gt; " O " -&gt; " O " -&gt; " O " -&gt; "XO " -&gt; "XO
" <br> " " " " " " " " " " "O "<br></p>
</blockquote>
<p>第一种解法，检查A或者B赢的所有可能情况：某玩家占据8种连线的任意一种情况则胜利，我们使用八个变量来保存所有情况。下面的代码使用了一个小技巧，将moves转换成3x3的棋盘状态数组，元素的值为1，-1和0。1，-1代表两个玩家，0代表空的棋盘格子，其优势在于后续我们只需累加棋盘的值到八个变量中关联的若干个，再检查这八个变量是否满足取胜条件。例如，row[0]表示第一行的状态，当遍历一次所有棋盘格局后，row[0]为第一行的3个格子的总和，只有当row[0]
== 3 才表明玩家A占据了第一行，-3表明玩家B占据了第一行。
</p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># AC</span></span><br><span class="line"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tictactoe</span>(<span class="hljs-params">self, moves: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-built_in">str</span>:</span></span><br><span class="line">        board = [[<span class="hljs-number">0</span>] * <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]</span><br><span class="line">        <span class="hljs-keyword">for</span> idx, xy <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(moves):</span><br><span class="line">            player = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span></span><br><span class="line">            board[xy[<span class="hljs-number">0</span>]][xy[<span class="hljs-number">1</span>]] = player</span><br><span class="line"></span><br><span class="line">        turn = <span class="hljs-number">0</span></span><br><span class="line">        row, col = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]</span><br><span class="line">        diag1, diag2 = <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span></span><br><span class="line">        <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):</span><br><span class="line">            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):</span><br><span class="line">                turn += board[r][c]</span><br><span class="line">                row[r] += board[r][c]</span><br><span class="line">                col[c] += board[r][c]</span><br><span class="line">                <span class="hljs-keyword">if</span> r == c:</span><br><span class="line">                    diag1 += board[r][c]</span><br><span class="line">                <span class="hljs-keyword">if</span> r + c == <span class="hljs-number">2</span>:</span><br><span class="line">                    diag2 += board[r][c]</span><br><span class="line"></span><br><span class="line">        oWin = <span class="hljs-built_in">any</span>(row[r] == <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> <span class="hljs-built_in">any</span>(col[c] == <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> diag1 == <span class="hljs-number">3</span> <span class="hljs-keyword">or</span> diag2 == <span class="hljs-number">3</span></span><br><span class="line">        xWin = <span class="hljs-built_in">any</span>(row[r] == -<span class="hljs-number">3</span> <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> <span class="hljs-built_in">any</span>(col[c] == -<span class="hljs-number">3</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> diag1 == -<span class="hljs-number">3</span> <span class="hljs-keyword">or</span> diag2 == -<span class="hljs-number">3</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-string">"A"</span> <span class="hljs-keyword">if</span> oWin <span class="hljs-keyword">else</span> <span class="hljs-string">"B"</span> <span class="hljs-keyword">if</span> xWin <span class="hljs-keyword">else</span> <span class="hljs-string">"Draw"</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(moves) == <span class="hljs-number">9</span> <span class="hljs-keyword">else</span> <span class="hljs-string">"Pending"</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>下面我们给出另一种解法，这种解法虽然代码较多，但可以不必遍历棋盘每个格子，比上一种严格遍历一次棋盘的解法略为高效。原理如下，题目保证了moves过程中不会产生输赢结果，因此我们直接检查最后一个棋子向外的八个方向，若任意方向有三连子，则此玩家获胜。这种解法主要是为后续井字棋扩展到五子棋时判断每个落子是否产生输赢做代码准备。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># AC</span></span><br><span class="line"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">checkWin</span>(<span class="hljs-params">self, r: <span class="hljs-built_in">int</span>, c: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:</span></span><br><span class="line">        north = self.getConnectedNum(r, c, -<span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span><br><span class="line">        south = self.getConnectedNum(r, c, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)</span><br><span class="line"></span><br><span class="line">        east = self.getConnectedNum(r, c, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)</span><br><span class="line">        west = self.getConnectedNum(r, c, <span class="hljs-number">0</span>, -<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        south_east = self.getConnectedNum(r, c, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">        north_west = self.getConnectedNum(r, c, -<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        north_east = self.getConnectedNum(r, c, -<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)</span><br><span class="line">        south_west = self.getConnectedNum(r, c, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> (north + south + <span class="hljs-number">1</span> &gt;= <span class="hljs-number">3</span>) <span class="hljs-keyword">or</span> (east + west + <span class="hljs-number">1</span> &gt;= <span class="hljs-number">3</span>) <span class="hljs-keyword">or</span> \</span><br><span class="line">                (south_east + north_west + <span class="hljs-number">1</span> &gt;= <span class="hljs-number">3</span>) <span class="hljs-keyword">or</span> (north_east + south_west + <span class="hljs-number">1</span> &gt;= <span class="hljs-number">3</span>):</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getConnectedNum</span>(<span class="hljs-params">self, r: <span class="hljs-built_in">int</span>, c: <span class="hljs-built_in">int</span>, dr: <span class="hljs-built_in">int</span>, dc: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">        player = self.board[r][c]</span><br><span class="line">        result = <span class="hljs-number">0</span></span><br><span class="line">        i = <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:</span><br><span class="line">            new_r = r + dr * i</span><br><span class="line">            new_c = c + dc * i</span><br><span class="line">            <span class="hljs-keyword">if</span> <span class="hljs-number">0</span> &lt;= new_r &lt; <span class="hljs-number">3</span> <span class="hljs-keyword">and</span> <span class="hljs-number">0</span> &lt;= new_c &lt; <span class="hljs-number">3</span>:</span><br><span class="line">                <span class="hljs-keyword">if</span> self.board[new_r][new_c] == player:</span><br><span class="line">                    result += <span class="hljs-number">1</span></span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    <span class="hljs-keyword">break</span></span><br><span class="line">            <span class="hljs-keyword">else</span>:</span><br><span class="line">                <span class="hljs-keyword">break</span></span><br><span class="line">            i += <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">tictactoe</span>(<span class="hljs-params">self, moves: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]</span>) -&gt; <span class="hljs-built_in">str</span>:</span></span><br><span class="line">        self.board = [[<span class="hljs-number">0</span>] * <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]</span><br><span class="line">        <span class="hljs-keyword">for</span> idx, xy <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(moves):</span><br><span class="line">            player = <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span></span><br><span class="line">            self.board[xy[<span class="hljs-number">0</span>]][xy[<span class="hljs-number">1</span>]] = player</span><br><span class="line"></span><br><span class="line">        <span class="hljs-comment"># only check last move</span></span><br><span class="line">        r, c = moves[-<span class="hljs-number">1</span>]</span><br><span class="line">        win = self.checkWin(r, c)</span><br><span class="line">        <span class="hljs-keyword">if</span> win:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-string">"A"</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(moves) % <span class="hljs-number">2</span> == <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-string">"B"</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-string">"Draw"</span> <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(moves) == <span class="hljs-number">9</span> <span class="hljs-keyword">else</span> <span class="hljs-string">"Pending"</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="leetcode-794.-有效的井字游戏-中等"><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/valid-tic-tac-toe-state/">Leetcode
794. 有效的井字游戏 (中等)</a></h3>
<blockquote>
<p>用字符串数组作为井字游戏的游戏板
board。当且仅当在井字游戏过程中，玩家有可能将字符放置成游戏板所显示的状态时，才返回
true。<br> 该游戏板是一个 3 x 3 数组，由字符 " "，"X" 和 "O" 组成。字符
" " 代表一个空位。<br> 以下是井字游戏的规则：<br>
玩家轮流将字符放入空位（" "）中。<br> 第一个玩家总是放字符
“X”，且第二个玩家总是放字符 “O”。<br> “X” 和 “O”
只允许放置在空位中，不允许对已放有字符的位置进行填充。<br> 当有 3
个相同（且非空）的字符填充任何行、列或对角线时，游戏结束。<br>
当所有位置非空时，也算为游戏结束。<br>
如果游戏结束，玩家不允许再放置字符。<br></p>
</blockquote>
<blockquote>
<p>示例 1:<br> 输入: board = ["O ", " ", " "]<br> 输出: false<br>
解释: 第一个玩家总是放置“X”。<br></p>
</blockquote>
<blockquote>
<p>示例 2:<br> 输入: board = ["XOX", " X ", " "]<br> 输出: false<br>
解释: 玩家应该是轮流放置的。<br></p>
</blockquote>
<blockquote>
<p>示例 3:<br> 输入: board = ["XXX", " ", "OOO"]<br> 输出:
false<br></p>
</blockquote>
<blockquote>
<p>示例 4:<br> 输入: board = ["XOX", "O O", "XOX"]<br> 输出: true<br>
说明:<br></p>
</blockquote>
<blockquote>
<p>游戏板 board 是长度为 3 的字符串数组，其中每个字符串 board[i]
的长度为 3。 board[i][j] 是集合 {" ", "X", "O"} 中的一个字符。</p>
</blockquote>
<p>这道题第一反应是需要DFS来判断给定状态是否可达，但其实可以用上面1275的思路，即通过检验最终棋盘的一些特点来判断给定状态是否合法。比如，X和O的数量只有可能相同，或X比O多一个。其关键在于需要找到判断状态合法的充要条件，就可以在<span class="math inline">\(O(1)\)</span> 时间复杂度完成判断。
此外，这道题给了我们井字棋所有可能状态数量的启示。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># AC</span></span><br><span class="line"><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">List</span></span><br><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">convertCell</span>(<span class="hljs-params">self, c:<span class="hljs-built_in">str</span></span>):</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> c == <span class="hljs-string">'X'</span> <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> c == <span class="hljs-string">'O'</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validTicTacToe</span>(<span class="hljs-params">self, board: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-built_in">bool</span>:</span></span><br><span class="line">        turn = <span class="hljs-number">0</span></span><br><span class="line">        row, col = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]</span><br><span class="line">        diag1, diag2 = <span class="hljs-literal">False</span>, <span class="hljs-literal">False</span></span><br><span class="line">        <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):</span><br><span class="line">            <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):</span><br><span class="line">                turn += self.convertCell(board[r][c])</span><br><span class="line">                row[r] += self.convertCell(board[r][c])</span><br><span class="line">                col[c] += self.convertCell(board[r][c])</span><br><span class="line">                <span class="hljs-keyword">if</span> r == c:</span><br><span class="line">                    diag1 += self.convertCell(board[r][c])</span><br><span class="line">                <span class="hljs-keyword">if</span> r + c == <span class="hljs-number">2</span>:</span><br><span class="line">                    diag2 += self.convertCell(board[r][c])</span><br><span class="line"></span><br><span class="line">        xWin = <span class="hljs-built_in">any</span>(row[r] == <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> <span class="hljs-built_in">any</span>(col[c] == <span class="hljs-number">3</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> diag1 == <span class="hljs-number">3</span> <span class="hljs-keyword">or</span> diag2 == <span class="hljs-number">3</span></span><br><span class="line">        oWin = <span class="hljs-built_in">any</span>(row[r] == -<span class="hljs-number">3</span> <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> <span class="hljs-built_in">any</span>(col[c] == -<span class="hljs-number">3</span> <span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)) <span class="hljs-keyword">or</span> diag1 == -<span class="hljs-number">3</span> <span class="hljs-keyword">or</span> diag2 == -<span class="hljs-number">3</span></span><br><span class="line">        <span class="hljs-keyword">if</span> (xWin <span class="hljs-keyword">and</span> turn == <span class="hljs-number">0</span>) <span class="hljs-keyword">or</span> (oWin <span class="hljs-keyword">and</span> turn == <span class="hljs-number">1</span>):</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span></span><br><span class="line">        <span class="hljs-keyword">return</span> (turn == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> turn == <span class="hljs-number">1</span>) <span class="hljs-keyword">and</span> (<span class="hljs-keyword">not</span> xWin <span class="hljs-keyword">or</span> <span class="hljs-keyword">not</span> oWin)</span><br></pre></td></tr></tbody></table></figure>
<h3 id="leetcode-348.-判定井字棋胜负-中等加锁"><a target="_blank" rel="noopener" href="https://leetcode.com/problems/design-tic-tac-toe/">Leetcode 348.
判定井字棋胜负 (中等，加锁)</a></h3>
<blockquote>
<p>请在 n × n
的棋盘上，实现一个判定井字棋（Tic-Tac-Toe）胜负的神器，判断每一次玩家落子后，是否有胜出的玩家。<br>
在这个井字棋游戏中，会有 2
名玩家，他们将轮流在棋盘上放置自己的棋子。<br>
在实现这个判定器的过程中，你可以假设以下这些规则一定成立：<br>
每一步棋都是在棋盘内的，并且只能被放置在一个空的格子里；<br>
一旦游戏中有一名玩家胜出的话，游戏将不能再继续；<br>
一个玩家如果在同一行、同一列或者同一斜对角线上都放置了自己的棋子，那么他便获得胜利。</p>
</blockquote>
<blockquote>
<p>示例： 给定棋盘边长 n = 3, 玩家 1 的棋子符号是 "X"，玩家 2
的棋子符号是 "O"。<br> TicTacToe toe = new TicTacToe(3);<br>
toe.move(0, 0, 1); -&gt; 函数返回 0
(此时，暂时没有玩家赢得这场对决)<br> |X| | |<br> | | | | // 玩家 1 在
(0, 0) 落子。<br> | | | |<br> <br> toe.move(0, 2, 2); -&gt; 函数返回
0 (暂时没有玩家赢得本场比赛)<br> |X| |O|<br> | | | | // 玩家 2 在 (0,
2) 落子。<br> | | | |<br> <br> toe.move(2, 2, 1); -&gt; 函数返回 0
(暂时没有玩家赢得比赛)<br> |X| |O|<br> | | | | // 玩家 1 在 (2, 2)
落子。<br> | | |X|<br> <br> toe.move(1, 1, 2); -&gt; 函数返回 0
(暂没有玩家赢得比赛)<br> |X| |O|<br> | |O| | // 玩家 2 在 (1, 1)
落子。<br> | | |X|<br> <br> toe.move(2, 0, 1); -&gt; 函数返回 0
(暂无玩家赢得比赛)<br> |X| |O|<br> | |O| | // 玩家 1 在 (2, 0)
落子。<br> |X| |X|<br> <br> toe.move(1, 0, 2); -&gt; 函数返回 0
(没有玩家赢得比赛)<br> |X| |O|<br> |O|O| | // 玩家 2 在 (1, 0)
落子.<br> |X| |X|<br> <br> toe.move(2, 1, 1); -&gt; 函数返回 1
(此时，玩家 1 赢得了该场比赛)<br> |X| |O|<br> |O|O| | // 玩家 1 在 (2,
1) 落子。<br> |X|X|X|<br></p>
</blockquote>
<p>348 是道加锁题，对于每次玩家的move，可以用1275第二种解法中的checkWin
函数。下面代码给出了另一种基于1275解法一的方法：保存八个关键变量，每次落子后更新这个子所关联的某几个变量。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-comment"># AC</span></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TicTacToe</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, n:<span class="hljs-built_in">int</span></span>):</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Initialize your data structure here.</span></span><br><span class="line"><span class="hljs-string">        :type n: int</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        self.row, self.col, self.diag1, self.diag2, self.n = [<span class="hljs-number">0</span>] * n, [<span class="hljs-number">0</span>] * n, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, n</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">move</span>(<span class="hljs-params">self, row:<span class="hljs-built_in">int</span>, col:<span class="hljs-built_in">int</span>, player:<span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string">        Player {player} makes a move at ({row}, {col}).</span></span><br><span class="line"><span class="hljs-string">        @param row The row of the board.</span></span><br><span class="line"><span class="hljs-string">        @param col The column of the board.</span></span><br><span class="line"><span class="hljs-string">        @param player The player, can be either 1 or 2.</span></span><br><span class="line"><span class="hljs-string">        @return The current winning condition, can be either:</span></span><br><span class="line"><span class="hljs-string">                0: No one wins.</span></span><br><span class="line"><span class="hljs-string">                1: Player 1 wins.</span></span><br><span class="line"><span class="hljs-string">                2: Player 2 wins.</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        <span class="hljs-keyword">if</span> player == <span class="hljs-number">2</span>:</span><br><span class="line">            player = -<span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">        self.row[row] += player</span><br><span class="line">        self.col[col] += player</span><br><span class="line">        <span class="hljs-keyword">if</span> row == col:</span><br><span class="line">            self.diag1 += player</span><br><span class="line">        <span class="hljs-keyword">if</span> row + col == self.n - <span class="hljs-number">1</span>:</span><br><span class="line">            self.diag2 += player</span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> self.n <span class="hljs-keyword">in</span> [self.row[row], self.col[col], self.diag1, self.diag2]:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span> -self.n <span class="hljs-keyword">in</span> [self.row[row], self.col[col], self.diag1, self.diag2]:</span><br><span class="line">            <span class="hljs-keyword">return</span> <span class="hljs-number">2</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="井字棋最佳策略">井字棋最佳策略</h2>
<p>井字棋的规模可以很自然的扩展成四子棋或五子棋等，区别在于棋盘大小和胜利时的连子数量。这类游戏最一般的形式为
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/M,n,k-game">M,n,k-game</a>，中文可能翻译为战略井字游戏，表示棋盘大小为M
x N，当k连子时获胜。
下面的ConnectNGame类实现了战略井字游戏（M=N）中，两个玩家轮流下子、更新棋盘状态和判断每次落子输赢等逻辑封装。其中undo方法用于撤销最后一个落子，方便在后续寻找最佳策略时回溯。</p>
<h3 id="connectngame">ConnectNGame</h3>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ConnectNGame</span>:</span></span><br><span class="line"></span><br><span class="line">    PLAYER_A = <span class="hljs-number">1</span></span><br><span class="line">    PLAYER_B = -<span class="hljs-number">1</span></span><br><span class="line">    AVAILABLE = <span class="hljs-number">0</span></span><br><span class="line">    RESULT_TIE = <span class="hljs-number">0</span></span><br><span class="line">    RESULT_A_WIN = <span class="hljs-number">1</span></span><br><span class="line">    RESULT_B_WIN = -<span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, N:<span class="hljs-built_in">int</span> = <span class="hljs-number">3</span>, board_size:<span class="hljs-built_in">int</span> = <span class="hljs-number">3</span></span>):</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> N &lt;= board_size</span><br><span class="line">        self.N = N</span><br><span class="line">        self.board_size = board_size</span><br><span class="line">        self.board = [[ConnectNGame.AVAILABLE] * board_size <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(board_size)]</span><br><span class="line">        self.gameOver = <span class="hljs-literal">False</span></span><br><span class="line">        self.gameResult = <span class="hljs-literal">None</span></span><br><span class="line">        self.currentPlayer = ConnectNGame.PLAYER_A</span><br><span class="line">        self.remainingPosNum = board_size * board_size</span><br><span class="line">        self.actionStack = []</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">move</span>(<span class="hljs-params">self, r: <span class="hljs-built_in">int</span>, c: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:</span></span><br><span class="line">        <span class="hljs-string">"""</span></span><br><span class="line"><span class="hljs-string"></span></span><br><span class="line"><span class="hljs-string">        :param r:</span></span><br><span class="line"><span class="hljs-string">        :param c:</span></span><br><span class="line"><span class="hljs-string">        :return: None: game ongoing</span></span><br><span class="line"><span class="hljs-string">        """</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> self.board[r][c] == ConnectNGame.AVAILABLE</span><br><span class="line">        self.board[r][c] = self.currentPlayer</span><br><span class="line">        self.actionStack.append((r, c))</span><br><span class="line">        self.remainingPosNum -= <span class="hljs-number">1</span></span><br><span class="line">        <span class="hljs-keyword">if</span> self.checkWin(r, c):</span><br><span class="line">            self.gameOver = <span class="hljs-literal">True</span></span><br><span class="line">            self.gameResult = self.currentPlayer</span><br><span class="line">            <span class="hljs-keyword">return</span> self.currentPlayer</span><br><span class="line">        <span class="hljs-keyword">if</span> self.remainingPosNum == <span class="hljs-number">0</span>:</span><br><span class="line">            self.gameOver = <span class="hljs-literal">True</span></span><br><span class="line">            self.gameResult = ConnectNGame.RESULT_TIE</span><br><span class="line">            <span class="hljs-keyword">return</span> ConnectNGame.RESULT_TIE</span><br><span class="line">        self.currentPlayer *= -<span class="hljs-number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">undo</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.actionStack) &gt; <span class="hljs-number">0</span>:</span><br><span class="line">            lastAction = self.actionStack.pop()</span><br><span class="line">            r, c = lastAction</span><br><span class="line">            self.board[r][c] = ConnectNGame.AVAILABLE</span><br><span class="line">            self.currentPlayer = ConnectNGame.PLAYER_A <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(self.actionStack) % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> ConnectNGame.PLAYER_B</span><br><span class="line">            self.remainingPosNum += <span class="hljs-number">1</span></span><br><span class="line">            self.gameOver = <span class="hljs-literal">False</span></span><br><span class="line">            self.gameResult = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">'No lastAction'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getAvailablePositions</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> [(i,j) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.board_size) <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.board_size) <span class="hljs-keyword">if</span> self.board[i][j] == ConnectNGame.AVAILABLE]</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">getStatus</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]:</span></span><br><span class="line">        <span class="hljs-keyword">return</span> <span class="hljs-built_in">tuple</span>([<span class="hljs-built_in">tuple</span>(self.board[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.board_size)])</span><br></pre></td></tr></tbody></table></figure>
<p>其中checkWin和1275解法二中的逻辑一致。</p>
<h3 id="minimax-算法">Minimax 算法</h3>
<p>此战略井字游戏的逻辑代码，结合之前的minimax算法，可以实现游戏最佳策略。</p>
<p>先定义一个通用的策略基类和抽象方法
action。action表示给定一个棋盘状态，返回一个动作决定。返回Tuple的第一个int值表示估计走这一步的结局，第二个值类型是Tuple[int,
int]，表示这次落子的位置，例如（1，1）。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Strategy</span>(<span class="hljs-params">ABC</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self</span>):</span></span><br><span class="line">        <span class="hljs-built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line"><span class="hljs-meta">    @abstractmethod</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        <span class="hljs-keyword">pass</span></span><br></pre></td></tr></tbody></table></figure>
MinimaxStrategy
的逻辑和之前的minimax模版算法大致相同，多了保存最佳move对应的动作，用于最后返回。
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">MinimaxStrategy</span>(<span class="hljs-params">Strategy</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        self.game = copy.deepcopy(game)</span><br><span class="line">        result, move = self.minimax()</span><br><span class="line">        <span class="hljs-keyword">return</span> result, move</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">minimax</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        game = self.game</span><br><span class="line">        bestMove = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">        <span class="hljs-keyword">if</span> game.currentPlayer == ConnectNGame.PLAYER_A:</span><br><span class="line">            ret = -math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.minimax()</span><br><span class="line">                game.undo()</span><br><span class="line">                ret = <span class="hljs-built_in">max</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> ret == <span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            ret = math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.minimax()</span><br><span class="line">                game.undo()</span><br><span class="line">                ret = <span class="hljs-built_in">min</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> ret == -<span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span>, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br></pre></td></tr></tbody></table></figure>
通过上面的代码可以画出初始两步的井字棋最终结局。对于先手O来说可以落9个位置，排除对称位置后只有三种，分别为角落，边上和正中。但无论哪一个位置作为先手，最好的结局都是被对方逼平，不存在必赢的开局。所以井字棋的结局是：如果两个玩家都采用最优策略（无失误），游戏结果为双方逼平。
<figure>
<img src="/zh/2020/combinatorial-game-2-tictactoe/tictactoe_1.gv.svg">
<figcaption>
井字棋第一步结局
</figcaption>
</figure>
下面分别画出三种开局后进一步的游戏结局。
<figure>
<img src="/zh/2020/combinatorial-game-2-tictactoe/tictactoe_2_1.gv.svg">
<figcaption>
井字棋角落开局
</figcaption>
</figure>
<figure>
<img src="/zh/2020/combinatorial-game-2-tictactoe/tictactoe_2_2.gv.svg">
<figcaption>
井字棋边上开局
</figcaption>
</figure>
<figure>
<img src="/zh/2020/combinatorial-game-2-tictactoe/tictactoe_2_3.gv.svg">
<figcaption>
井字棋中间开局
</figcaption>
</figure>
<h3 id="井字棋游戏状态数和解">井字棋游戏状态数和解</h3>
<p>有趣的是井字棋游戏的状态数量，简单的上限估算是<span class="math inline">\(3^9=19683\)</span>。这显然是个较宽泛的上限，因为很多状态在游戏结束后无法达到。
这篇文章 <a target="_blank" rel="noopener" href="http://www.mathrec.org/old/2002jan/solutions.html">Tic-Tac-Toe
(Naughts and Crosses, Cheese and Crackers, etc</a>
中列出了每一步的状态数，合计5478个。</p>
<table>
<thead>
<tr class="header">
<th>Moves</th>
<th>Positions</th>
<th>Terminal Positions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>1</td>
<td></td>
</tr>
<tr class="even">
<td>1</td>
<td>9</td>
<td></td>
</tr>
<tr class="odd">
<td>2</td>
<td>72</td>
<td></td>
</tr>
<tr class="even">
<td>3</td>
<td>252</td>
<td></td>
</tr>
<tr class="odd">
<td>4</td>
<td>756</td>
<td></td>
</tr>
<tr class="even">
<td>5</td>
<td>1260</td>
<td>120</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1520</td>
<td>148</td>
</tr>
<tr class="even">
<td>7</td>
<td>1140</td>
<td>444</td>
</tr>
<tr class="odd">
<td>8</td>
<td>390</td>
<td>168</td>
</tr>
<tr class="even">
<td>9</td>
<td>78</td>
<td>78</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>5478</td>
<td>958</td>
</tr>
</tbody>
</table>
<p>我们已经实现了井字棋的minimax策略，算法本质上遍历了所有情况，稍加改造后增加dp数组，就可以确认上面的总状态数。</p>
<figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CountingMinimaxStrategy</span>(<span class="hljs-params">Strategy</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        self.game = copy.deepcopy(game)</span><br><span class="line">        self.dpMap = {}</span><br><span class="line">        result, move = self.minimax(game.getStatus())</span><br><span class="line">        <span class="hljs-keyword">return</span> result, move</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">minimax</span>(<span class="hljs-params">self, gameStatus: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        <span class="hljs-comment"># print(f'Current {len(strategy.dpMap)}')</span></span><br><span class="line"></span><br><span class="line">        <span class="hljs-keyword">if</span> gameStatus <span class="hljs-keyword">in</span> self.dpMap:</span><br><span class="line">            <span class="hljs-keyword">return</span> self.dpMap[gameStatus]</span><br><span class="line"></span><br><span class="line">        game = self.game</span><br><span class="line">        bestMove = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">        <span class="hljs-keyword">if</span> game.currentPlayer == ConnectNGame.PLAYER_A:</span><br><span class="line">            ret = -math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.minimax(game.getStatus())</span><br><span class="line">                    self.dpMap[game.getStatus()] = result, oppMove</span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    self.dpMap[game.getStatus()] = result, move</span><br><span class="line">                game.undo()</span><br><span class="line">                ret = <span class="hljs-built_in">max</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">            self.dpMap[gameStatus] = ret, bestMove</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            ret = math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line"></span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.minimax(game.getStatus())</span><br><span class="line">                    self.dpMap[game.getStatus()] = result, oppMove</span><br><span class="line">                <span class="hljs-keyword">else</span>:</span><br><span class="line">                    self.dpMap[game.getStatus()] = result, move</span><br><span class="line">                game.undo()</span><br><span class="line">                ret = <span class="hljs-built_in">min</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">            self.dpMap[gameStatus] = ret, bestMove</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:</span><br><span class="line">    tic_tac_toe = ConnectNGame(N=<span class="hljs-number">3</span>, board_size=<span class="hljs-number">3</span>)</span><br><span class="line">    strategy = CountingMinimaxStrategy()</span><br><span class="line">    strategy.action(tic_tac_toe)</span><br><span class="line">    <span class="hljs-built_in">print</span>(<span class="hljs-string">f'Game States Number <span class="hljs-subst">{<span class="hljs-built_in">len</span>(strategy.dpMap)}</span>'</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p>运行程序证实了井字棋状态数为5478，下面是一些极小规模时代码运行结果：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>3x3</th>
<th>4x4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>k=3</td>
<td>5478 （Draw)</td>
<td>6035992 （Win）</td>
</tr>
<tr class="even">
<td>k=4</td>
<td></td>
<td>9722011 （Draw）</td>
</tr>
<tr class="odd">
<td>k=5</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>根据 <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/M,n,k-game">Wikipedia
M,n,k-game</a>, 列出了一些小规模下的游戏解：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>3x3</th>
<th>4x4</th>
<th>5x5</th>
<th>6x6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>k=3</td>
<td>Draw</td>
<td>Win</td>
<td>Win</td>
<td>Win</td>
</tr>
<tr class="even">
<td>k=4</td>
<td></td>
<td>Draw</td>
<td>Draw</td>
<td>Win</td>
</tr>
<tr class="odd">
<td>k=5</td>
<td></td>
<td></td>
<td>Draw</td>
<td>Draw</td>
</tr>
</tbody>
</table>
<p>值得一提的是，五子棋（棋盘15x15或以上）被 L. Victor
Allis证明是先手赢。</p>
<h3 id="alpha-beta剪枝策略">Alpha-Beta剪枝策略</h3>
<p>Alpha Beta 剪枝策略的代码如下（和之前代码比较类似，不再赘述）：
</p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlphaBetaStrategy</span>(<span class="hljs-params">Strategy</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        self.game = game</span><br><span class="line">        result, move = self.alpha_beta(self.game.getStatus(), -math.inf, math.inf)</span><br><span class="line">        <span class="hljs-keyword">return</span> result, move</span><br><span class="line"></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">alpha_beta</span>(<span class="hljs-params">self, gameStatus: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]], alpha:<span class="hljs-built_in">int</span>=<span class="hljs-literal">None</span>, beta:<span class="hljs-built_in">int</span>=<span class="hljs-literal">None</span></span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        game = self.game</span><br><span class="line">        bestMove = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">        <span class="hljs-keyword">if</span> game.currentPlayer == ConnectNGame.PLAYER_A:</span><br><span class="line">            ret = -math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.alpha_beta(game.getStatus(), alpha, beta)</span><br><span class="line">                game.undo()</span><br><span class="line">                alpha = <span class="hljs-built_in">max</span>(alpha, result)</span><br><span class="line">                ret = <span class="hljs-built_in">max</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> alpha &gt;= beta <span class="hljs-keyword">or</span> ret == <span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> ret, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            ret = math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    result, oppMove = self.alpha_beta(game.getStatus(), alpha, beta)</span><br><span class="line">                game.undo()</span><br><span class="line">                beta = <span class="hljs-built_in">min</span>(beta, result)</span><br><span class="line">                ret = <span class="hljs-built_in">min</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> alpha &gt;= beta <span class="hljs-keyword">or</span> ret == -<span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> ret, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p></p>
<p>Alpha Beta
的DP版本中，由于lru_cache无法指定cache的有效参数，递归函数并没有传入alpha,
beta。因此我们将alpha，beta参数隐式放入自己维护的栈中，并保证栈的状态和alpha_beta_dp函数调用状态一致。
</p><figure class="highlight python hljs"><figcaption><span>{linenos</span></figcaption><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">AlphaBetaDPStrategy</span>(<span class="hljs-params">Strategy</span>):</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">action</span>(<span class="hljs-params">self, game: ConnectNGame</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        self.game = game</span><br><span class="line">        self.alphaBetaStack = [(-math.inf, math.inf)]</span><br><span class="line">        result, move = self.alpha_beta_dp(self.game.getStatus())</span><br><span class="line">        <span class="hljs-keyword">return</span> result, move</span><br><span class="line"></span><br><span class="line"><span class="hljs-meta">    @lru_cache(<span class="hljs-params">maxsize=<span class="hljs-literal">None</span></span>)</span></span><br><span class="line">    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">alpha_beta_dp</span>(<span class="hljs-params">self, gameStatus: <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, ...]]</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-type">Tuple</span>[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">int</span>]]:</span></span><br><span class="line">        alpha, beta = self.alphaBetaStack[-<span class="hljs-number">1</span>]</span><br><span class="line">        game = self.game</span><br><span class="line">        bestMove = <span class="hljs-literal">None</span></span><br><span class="line">        <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">        <span class="hljs-keyword">if</span> game.currentPlayer == ConnectNGame.PLAYER_A:</span><br><span class="line">            ret = -math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    self.alphaBetaStack.append((alpha, beta))</span><br><span class="line">                    result, oppMove = self.alpha_beta_dp(game.getStatus())</span><br><span class="line">                    self.alphaBetaStack.pop()</span><br><span class="line">                game.undo()</span><br><span class="line">                alpha = <span class="hljs-built_in">max</span>(alpha, result)</span><br><span class="line">                ret = <span class="hljs-built_in">max</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> alpha &gt;= beta <span class="hljs-keyword">or</span> ret == <span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> ret, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br><span class="line">        <span class="hljs-keyword">else</span>:</span><br><span class="line">            ret = math.inf</span><br><span class="line">            <span class="hljs-keyword">for</span> pos <span class="hljs-keyword">in</span> game.getAvailablePositions():</span><br><span class="line">                move = pos</span><br><span class="line">                result = game.move(*pos)</span><br><span class="line">                <span class="hljs-keyword">if</span> result <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:</span><br><span class="line">                    <span class="hljs-keyword">assert</span> <span class="hljs-keyword">not</span> game.gameOver</span><br><span class="line">                    self.alphaBetaStack.append((alpha, beta))</span><br><span class="line">                    result, oppMove = self.alpha_beta_dp(game.getStatus())</span><br><span class="line">                    self.alphaBetaStack.pop()</span><br><span class="line">                game.undo()</span><br><span class="line">                beta = <span class="hljs-built_in">min</span>(beta, result)</span><br><span class="line">                ret = <span class="hljs-built_in">min</span>(ret, result)</span><br><span class="line">                bestMove = move <span class="hljs-keyword">if</span> ret == result <span class="hljs-keyword">else</span> bestMove</span><br><span class="line">                <span class="hljs-keyword">if</span> alpha &gt;= beta <span class="hljs-keyword">or</span> ret == -<span class="hljs-number">1</span>:</span><br><span class="line">                    <span class="hljs-keyword">return</span> ret, move</span><br><span class="line">            <span class="hljs-keyword">return</span> ret, bestMove</span><br></pre></td></tr></tbody></table></figure><p></p>
<link rel="stylesheet" href="/css/bilicard.css" type="text/css"></body></html>
    
    </div>
    
    
</article>








    
    
    </div>
</section>
    <footer class="footer">
    <div class="container">
        <div class="columns content">
            <div class="column is-narrow has-text-centered">
                &copy; 2023 MyEncyclopedia&nbsp;
                Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> & <a
                        target="_blank" rel="noopener" href="http://github.com/ppoffice/hexo-theme-minos">Minos</a>
            </div>
            <div class="column is-hidden-mobile"></div>

            
            <div class="column is-narrow">
                <div class="columns is-mobile is-multiline is-centered">
                
                    
                <a class="column is-narrow has-text-black" title="GitHub" target="_blank" rel="noopener" href="https://github.com/ppoffice/hexo-theme-minos">
                    
                    GitHub
                    
                </a>
                
                </div>
            </div>
            
            
<div class="column is-narrow has-text-centered">
    <div class="dropdown is-up is-right is-hoverable" style="margin-top: -0.2em;">
        <div class="dropdown-trigger">
            <button class="button is-small" aria-haspopup="true" aria-controls="dropdown-menu7">
                <span class="icon">
                    <i class="fas fa-globe"></i>
                </span>
                <span>简体中文</span>
                <span class="icon is-small">
            <i class="fas fa-angle-down" aria-hidden="true"></i>
          </span>
            </button>
        </div>
        <div class="dropdown-menu has-text-left" role="menu">
            <div class="dropdown-content">
            
                <a href="/tags/OpenAI-Gym/" class="dropdown-item">
                    简体中文
                </a>
            
                <a href="/en/tags/OpenAI-Gym/" class="dropdown-item">
                    English
                </a>
            
            </div>
        </div>
    </div>
</div>

        </div>
    </div>
</footer>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script>

<!-- test if the browser is outdated -->
<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" target="_blank" rel="noopener" href="http://outdatedbrowser.com/">Update my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="//cdnjs.cloudflare.com/ajax/libs/outdated-browser/1.1.5/outdatedbrowser.min.js"></script>
<script>
    $(document).ready(function () {
        // plugin function, place inside DOM ready function
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        })
    });
</script>

<script>
    window.FontAwesomeConfig = {
        searchPseudoElements: true
    }
    moment.locale("zh-CN");
</script>


    
    
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script>
    MathJax.Hub.Config({
        "HTML-CSS": {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
</script>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.css" integrity="sha384-9eLZqc9ds8eNjO3TmqPeYcDj8n+Qfa4nuSiGYa6DjLNcv9BtN69ZIulL9+8CqC9Y" crossorigin="anonymous">
<style>
 .katex-display {
    overflow-x: auto;
    overflow-y: hidden;
    height: 100%;
  }
</style>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/katex.min.js" integrity="sha384-K3vbOmF2BtaVai+Qk37uypf7VrgBubhQreNQe9aGsz9lB63dIFiQVlJbr92dw2Lx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/contrib/auto-render.min.js" integrity="sha384-kmZOZB5ObwgQnS/DuDg6TScgOiWWBiVt0plIRkZCmE6rDZGrEOQeHM5PcHi+nyqe" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>

<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        // ...options...
    });
});
</script>

    
    
<script src="//cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery-all.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.6.5/js/jquery.justifiedGallery.min.js"></script>
<script>
    (function ($) {
        $(document).ready(function () {
            if (typeof($.fn.lightGallery) === 'function') {
                $('.article.gallery').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof($.fn.justifiedGallery) === 'function') {
                $('.justified-gallery').justifiedGallery();
            }
        });
    })(jQuery);
</script>

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script>
    <style>
        .hljs {
            position: relative;
        }

        .hljs .clipboard-btn {
            float: right;
            color: #9a9a9a;
            background: none;
            border: none;
            cursor: pointer;
        }

        .hljs .clipboard-btn:hover {
          color: #8a8a8a;
        }

        .hljs > .clipboard-btn {
            display: none;
            position: absolute;
            right: 4px;
            top: 4px;
        }

        .hljs:hover > .clipboard-btn {
            display: inline;
        }

        .hljs > figcaption > .clipboard-btn {
            margin-right: 4px;
        }
    </style>
    <script>
      $(document).ready(function () {
        $('figure.hljs').each(function(i, figure) {
          var codeId = 'code-' + i;
          var code = figure.querySelector('.code');
          var copyButton = $('<button>Copy <i class="far fa-clipboard"></i></button>');
          code.id = codeId;
          copyButton.addClass('clipboard-btn');
          copyButton.attr('data-clipboard-target-id', codeId);

          var figcaption = figure.querySelector('figcaption');

          if (figcaption) {
            figcaption.append(copyButton[0]);
          } else {
            figure.prepend(copyButton[0]);
          }
        })

        var clipboard = new ClipboardJS('.clipboard-btn', {
          target: function(trigger) {
            return document.getElementById(trigger.getAttribute('data-clipboard-target-id'));
          }
        });
        clipboard.on('success', function(e) {
          e.clearSelection();
        })
      })
    </script>

    
    

    



<script src="/js/script.js"></script>


    
    <div class="searchbox ins-search">
    <div class="searchbox-mask"></div>
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="站内搜索" />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: '文章',
                PAGES: '页面',
                CATEGORIES: '分类',
                TAGS: '标签',
                UNTITLED: '(无标题)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>

<script src="/js/insight.js"></script>

    
</body>
</html>